bert/encoder/layer_4/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_2/output/LayerNorm/batchnorm/Rsqrt	1599833759869556	4
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760004620	10
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760063611	73
add_689	1599833760291647	10
bert/encoder/layer_7/attention/self/dropout/random_uniform/RandomUniform	1599833759802040	84
Square_123	1599833760276298	12
Assign_95	1599833760309837	41
Square_119	1599833760276271	11
bert/encoder/layer_1/attention/self/Reshape_1	-1	-1
bert/encoder/layer_9/attention/self/key/MatMul	1599833759950793	631
bert/encoder/layer_7/attention/self/value/bias/adam_v	-1	-1
Mul_871	1599833759807617	45
bert/pooler/dense/kernel/adam_v/read	-1	-1
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/sub	1599833760004778	12
Mul_673	1599833759817053	4
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760050040	32
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760262547	53
bert/encoder/layer_3/attention/output/LayerNorm/gamma/read	-1	-1
Mul_192	1599833760282998	4
gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul	1599833760090203	244
truediv_150	1599833760323149	18
mul_375	1599833760328907	4
bert/encoder/layer_3/attention/self/value/bias/adam_m	-1	-1
bert/encoder/layer_9/attention/self/key/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760063503	54
mul_795	1599833760329404	4
bert/encoder/layer_0/attention/output/LayerNorm/moments/variance	1599833759834649	32
bert/encoder/layer_5/attention/output/dense/kernel	-1	-1
Assign_320	1599833760307422	11
Square_12	1599833760278579	4
bert/encoder/layer_3/output/LayerNorm/moments/mean	1599833759882821	31
gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul_1	1599833760266154	611
cls/predictions/transform/dense/bias/read	-1	-1
bert/encoder/layer_4/attention/self/value/bias/adam_v/read	-1	-1
bert/encoder/layer_10/attention/output/dropout/GreaterEqual	1599833759822807	39
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v	-1	-1
gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul	1599833760063968	641
cls/predictions/output_bias/adam_v	-1	-1
gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul	1599833760028852	2220
add_601	1599833760319459	7
add_63	1599833760300129	56
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v	-1	-1
bert/encoder/layer_0/output/LayerNorm/beta/adam_v	-1	-1
global_norm/L2Loss_136	1599833760093777	5
Mul_243	1599833759812082	12
Mul_448	1599833760280565	4
Square_46	1599833760277501	4
global_step	-1	-1
bert/encoder/layer_10/intermediate/dense/kernel/read	-1	-1
gradients/bert/embeddings/LayerNorm/moments/mean_grad/Tile	1599833760271437	32
bert/encoder/layer_5/output/dense/bias/adam_m	-1	-1
bert/encoder/layer_8/attention/self/key/bias/adam_v	-1	-1
truediv_93	1599833760321140	4
bert/encoder/layer_11/output/dense/kernel/adam_v/read	-1	-1
bert/embeddings/token_type_embeddings	-1	-1
mul_778	1599833759817006	46
mul_638	1599833759813589	13
gradients/bert/encoder/layer_3/output/dropout/mul_1_grad/Mul	1599833760183451	51
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m/read	-1	-1
Assign_374	1599833760316090	10
bert/encoder/layer_5/attention/output/dense/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760116080	72
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1	1599833759915091	73
sub_196	1599833760332381	8
add_212	1599833760297147	9
Mul_665	1599833760281577	4
bert/encoder/layer_9/attention/self/transpose_2	1599833759952628	190
bert/encoder/layer_7/attention/self/query/kernel/read	-1	-1
bert/encoder/layer_8/intermediate/dense/kernel/adam_m/read	-1	-1
mul_15	1599833760332559	11
Mul_898	1599833759810003	11
mul_596	1599833760328918	4
gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul_1	1599833760241543	614
bert/encoder/layer_10/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_0/attention/self/value/bias/adam_m/read	-1	-1
bert/encoder/layer_9/attention/output/dense/BiasAdd	1599833759954847	61
mul_661	1599833760333670	17
add_633	1599833760290695	9
bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2	1599833759923222	53
add_173	1599833760326369	59
bert/encoder/layer_8/output/LayerNorm/beta/adam_m/read	-1	-1
edge_1785_bert/embeddings/Reshape@@MemcpyHtoD	1599833759797646	78
Sqrt_112	1599833760314978	8
Sqrt_80	1599833760308100	10
Assign_354	1599833760333817	6
gradients/bert/encoder/layer_3/output/LayerNorm/moments/variance_grad/truediv	1599833760183118	51
Mul_127	1599833760292183	9
gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul	1599833760101814	2500
add_375	1599833760320074	8
gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul	1599833760086112	641
Assign_336	1599833760337612	12
bert/encoder/layer_9/attention/output/dense/bias/adam_v	-1	-1
bert/pooler/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_1/output/LayerNorm/moments/variance_grad/Tile	1599833760227375	27
global_norm/L2Loss_125	1599833760107300	5
Assign_453	1599833760341184	14
bert/encoder/layer_3/attention/output/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_11/output/LayerNorm/gamma/adam_v	-1	-1
truediv_125	1599833760325060	4
Mul_293	1599833760282645	12
bert/encoder/layer_1/output/LayerNorm/gamma	-1	-1
Mul_689	1599833760276311	39
mul_279	1599833760326581	7
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760072054	68
gradients/AddN_54	1599833760161246	96
clip_by_global_norm/mul_157	1599833760274660	4
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760040975	54
Assign_449	1599833760311270	20
Square_22	1599833760280102	4
Assign_52	1599833760300186	41
gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul	1599833760264480	161
truediv_117	1599833760324898	4
bert/encoder/layer_1/attention/self/key/kernel/adam_m	-1	-1
sub_127	1599833760329731	9
gradients/bert/encoder/layer_1/attention/self/transpose_grad/transpose	1599833760245475	190
bert/encoder/layer_5/attention/self/query/bias/adam_v	-1	-1
Mul_124	1599833759811575	11
bert/encoder/layer_8/attention/self/dropout/random_uniform	-1	-1
bert/pooler/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
mul_891	1599833760328020	4
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul_1	1599833759873241	100
bert/encoder/layer_2/attention/self/key/kernel/adam_v	-1	-1
Assign_429	1599833760339993	41
bert/encoder/layer_11/attention/self/Reshape_2	-1	-1
gradients/bert/encoder/layer_7/output/dropout/mul_1_grad/Mul	1599833760095092	51
bert/embeddings/position_embeddings	-1	-1
Sqrt_55	1599833760314220	17
gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1	1599833760267325	241
gradients/cls/seq_relationship/BiasAdd_grad/BiasAddGrad	1599833759990765	8
bert/encoder/layer_2/attention/self/value/MatMul	1599833759857726	631
mul_155	1599833759815322	18
add_696	1599833760291719	10
bert/encoder/layer_11/attention/self/query/kernel/adam_m/read	-1	-1
Mul_104	1599833760278729	4
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/pooler/dense/MatMul_grad/MatMul_1	1599833759990903	16
global_norm/L2Loss_90	1599833760156244	5
truediv_29	1599833760321257	4
bert/encoder/layer_0/intermediate/dense/bias/adam_v	-1	-1
clip_by_global_norm/mul_113	1599833760275633	4
bert/encoder/layer_2/attention/output/LayerNorm/moments/variance	1599833759861456	31
Assign_481	1599833760301135	4
Square_64	1599833760276126	4
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760160681	55
Mul_16	1599833759808595	14
cls/predictions/Sum	1599833759996788	149
clip_by_global_norm/mul_37	1599833760274038	4
bert/encoder/layer_5/intermediate/dense/add_1	1599833759906227	196
gradients/bert/embeddings/add_1_grad/Sum	1599833760272000	33
Mul_912	1599833760290434	9
gradients/bert/encoder/layer_1/output/LayerNorm/moments/variance_grad/truediv	1599833760227404	52
Sqrt_78	1599833760314507	8
clip_by_global_norm/mul	-1	-1
bert/encoder/layer_4/intermediate/dense/kernel	-1	-1
add_378	1599833760320113	7
bert/encoder/layer_7/attention/output/dropout/GreaterEqual	1599833759822647	38
Mul_224	1599833760283205	4
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul	1599833759901666	38
gradients/bert/pooler/dense/MatMul_grad/MatMul	1599833759990874	27
Square_167	1599833760279294	11
bert/encoder/layer_10/output/dense/MatMul	1599833759973822	2475
gradients/bert/encoder/layer_4/intermediate/dense/mul_2_grad/Mul_1	1599833760166507	279
bert/encoder/layer_3/attention/self/dropout/mul	1599833759827287	99
add_133	1599833760282794	4
sub_121	1599833760333313	8
gradients/bert/encoder/layer_5/output/LayerNorm/moments/variance_grad/truediv	1599833760138939	52
Assign_398	1599833760315689	4
add_587	1599833760301579	9
global_norm/L2Loss_118	1599833760115839	5
Mul_1098	1599833759816072	11
bert/encoder/layer_9/output/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul_1	1599833760230145	2469
bert/encoder/layer_7/output/dense/kernel	-1	-1
bert/encoder/layer_2/intermediate/dense/bias	-1	-1
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760174150	72
gradients/bert/pooler/Squeeze_grad/Shape	-1	-1
Mul_696	1599833759805708	4
bert/embeddings/LayerNorm/beta/adam_m	-1	-1
mul_633	1599833760329260	4
sub_134	1599833760339566	22
sub_181	1599833760332329	4
Mul_611	1599833760293362	10
bert/encoder/layer_10/attention/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2	1599833759941920	53
add_162	1599833760317926	7
bert/encoder/layer_9/attention/output/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_8/output/dense/bias/adam_m/read	-1	-1
Mul_314	1599833759817938	17
gradients/bert/encoder/layer_11/output/dropout/mul_1_grad/Mul	1599833760006585	51
add_581	1599833760318824	4
bert/encoder/layer_4/attention/self/Mul	1599833759886076	98
bert/encoder/layer_7/attention/self/key/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760072462	65
truediv_120	1599833760321036	17
edge_1712_Cast@@MemcpyDtoH	1599833759822935	4
mul_639	1599833760333647	17
bert/encoder/layer_11/intermediate/dense/add	1599833759985845	279
truediv_162	1599833760323168	58
global_norm/L2Loss_57	1599833760200486	15
bert/encoder/layer_5/attention/output/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/Pow	1599833759945088	195
sub_137	1599833760333194	8
bert/encoder/layer_7/output/dropout/Cast	1599833759824708	37
Sqrt_107	1599833760307908	16
clip_by_global_norm/mul_81	1599833760273640	4
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1	1599833759874941	73
gradients/bert/encoder/layer_9/output/dropout/mul_1_grad/Mul	1599833760050807	52
bert/encoder/layer_5/intermediate/dense/kernel/adam_v	-1	-1
bert/encoder/layer_6/attention/self/query/bias/adam_m/read	-1	-1
mul_843	1599833759816462	19
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760271826	73
Assign_360	1599833760339366	17
cls/predictions/transform/dense/add	1599833759990606	14
gradients/bert/encoder/layer_1/intermediate/dense/mul_3_grad/Mul_1	1599833759851188	195
global_norm/L2Loss_1	1599833760272503	6
gradients/AddN_31	1599833760093784	118
bert/encoder/layer_7/attention/self/value/MatMul	1599833759924671	630
bert/encoder/layer_10/attention/output/LayerNorm/beta	-1	-1
bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1	1599833759923147	73
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760182686	31
Mul_277	1599833759818087	4
Mul_158	1599833760276547	4
Mul_309	1599833759819283	11
truediv_60	1599833760324233	17
bert/encoder/layer_6/attention/self/key/bias/adam_m/read	-1	-1
bert/encoder/layer_5/attention/self/key/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_6/attention/self/transpose_1_grad/transpose	1599833760135116	189
add_456	1599833760329314	20
bert/encoder/layer_3/intermediate/dense/add_1	1599833759879461	195
bert/encoder/layer_4/attention/output/dropout/random_uniform	-1	-1
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760049633	71
Square_3	1599833760278735	4
mul_714	1599833760325406	8
sub_25	1599833760330077	4
mul_414	1599833760333293	16
Assign_126	1599833760335202	5
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760063294	66
truediv_36	1599833760322158	5
bert/encoder/layer_3/intermediate/dense/mul	1599833759878383	195
gradients/bert/encoder/layer_10/output/dropout/mul_grad/Mul	1599833760028718	72
bert/encoder/layer_1/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_7/attention/self/value/bias/adam_v/read	-1	-1
global_norm/L2Loss_98	1599833760144154	5
bert/encoder/layer_8/intermediate/dense/kernel/adam_v/read	-1	-1
cls/predictions/one_hot	1599833759821179	759
global_norm/L2Loss_64	1599833760195144	9
gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/mul	1599833759958874	195
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v/read	-1	-1
global_step/add	1599833759827093	4
gradients/bert/encoder/layer_3/intermediate/dense/mul_1_grad/Mul_1	1599833760189212	194
bert/encoder/layer_4/attention/output/LayerNorm/beta	-1	-1
bert/encoder/layer_9/output/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul	1599833760227924	2220
gradients/bert/encoder/layer_2/output/LayerNorm/moments/mean_grad/Tile	1599833760205090	27
bert/encoder/layer_1/output/add	1599833759855948	72
gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul	1599833760154930	637
cls/predictions/transform/dense/kernel/adam_v/read	-1	-1
add_687	1599833760302373	9
sub_87	1599833760330365	7
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760107548	32
add_110	1599833760299022	60
add_40	1599833760300118	4
gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/Pow	1599833759971871	196
bert/encoder/layer_3/attention/self/query/kernel/adam_v/read	-1	-1
bert/encoder/layer_2/attention/self/key/bias	-1	-1
bert/encoder/layer_5/attention/self/query/kernel/adam_v/read	-1	-1
bert/encoder/layer_6/output/dense/kernel/read	-1	-1
bert/encoder/layer_6/output/LayerNorm/batchnorm/add_1	1599833759923332	73
cls/predictions/MatMul	1599833759991053	3498
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m/read	-1	-1
bert/encoder/layer_6/intermediate/dense/Pow	1599833759917774	365
bert/encoder/layer_4/attention/self/add	1599833759886176	129
Mul_937	1599833760279251	4
add_383	1599833760293335	9
gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul_1	1599833760025392	612
truediv_86	1599833760324434	21
bert/encoder/layer_2/attention/output/dense/bias/read	-1	-1
add_321	1599833760317153	4
truediv_100	1599833760321367	4
gradients/bert/encoder/layer_4/output/LayerNorm/moments/variance_grad/truediv	1599833760161012	51
bert/encoder/layer_8/output/LayerNorm/gamma/adam_m	-1	-1
gradients/bert/encoder/layer_7/output/LayerNorm/moments/variance_grad/truediv	1599833760094760	52
bert/encoder/layer_9/attention/self/query/kernel/adam_v/read	-1	-1
bert/encoder/layer_7/attention/output/dense/bias/adam_m/read	-1	-1
Mul_206	1599833759811409	18
mul_64	1599833760327321	4
Assign_531	1599833760341241	41
Mul_751	1599833760295179	4
bert/encoder/layer_3/intermediate/dense/Pow	1599833759877620	366
add_114	1599833760299137	5
Assign_288	1599833760334146	4
bert/encoder/layer_10/attention/output/dense/kernel/adam_m	-1	-1
Assign_464	1599833760311151	4
Mul_190	1599833760277304	4
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760129054	72
bert/encoder/layer_6/attention/self/add	1599833759912940	128
Assign_384	1599833760337810	1488
Assign_218	1599833760315761	10
bert/encoder/layer_4/attention/self/Reshape	-1	-1
global_norm/L2Loss_190	1599833760019195	5
sub_59	1599833760330726	4
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/sub	1599833760156266	101
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760151353	69
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760173714	52
gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1	1599833760043456	241
bert/encoder/layer_5/attention/self/dropout/GreaterEqual	1599833759823351	74
bert/encoder/layer_6/output/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_6/attention/output/dropout/mul_1_grad/Mul	1599833760130153	52
bert/encoder/layer_2/output/dropout/random_uniform/mul	-1	-1
Assign_612	1599833760341546	4
bert/encoder/layer_8/attention/self/query/kernel/read	-1	-1
Mul_142	1599833759811841	18
Mul_1006	1599833759807344	11
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2	1599833759834809	55
Mul_253	1599833759817425	11
global_norm/L2Loss_148	1599833760072577	5
Assign_574	1599833760302952	47
global_norm/L2Loss_131	1599833760094320	5
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760151970	53
clip_by_global_norm/mul_31	1599833760273646	4
Mul_523	1599833760276415	4
bert/encoder/layer_2/attention/self/dropout/GreaterEqual	1599833759823872	74
gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul_1	1599833760075357	2469
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760173148	69
Mul_818	1599833760279023	13
gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul	1599833760256565	2500
clip_by_global_norm/mul_200	1599833760275188	12
global_norm/L2Loss_25	1599833760244726	15
Square_76	1599833760281058	4
Mul_185	1599833759811129	40
Mul_570	1599833759815014	17
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760249174	56
bert/encoder/layer_2/output/LayerNorm/moments/mean	1599833759869426	32
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul	1599833759861501	39
add_447	1599833760281594	4
sub_112	1599833760339912	59
Assign_362	1599833760315523	11
bert/encoder/layer_6/output/LayerNorm/beta	-1	-1
Sqrt_138	1599833760316264	4
bert/encoder/layer_7/intermediate/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_3/attention/self/dropout/random_uniform/RandomUniform	1599833759801445	83
bert/encoder/layer_9/attention/output/dropout/Cast	1599833759824822	36
mul_1015	1599833759817331	18
Mul_372	1599833760280707	4
truediv_175	1599833760323549	5
bert/encoder/layer_10/attention/self/dropout/GreaterEqual	1599833759823426	73
mul_982	1599833759817310	13
bert/encoder/layer_1/intermediate/dense/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1	1599833760065600	241
bert/encoder/layer_4/attention/output/dense/kernel/read	-1	-1
bert/encoder/layer_6/output/LayerNorm/batchnorm/sub	1599833759923277	54
Square_45	1599833760277365	4
bert/encoder/layer_4/attention/self/query/kernel/adam_v/read	-1	-1
add_103	1599833760282141	4
gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul_1	1599833760246567	615
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760271225	31
gradients/bert/embeddings/Slice_grad/Pad	1599833760272034	8
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760182426	51
bert/encoder/layer_8/output/dropout/mul	1599833759826615	51
gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul	1599833760200747	244
gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul	1599833760123895	2500
Mul_980	1599833759819433	17
bert/encoder/layer_2/attention/self/dropout/mul	1599833759828296	99
Assign_462	1599833760336109	4
add_458	1599833760306273	10
Mul_555	1599833760277136	4
bert/encoder/layer_6/intermediate/dense/bias	-1	-1
global_norm/L2Loss_205	1599833759990834	5
bert/encoder/layer_9/output/LayerNorm/gamma/adam_v	-1	-1
clip_by_global_norm/mul_64	1599833760275440	40
clip_by_global_norm/mul_2	1599833760275282	4
add_586	1599833760290512	10
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760129326	52
Mul_857	1599833759805422	11
Assign_269	1599833760308263	20
gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/mul_1	1599833760056763	279
bert/encoder/layer_5/attention/self/query/bias/adam_v/read	-1	-1
bert/encoder/layer_0/attention/self/Mul/y	-1	-1
global_norm/L2Loss_8	1599833760270599	5
gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul	1599833760024750	637
Square_28	1599833760276542	4
bert/encoder/layer_8/output/add	1599833759949648	73
Mul_180	1599833759811870	11
Square_185	1599833760279397	11
bert/encoder/layer_11/output/dropout/Cast	1599833759824065	36
bert/encoder/layer_10/attention/output/dropout/mul_1	1599833759968304	73
bert/encoder/layer_11/output/LayerNorm/beta/adam_v	-1	-1
Assign_427	1599833760305715	4
global_norm/L2Loss_75	1599833760175769	13
bert/encoder/layer_8/attention/output/dropout/mul_1	1599833759941522	73
add_229	1599833760303943	4
bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference	1599833759869459	55
bert/encoder/layer_4/intermediate/dense/bias/adam_v/read	-1	-1
add_604	1599833760318777	39
Assign_72	1599833760334262	4
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2	1599833759901780	54
Mul_848	1599833760284914	4
clip_by_global_norm/mul_152	1599833760274754	13
bert/encoder/layer_4/attention/output/dense/kernel/adam_m/read	-1	-1
global_norm/L2Loss_146	1599833760077828	5
Assign_174	1599833760334949	4
bert/encoder/layer_4/attention/output/LayerNorm/moments/variance	1599833759888231	31
bert/encoder/layer_4/attention/output/dropout/GreaterEqual	1599833759822847	38
global_norm/L2Loss_94	1599833760151876	5
bert/encoder/layer_7/attention/self/transpose	1599833759925490	189
gradients/bert/encoder/layer_3/attention/self/Reshape_2_grad/Reshape	-1	-1
Sqrt_2	1599833760310952	9
Square_30	1599833760276818	4
bert/encoder/layer_2/attention/self/query/kernel	-1	-1
truediv_192	1599833760323874	58
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v/read	-1	-1
add_227	1599833760325705	59
Assign_548	1599833760312426	11
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759941800	4
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760138904	5
truediv_153	1599833760323487	4
add_596	1599833760302424	60
bert/encoder/layer_3/attention/output/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_8/intermediate/dense/mul_1_grad/Mul_1	1599833760078706	195
global_step/cond/Switch_1	-1	-1
bert/encoder/layer_6/attention/self/value/kernel/adam_m	-1	-1
Assign_122	1599833760309661	12
sub_113	1599833760333097	8
Square_62	1599833760277043	4
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760041149	66
gradients/cls/predictions/transform/dense/mul_1_grad/Mul_1	1599833760004883	7
Assign_455	1599833760311517	23
Assign_37	1599833760300346	5
Assign_514	1599833760301294	23
bert/encoder/layer_11/intermediate/dense/mul_1	1599833759986125	194
add_123	1599833760283199	4
gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/Pow	1599833759985256	195
Cast_1	1599833759820781	13
bert/encoder/layer_6/output/LayerNorm/gamma	-1	-1
Mul_769	1599833759806401	12
bert/encoder/layer_8/attention/output/dense/kernel/adam_v/read	-1	-1
Assign_287	1599833760309569	40
bert/encoder/layer_5/output/dropout/random_uniform/RandomUniform	1599833759800495	47
bert/encoder/layer_9/intermediate/dense/kernel	-1	-1
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760116813	5
bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference	1599833759955092	55
truediv_44	1599833760324125	21
add_55	1599833760284729	55
Mul_694	1599833759805155	11
bert/encoder/layer_5/attention/self/query/kernel/adam_m/read	-1	-1
add_304	1599833760317702	10
bert/encoder/layer_0/output/LayerNorm/gamma/adam_v/read	-1	-1
Square_10	1599833760278384	4
sub_28	1599833760334017	17
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760205419	51
add_13	1599833760318500	9
bert/encoder/layer_6/intermediate/dense/bias/adam_v	-1	-1
add_310	1599833760297974	9
Square_192	1599833760280059	4
Assign_340	1599833760305608	49
Mul_292	1599833759818918	17
bert/encoder/layer_7/attention/self/value/kernel/adam_v/read	-1	-1
bert/encoder/layer_4/attention/output/dense/bias/adam_m/read	-1	-1
bert/embeddings/Reshape_1/shape	-1	-1
cls/predictions/transform/dense/bias/adam_v	-1	-1
Mul_582	1599833760276715	13
clip_by_global_norm/mul_77	1599833760275799	4
bert/encoder/layer_0/attention/self/transpose_2	1599833759832121	190
clip_by_global_norm/mul_87	1599833760273651	4
bert/encoder/layer_2/attention/self/query/bias	-1	-1
Assign_144	1599833760334134	4
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v/read	-1	-1
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Mul_458	1599833760277255	4
Assign_22	1599833760300103	13
Sqrt_102	1599833760308876	10
sub_117	1599833760333403	4
bert/encoder/layer_2/output/LayerNorm/gamma/adam_v/read	-1	-1
gradients/bert/encoder/layer_6/output/LayerNorm/moments/mean_grad/Tile	1599833760116628	27
gradients/bert/encoder/layer_4/attention/self/transpose_1_grad/transpose	1599833760179283	189
Assign_404	1599833760316511	10
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760018597	74
gradients/cls/predictions/transform/LayerNorm/moments/mean_grad/truediv	1599833760004737	10
Square_44	1599833760276672	4
clip_by_global_norm/mul_123	1599833760273266	4
Mul_653	1599833759814901	10
bert/encoder/layer_6/attention/output/LayerNorm/moments/mean	1599833759914916	31
clip_by_global_norm/mul_198	1599833760275044	12
Mul_207	1599833760282245	12
Sqrt_70	1599833760314420	7
add_20	1599833760318376	4
bert/encoder/layer_2/attention/output/dropout/mul	1599833759826244	51
Mul_421	1599833760277059	4
cls/seq_relationship/output_weights/adam_v/read	-1	-1
bert/encoder/layer_0/attention/self/key/bias/adam_v	-1	-1
Mul_362	1599833760280440	4
Mul_1099	1599833760291670	12
bert/encoder/layer_10/output/dense/BiasAdd	1599833759976298	61
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v	-1	-1
truediv_140	1599833760325273	22
bert/encoder/layer_5/output/LayerNorm/batchnorm/add	1599833759909710	5
bert/encoder/layer_3/attention/self/key/BiasAdd	1599833759871827	60
bert/encoder/layer_8/attention/output/dense/kernel	-1	-1
bert/encoder/layer_11/attention/self/key/bias/read	-1	-1
Assign_492	1599833760336557	4
bert/encoder/layer_10/attention/output/dense/kernel/read	-1	-1
Mul_167	1599833759809848	12
bert/encoder/layer_1/output/dense/bias/adam_m/read	-1	-1
bert/embeddings/position_embeddings/adam_v	-1	-1
truediv_101	1599833760321379	8
sub_48	1599833760337217	60
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul	1599833759888276	39
gradients/bert/encoder/layer_8/attention/self/Mul_grad/Mul	1599833760090059	142
gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1	1599833760046158	241
bert/encoder/Reshape_1	-1	-1
gradients/bert/encoder/layer_7/output/LayerNorm/moments/mean_grad/Tile	1599833760094541	27
gradients/bert/encoder/layer_0/attention/output/dropout/mul_grad/Mul	1599833760262879	73
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760227127	53
gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul	1599833760218805	644
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760049907	69
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760151484	31
bert/encoder/layer_11/attention/output/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760027490	71
gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul	1599833760183635	2220
Assign_361	1599833760305138	12
bert/encoder/layer_5/output/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_0/attention/self/Reshape_grad/Reshape	-1	-1
Assign_192	1599833760333928	15
Mul_536	1599833760282799	4
bert/encoder/layer_2/output/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m	-1	-1
bert/encoder/layer_1/attention/output/dense/kernel/read	-1	-1
Mul_485	1599833760276854	12
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m/read	-1	-1
sub_202	1599833760332144	4
bert/encoder/layer_4/attention/self/query/BiasAdd	1599833759885157	61
bert/encoder/layer_4/attention/self/value/bias/read	-1	-1
add_651	1599833760319480	43
Mul_1078	1599833760291596	16
bert/encoder/layer_6/attention/self/value/kernel/adam_v/read	-1	-1
Mul_514	1599833760282509	4
bert/encoder/layer_6/attention/self/value/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_5/output/dropout/mul_1_grad/Mul	1599833760139270	52
truediv_79	1599833760324329	5
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760041086	27
clip_by_global_norm/mul_117	1599833760275788	4
bert/encoder/layer_5/attention/self/query/bias	-1	-1
mul_69	1599833759813219	13
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_4/output/LayerNorm/beta/adam_v/read	-1	-1
bert/encoder/layer_1/intermediate/dense/kernel	-1	-1
Sqrt_191	1599833760313493	45
add_120	1599833760283003	4
Mul_1082	1599833760280025	4
Mul_108	1599833759808781	11
bert/encoder/layer_1/attention/output/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_1/output/dense/bias/read	-1	-1
Mul_1102	1599833760280031	4
bert/encoder/layer_3/intermediate/dense/Tanh	1599833759879252	207
gradients/AddN_37	1599833760107976	95
bert/encoder/layer_3/intermediate/dense/MatMul	1599833759875200	2195
bert/encoder/layer_3/attention/self/query/bias/read	-1	-1
Mul_84	1599833760284629	4
bert/encoder/layer_8/output/dense/MatMul	1599833759947034	2475
add_649	1599833760291794	60
Assign_501	1599833760341284	14
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760262337	65
bert/encoder/layer_2/attention/self/Reshape_3	-1	-1
Mul_152	1599833760276233	12
mul_397	1599833760329265	4
add_186	1599833760326187	17
bert/encoder/layer_11/attention/output/dense/kernel/adam_v	-1	-1
Assign_489	1599833760336422	4
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_10/intermediate/dense/bias/adam_m/read	-1	-1
add_433	1599833760281858	56
bert/encoder/layer_3/attention/self/MatMul	1599833759872523	159
gradients/bert/encoder/layer_9/output/LayerNorm/moments/mean_grad/truediv	1599833760050386	46
bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2	1599833759936593	53
bert/encoder/layer_0/attention/self/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760173218	72
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760107100	73
Mul_415	1599833759819840	12
gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul_1	1599833760082166	2469
mul_876	1599833760331689	39
gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul_1	1599833760236951	2467
bert/encoder/layer_7/intermediate/dense/Tanh	1599833759932775	208
Mul_608	1599833759814973	4
truediv_33	1599833760322285	5
Square_7	1599833760278358	12
add_347	1599833760317431	12
add_645	1599833760319183	8
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
Assign_204	1599833760337587	4
Square_188	1599833760279839	4
gradients/bert/encoder/layer_8/output/dropout/mul_1_grad/Mul	1599833760072950	51
bert/encoder/layer_1/output/LayerNorm/batchnorm/mul	1599833759856157	38
bert/encoder/layer_7/attention/self/query/bias/adam_v/read	-1	-1
clip_by_global_norm/mul_110	1599833760275628	4
gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul	1599833760180853	642
bert/encoder/layer_5/attention/self/value/BiasAdd	1599833759898657	60
bert/encoder/layer_6/attention/self/transpose_1	1599833759912298	190
add_665	1599833760319361	7
bert/encoder/layer_5/attention/output/dense/MatMul	1599833759900686	634
gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul	1599833760153905	160
bert/encoder/layer_4/intermediate/dense/Tanh	1599833759892630	206
bert/embeddings/LayerNorm/batchnorm/add	1599833759828521	5
bert/encoder/layer_6/output/LayerNorm/beta/adam_v	-1	-1
bert/encoder/layer_5/attention/self/query/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_11/intermediate/dense/Tanh_grad/TanhGrad	1599833760012078	274
bert/encoder/layer_7/output/LayerNorm/batchnorm/sub	1599833759936648	54
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m/read	-1	-1
cls/seq_relationship/BiasAdd	1599833759990655	4
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul	1599833760265307	141
bert/encoder/layer_6/output/LayerNorm/gamma/adam_m	-1	-1
Assign_296	1599833760308847	15
bert/encoder/layer_1/attention/output/dropout/GreaterEqual	1599833759822486	39
Assign_544	1599833760301449	20
add_276	1599833760297296	9
bert/encoder/layer_4/attention/output/dropout/mul_1	1599833759887994	73
bert/encoder/layer_11/output/dropout/GreaterEqual	1599833759822084	38
bert/encoder/layer_10/attention/self/key/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760227457	53
Assign_286	1599833760298765	46
Assign_102	1599833760335018	10
bert/encoder/layer_2/attention/self/value/kernel/adam_v	-1	-1
add_583	1599833760301270	22
cls/predictions/transform/LayerNorm/batchnorm/add_1	1599833759991041	11
bert/encoder/layer_11/intermediate/dense/bias/adam_v	-1	-1
gradients/AddN_22	1599833760063048	69
Sqrt_89	1599833760308241	16
bert/encoder/layer_5/attention/self/Softmax	1599833759899683	322
PolynomialDecay	-1	-1
mul_881	1599833760327719	4
add_324	1599833760317367	7
Mul_470	1599833759819591	11
Sqrt_45	1599833760309416	10
bert/encoder/layer_1/attention/self/Mul	1599833759845873	99
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul	1599833760110563	141
Mul_276	1599833760277381	4
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
bert/encoder/layer_1/output/dense/bias/adam_m	-1	-1
Assign_275	1599833760309219	21
global_norm/L2Loss_160	1599833760062493	9
global_norm/L2Loss_201	1599833760004632	4
clip_by_global_norm/mul_59	1599833760273859	4
add_460	1599833760281605	17
bert/encoder/layer_2/attention/self/key/kernel/adam_m	-1	-1
clip_by_global_norm/mul_169	1599833760274955	4
Mul_265	1599833760276399	4
cls/predictions/transform/LayerNorm/moments/mean	1599833759990756	6
gradients/bert/encoder/layer_6/attention/self/dropout/mul_1_grad/Mul	1599833760132218	98
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760005979	53
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760174042	53
Mul_291	1599833760277001	12
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
bert/encoder/layer_11/intermediate/dense/BiasAdd	1599833759984469	222
bert/encoder/layer_6/attention/output/LayerNorm/beta/read	-1	-1
global_norm/L2Loss_176	1599833760040349	9
bert/encoder/layer_10/intermediate/dense/bias/read	-1	-1
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760182789	54
mul_773	1599833760329398	4
mul_757	1599833759815400	12
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760094921	73
bert/encoder/layer_5/output/LayerNorm/gamma/adam_m	-1	-1
Mul_443	1599833759820202	44
bert/encoder/layer_2/attention/self/dropout/Cast	1599833759825751	67
Assign_109	1599833760299132	4
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub	1599833759834866	55
bert/encoder/layer_6/attention/self/dropout/mul	1599833759827489	99
bert/encoder/layer_11/attention/output/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760018907	54
mul_945	1599833760327985	4
Assign_186	1599833760334739	4
bert/encoder/layer_8/output/LayerNorm/batchnorm/add_1	1599833759950084	73
mul_80	1599833760327475	4
gradients/bert/encoder/layer_1/attention/self/Reshape_1_grad/Reshape	-1	-1
clip_by_global_norm/mul_197	1599833760275265	4
bert/encoder/layer_7/attention/self/Reshape_1	-1	-1
sub_65	1599833760329790	11
gradients/bert/encoder/layer_9/attention/self/dropout/mul_grad/Mul	1599833760066133	142
Assign_250	1599833760297892	11
gradients/bert/encoder/layer_10/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760035269	108
Assign_578	1599833760313747	10
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760173898	47
Assign_314	1599833760315852	10
Assign_380	1599833760306603	11
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760107849	51
gradients/cls/predictions/MatMul_grad/MatMul	1599833759997536	3571
bert/encoder/layer_7/attention/self/query/MatMul	1599833759923407	630
bert/encoder/layer_6/attention/self/MatMul_1	1599833759913636	244
bert/encoder/layer_6/attention/self/query/kernel/read	-1	-1
sub_111	1599833760333114	10
gradients/AddN_8	1599833760018837	69
Sqrt_194	1599833760312877	12
Assign_289	1599833760296405	13
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760084953	74
truediv_99	1599833760321810	8
bert/encoder/layer_2/output/dense/BiasAdd	1599833759869215	61
gradients/bert/encoder/layer_10/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760048152	54
sub_2	1599833760337014	7
bert/encoder/layer_6/output/LayerNorm/gamma/read	-1	-1
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760204541	73
truediv_17	1599833760323087	5
add_473	1599833760320622	4
bert/encoder/layer_11/intermediate/dense/kernel/read	-1	-1
truediv_114	1599833760324985	62
Mul_909	1599833759808160	11
Assign_565	1599833760302590	11
Sqrt_20	1599833760313773	8
Mul_172	1599833759811088	40
mul_784	1599833760329070	8
add_10	1599833760328366	6
Mul_717	1599833759805601	11
Mul_254	1599833760277507	4
global_norm/L2Loss_126	1599833760107698	5
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Mul	1599833760004674	12
bert/encoder/layer_3/attention/self/key/kernel/adam_m/read	-1	-1
bert/encoder/layer_8/output/LayerNorm/beta/adam_m	-1	-1
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
Sqrt_131	1599833760306631	9
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760204744	69
add_251	1599833760282417	17
Mul_976	1599833760291393	4
Square_150	1599833760278784	4
sub_7	1599833760331557	4
Assign_368	1599833760306552	12
gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul	1599833760130340	639
add_107	1599833760297347	12
clip_by_global_norm/mul_126	1599833760275976	4
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/add_1	1599833759955419	73
bert/encoder/layer_0/attention/self/query/bias/adam_v	-1	-1
bert/encoder/layer_3/attention/self/key/kernel/adam_v	-1	-1
Assign_231	1599833760334745	4
clip_by_global_norm/mul_204	1599833760275202	5
Mul_462	1599833759804843	17
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760129982	73
gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Sum	1599833760004583	15
add_527	1599833760318598	4
Assign_199	1599833760303948	4
bert/embeddings/one_hot/off_value	-1	-1
bert/encoder/layer_11/attention/output/dense/BiasAdd	1599833759981631	61
Assign_386	1599833760315429	11
cls/predictions/transform/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_4/attention/self/dropout/Cast	1599833759825679	70
Mul_1030	1599833760291522	9
mul_606	1599833759816689	45
gradients/bert/encoder/layer_0/output/LayerNorm/moments/mean_grad/Tile	1599833760249285	28
Mul_1091	1599833759820430	4
Mul_748	1599833759805958	10
add	-1	-1
global_norm/L2Loss_143	1599833760084721	32
add_333	1599833760297554	60
bert/encoder/layer_0/attention/self/dropout/mul_1	1599833759833136	142
gradients/bert/encoder/layer_3/intermediate/dense/mul_3_grad/Mul	1599833760188338	280
add_468	1599833760305011	21
bert/encoder/layer_11/attention/output/dense/bias	-1	-1
bert/encoder/layer_3/output/dropout/mul	1599833759826827	51
truediv_178	1599833760323357	59
sub_56	1599833760337283	17
add_639	1599833760328132	17
gradients/bert/encoder/layer_5/attention/self/dropout/mul_1_grad/Mul	1599833760154295	99
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760071979	74
bert/encoder/layer_10/attention/self/dropout/mul_1	1599833759967030	141
gradients/bert/encoder/layer_3/attention/self/transpose_3_grad/transpose	1599833760197946	189
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760196073	27
add_559	1599833760301869	9
bert/encoder/layer_4/intermediate/dense/add_1	1599833759892838	195
Assign_595	1599833760302321	12
Assign_112	1599833760296903	14
bert/encoder/layer_1/attention/self/query/kernel/adam_v/read	-1	-1
Assign_486	1599833760336256	4
bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2	1599833759869677	53
bert/encoder/layer_5/intermediate/dense/mul	1599833759905146	195
bert/encoder/layer_11/attention/output/LayerNorm/moments/variance	1599833759981931	31
bert/encoder/layer_4/output/dropout/mul_1	1599833759896049	73
bert/encoder/layer_11/attention/output/dense/MatMul	1599833759980999	630
Sqrt_132	1599833760315683	4
bert/encoder/layer_10/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_11/output/dropout/random_uniform/RandomUniform	1599833759800303	46
add_575	1599833760285096	4458
bert/encoder/layer_4/output/LayerNorm/moments/variance	1599833759896287	32
Sqrt_196	1599833760313581	4
Assign_155	1599833760314215	3
bert/encoder/layer_4/attention/self/value/kernel/adam_v	-1	-1
global_norm/L2Loss_78	1599833760173946	5
mul_908	1599833760331893	12
Assign_431	1599833760306977	41
Assign_23	1599833760310722	13
global_norm/L2Loss_41	1599833760222642	14
gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul_1	1599833760159386	614
Mul_637	1599833760295260	17
Square_125	1599833760281412	4
Mul_28	1599833760278352	4
gradients/bert/encoder/layer_1/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760247184	54
mul_1058	1599833760328251	4
Mul_1062	1599833760291903	4
bert/encoder/layer_2/intermediate/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760050529	53
bert/encoder/layer_1/intermediate/dense/bias/adam_m	-1	-1
truediv_73	1599833760324904	4
Assign_132	1599833760334371	4
clip_by_global_norm/mul_8	1599833760274419	12
bert/encoder/layer_10/attention/output/dense/MatMul	1599833759967607	634
bert/encoder/layer_1/attention/self/key/bias/read	-1	-1
bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference	1599833759856055	55
gradients/bert/encoder/layer_2/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760223761	55
bert/encoder/layer_7/attention/self/query/kernel/adam_m	-1	-1
Mul_1023	1599833760279685	4
gradients/bert/encoder/layer_5/attention/self/Reshape_1_grad/Reshape	-1	-1
edge_3486_Reshape_1@@MemcpyHtoD	1599833759821946	12
gradients/bert/encoder/layer_8/attention/self/transpose_grad/transpose	1599833760090691	189
gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/Pow	1599833759918338	195
bert/encoder/layer_5/intermediate/dense/mul_2	1599833759906424	195
Assign_451	1599833760300568	4
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760063175	53
clip_by_global_norm/mul_122	1599833760275962	12
add_622	1599833760291018	22
truediv_134	1599833760324959	21
Sqrt_16	1599833760311101	5
global_norm/L2Loss_10	1599833760266824	5
Sqrt_171	1599833760311973	15
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
bert/encoder/layer_0/intermediate/dense/Pow/y	-1	-1
gradients/bert/encoder/layer_6/output/LayerNorm/moments/variance_grad/Tile	1599833760116820	27
bert/encoder/layer_5/intermediate/dense/kernel/adam_m	-1	-1
mul_1010	1599833760328014	4
add_247	1599833760328989	20
bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference	1599833759861398	56
add_668	1599833760319525	4
bert/encoder/layer_9/attention/self/key/kernel	-1	-1
bert/encoder/mul	1599833759798036	34
sub_128	1599833760334077	56
Assign_408	1599833760337723	10
add_37	1599833760318475	12
bert/encoder/layer_9/attention/self/transpose_3	1599833759954025	189
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760062755	51
bert/encoder/layer_8/intermediate/dense/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
add_158	1599833760298659	9
bert/encoder/layer_10/intermediate/dense/bias	-1	-1
Mul_503	1599833759811521	10
gradients/bert/encoder/layer_5/output/LayerNorm/moments/mean_grad/truediv	1599833760138849	47
bert/embeddings/Reshape_4/shape	-1	-1
bert/encoder/layer_4/attention/self/key/bias/adam_v	-1	-1
gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/mul	1599833759865182	196
clip_by_global_norm/mul_30	1599833760273510	4
bert/encoder/layer_7/attention/output/dense/bias/adam_v	-1	-1
gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul	1599833760022252	639
Mul_917	1599833760285082	12
bert/encoder/layer_5/intermediate/dense/bias/adam_v	-1	-1
bert/embeddings/one_hot/depth	-1	-1
gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul	1599833760168056	2500
bert/encoder/layer_2/output/LayerNorm/batchnorm/add	1599833759869550	5
Mul_495	1599833759804904	18
Assign_61	1599833760303292	12
Mul_29	1599833759807771	11
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
Square_124	1599833760281217	4
Assign_116	1599833760308796	12
gradients/bert/encoder/layer_3/attention/output/dropout/mul_1_grad/Mul	1599833760196434	52
Assign_397	1599833760305346	4
gradients/AddN_2	1599833760004904	15
add_266	1599833760297815	12
bert/encoder/layer_11/output/LayerNorm/gamma/read	-1	-1
Assign_351	1599833760342049	14
gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1	1599833760200993	241
Cast_3	1599833759825384	6
Assign_90	1599833760334529	9
gradients/bert/pooler/dense/Tanh_grad/TanhGrad	1599833759990840	5
bert/encoder/layer_4/attention/self/Reshape_3	-1	-1
sub_91	1599833760330600	8
Assign_179	1599833760314292	19
clip_by_global_norm/mul_170	1599833760274734	12
add_223	1599833760316765	8
Sqrt_146	1599833760316027	6
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760050143	56
Mul_676	1599833760295017	9
Mul_824	1599833760279334	4
bert/encoder/layer_6/attention/output/dense/kernel/read	-1	-1
bert/encoder/layer_3/attention/self/query/MatMul	1599833759869863	634
gradients/bert/encoder/layer_0/output/dropout/mul_grad/Mul	1599833760249890	73
Mul_753	1599833759806369	18
truediv_39	1599833760321788	10
Mul_1049	1599833759806718	11
Assign_251	1599833760308643	11
Square_79	1599833760280620	39
gradients/bert/encoder/layer_3/attention/output/dropout/mul_grad/Mul	1599833760196487	73
Mul_484	1599833759819620	17
bert/encoder/layer_6/intermediate/dense/bias/adam_m	-1	-1
gradients/bert/encoder/layer_7/attention/self/Reshape_3_grad/Reshape	-1	-1
bert/encoder/layer_8/output/LayerNorm/batchnorm/Rsqrt	1599833759949851	4
bert/encoder/layer_11/attention/output/dense/bias/read	-1	-1
Sqrt_21	1599833760313915	16
truediv_197	1599833760323934	5
bert/pooler/dense/MatMul	1599833759990515	26
bert/encoder/layer_0/attention/self/Reshape_2	-1	-1
Square_161	1599833760278917	39
Sqrt_115	1599833760307048	4
bert/pooler/strided_slice/stack_1	-1	-1
add_393	1599833760281781	4
bert/embeddings/Reshape_1	-1	-1
bert/encoder/layer_3/attention/self/key/bias/adam_m	-1	-1
Square_108	1599833760280719	4
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760084900	52
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m	-1	-1
Mul_76	1599833759808288	11
cls/predictions/Neg	1599833759997369	5
add_648	1599833760319340	8
add_364	1599833760296535	13
Mul_487	1599833760282470	13
bert/encoder/layer_2/attention/self/query/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_10/attention/self/Reshape_1_grad/Reshape	-1	-1
Mul_229	1599833760283041	12
Mul_141	1599833760280089	11
bert/encoder/layer_2/attention/output/dense/bias/adam_m/read	-1	-1
gradients/AddN_53	1599833760160611	69
bert/encoder/layer_9/attention/output/dropout/mul_1	1599833759954909	73
global_norm/L2Loss_155	1599833760065235	13
Square_121	1599833760281394	11
bert/encoder/layer_10/attention/self/query/bias/adam_v	-1	-1
Sqrt_145	1599833760315594	45
clip_by_global_norm/IsFinite	1599833760273237	4
mul_348	1599833759814473	46
bert/encoder/layer_8/intermediate/dense/bias/adam_v	-1	-1
sub_86	1599833760337502	21
mul_822	1599833760331763	12
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760129180	74
bert/encoder/layer_0/attention/self/value/MatMul	1599833759830921	630
bert/encoder/layer_0/output/LayerNorm/beta/adam_v/read	-1	-1
add_64	1599833760318401	39
truediv_66	1599833760321187	62
gradients/bert/encoder/layer_0/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760262956	56
clip_by_global_norm/mul_22	1599833760275318	13
bert/encoder/layer_4/attention/output/dropout/mul	1599833759826986	51
Mul_1087	1599833760279615	4
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1	1599833759834735	73
add_611	1599833760319126	6
gradients/cls/seq_relationship/MatMul_grad/MatMul_1	1599833759990791	22
bert/encoder/layer_0/attention/self/dropout/random_uniform	-1	-1
mul_961	1599833759816529	45
add_561	1599833760285050	17
bert/encoder/layer_1/output/dense/kernel/adam_m	-1	-1
bert/encoder/layer_0/attention/self/key/bias/adam_v/read	-1	-1
truediv_38	1599833760321335	18
truediv_49	1599833760321133	5
gradients/bert/encoder/layer_10/attention/self/transpose_2_grad/transpose	1599833760043800	189
Sqrt_50	1599833760309470	10
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760094266	52
Sqrt_159	1599833760311591	44
Sqrt_122	1599833760306528	9
mul_306	1599833760332792	12
global_norm/L2Loss_89	1599833760156250	15
gradients/bert/encoder/layer_11/output/LayerNorm/moments/variance_grad/Tile	1599833760006226	26
Sqrt_34	1599833760309250	10
sub_35	1599833760330795	7
gradients/bert/encoder/layer_4/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760179478	55
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v	-1	-1
add_86	1599833760282119	4
bert/encoder/layer_8/attention/output/dropout/GreaterEqual	1599833759822203	40
add_322	1599833760282503	4
cls/seq_relationship/MatMul	1599833759990621	25
bert/encoder/layer_8/attention/output/dense/BiasAdd	1599833759941459	61
bert/encoder/layer_3/attention/self/value/kernel/adam_v	-1	-1
bert/encoder/layer_0/attention/self/value/BiasAdd	1599833759831677	61
gradients/bert/encoder/layer_5/attention/output/dropout/mul_grad/Mul	1599833760152303	72
add_16	1599833760300523	4
add_481	1599833760294466	8
gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul	1599833760095278	2219
Assign_545	1599833760312137	20
Square_61	1599833760276742	4
bert/encoder/layer_9/attention/self/value/bias/adam_m	-1	-1
Square_11	1599833760278498	13
Mul_588	1599833760280713	4
global_norm/L2Loss_129	1599833760100541	33
Assign_491	1599833760311897	10
bert/encoder/layer_5/attention/self/value/bias/adam_v/read	-1	-1
bert/encoder/layer_4/output/dropout/random_uniform/RandomUniform	1599833759800831	47
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759888271	4
bert/encoder/layer_5/output/LayerNorm/gamma/read	-1	-1
add_621	1599833760319002	8
Mul_57	1599833760284585	13
Square_131	1599833760276093	4
Mul_191	1599833759811683	10
mul_112	1599833760327555	4
bert/encoder/layer_5/output/LayerNorm/gamma	-1	-1
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760019398	72
truediv_34	1599833760321271	62
clip_by_global_norm/mul_161	1599833760274875	5
bert/encoder/layer_10/attention/output/dense/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760085405	33
Sqrt_151	1599833760311495	16
Mul_103	1599833759811057	4
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760085160	32
bert/encoder/layer_2/attention/self/value/bias/read	-1	-1
bert/encoder/layer_3/output/LayerNorm/batchnorm/sub	1599833759883126	53
add_236	1599833760320061	4
gradients/bert/encoder/layer_1/attention/self/transpose_3_grad/transpose	1599833760242179	189
Mul_710	1599833759805918	4
bert/encoder/layer_1/attention/output/dropout/mul_1	1599833759847818	73
add_532	1599833760284908	4
cls/predictions/output_bias/adam_m/read	-1	-1
Mul_1089	1599833760290931	4
Mul_371	1599833759819070	11
add_100	1599833760282130	4
Square_152	1599833760279339	4
gradients/bert/encoder/layer_8/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760086056	55
Mul_33	1599833760278539	12
bert/encoder/layer_7/output/LayerNorm/moments/mean	1599833759936345	32
add_184	1599833760297691	17
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760195334	73
Square_86	1599833760276829	4
sub_54	1599833760334702	17
gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul	1599833760023835	232
gradients/bert/encoder/layer_10/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760044279	54
bert/encoder/layer_11/output/LayerNorm/moments/variance	1599833759989977	31
gradients/AddN_83	1599833760256080	367
bert/encoder/layer_7/attention/output/dense/kernel/read	-1	-1
global_norm/L2Loss_99	1599833760138499	5
Square_175	1599833760279729	40
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
mul_564	1599833760333134	16
Mul_981	1599833760290336	17
gradients/bert/encoder/layer_11/attention/self/transpose_3_grad/transpose	1599833760021033	190
bert/encoder/layer_5/attention/self/MatMul_1	1599833759900250	244
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/add_1	1599833759861727	73
add_300	1599833760317524	16
Assign_375	1599833760339826	7
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760195911	33
bert/embeddings/word_embeddings/read	-1	-1
add_61	1599833760318576	4
gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Neg	1599833760004571	10
bert/encoder/layer_9/attention/output/dense/kernel	-1	-1
bert/encoder/layer_5/attention/self/Reshape_3	-1	-1
Assign_477	1599833760341199	41
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul_1	1599833759926780	100
gradients/AddN_4	1599833760005851	70
mul_918	1599833759815634	18
Mul_482	1599833760282954	4
Mul_5	1599833760277984	361
bert/encoder/layer_8/attention/self/key/kernel/adam_v/read	-1	-1
bert/encoder/layer_6/attention/self/dropout/random_uniform/RandomUniform	1599833759801615	83
add_323	1599833760297530	9
clip_by_global_norm/mul_151	1599833760274629	4
add_270	1599833760319900	7
bert/encoder/layer_7/attention/self/value/kernel/adam_m/read	-1	-1
bert/encoder/layer_6/output/add	1599833759922898	73
Sqrt_204	1599833760313637	8
bert/encoder/layer_8/attention/self/dropout/random_uniform/mul	-1	-1
gradients/AddN_66	1599833760204329	119
bert/encoder/layer_5/attention/output/LayerNorm/gamma/read	-1	-1
add_327	1599833760317849	43
bert/encoder/layer_6/attention/self/Mul	1599833759912840	99
Sqrt_158	1599833760311184	4
gradients/bert/encoder/layer_10/attention/self/dropout/mul_grad/Mul	1599833760043990	142
Sqrt_172	1599833760312263	11
ConstantFolding/gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/truediv_recip	-1	-1
gradients/bert/encoder/layer_6/output/dropout/mul_grad/Mul	1599833760117234	73
bert/encoder/layer_2/attention/self/value/kernel/adam_m	-1	-1
Mul_460	1599833760282943	4
add_337	1599833760298076	12
add_285	1599833760282739	4
Assign_187	1599833760297790	12
clip_by_global_norm/mul_178	1599833760274815	39
Mul_825	1599833759805365	11
Mul_732	1599833760276098	13
gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/mul	1599833759838374	195
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760218447	72
Assign_241	1599833760297323	12
Mul_731	1599833759805630	18
global_norm/L2Loss_184	1599833760027345	5
gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul	1599833760203043	637
mul_983	1599833760331933	12
Assign_473	1599833760311567	11
Assign_440	1599833760316046	11
Mul_351	1599833760276132	4
Mul_691	1599833760281915	39
bert/encoder/layer_11/attention/self/value/bias/adam_v/read	-1	-1
bert/encoder/layer_7/output/dense/bias/adam_v/read	-1	-1
Mul_270	1599833760277188	39
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul	1599833760243196	141
bert/encoder/layer_4/output/LayerNorm/beta	-1	-1
add_434	1599833760296300	56
add_209	1599833760296460	9
bert/encoder/layer_8/attention/output/add	1599833759941596	73
gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul_1	1599833760115224	613
clip_by_global_norm/mul_100	1599833760273581	4
Assign_357	1599833760340051	14
add_147	1599833760283261	9
bert/encoder/layer_10/output/LayerNorm/gamma/adam_m	-1	-1
bert/embeddings/word_embeddings/adam_v	-1	-1
gradients/bert/encoder/layer_7/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760113202	55
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/sub	1599833760266847	99
mul_897	1599833760331729	12
global_norm/L2Loss_149	1599833760071619	15
bert/encoder/layer_4/attention/self/key/bias/adam_m/read	-1	-1
Assign_142	1599833760303696	41
truediv_200	1599833760323835	17
add_624	1599833760319142	15
add_535	1599833760289723	9
add_315	1599833760326470	20
clip_by_global_norm/mul_171	1599833760274864	4
Assign_60	1599833760337057	11
bert/encoder/layer_10/attention/self/key/MatMul	1599833759964186	634
Sqrt_105	1599833760316235	12
sub_183	1599833760332027	8
mul_703	1599833759816363	73
global_norm/L2Loss_11	1599833760264276	12
gradients/bert/encoder/layer_1/attention/self/dropout/mul_grad/Mul	1599833760243053	142
bert/pooler/dense/bias/adam_v/read	-1	-1
Mul_10	1599833759807373	11
Mul_1076	1599833760279879	11
bert/encoder/layer_9/attention/output/LayerNorm/beta/read	-1	-1
Mul_88	1599833759811250	39
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m/read	-1	-1
bert/encoder/Reshape_13	-1	-1
add_97	1599833760296094	17
add_135	1599833760317564	9
add_507	1599833760284919	17
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760138101	71
add_536	1599833760300896	4
Assign_124	1599833760298632	20
sub_79	1599833760332902	8
bert/encoder/layer_4/output/LayerNorm/moments/mean	1599833759896197	32
bert/encoder/layer_11/intermediate/dense/mul_3	1599833759986922	279
global_norm/L2Loss_80	1599833760173029	9
bert/encoder/layer_7/intermediate/dense/MatMul	1599833759928724	2193
Mul_299	1599833760282920	4
Square_24	1599833760276531	4
bert/encoder/layer_10/output/LayerNorm/gamma/adam_v/read	-1	-1
clip_by_global_norm/mul_15	1599833760274532	4
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760271093	69
mul_768	1599833760329174	7
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul	1599833760132652	141
bert/embeddings/Reshape_2	-1	-1
add_680	1599833760291914	8
bert/encoder/layer_9/attention/self/value/bias/read	-1	-1
bert/encoder/layer_4/output/LayerNorm/gamma	-1	-1
Square_103	1599833760280771	11
Mul_885	1599833760290095	19
bert/encoder/layer_6/attention/self/key/kernel/adam_v/read	-1	-1
mul_1100	1599833760328274	5
gradients/AddN_43	1599833760129420	69
clip_by_global_norm/mul_119	1599833760273261	4
sub_103	1599833760330658	8
bert/encoder/layer_5/output/dense/bias/adam_v/read	-1	-1
Assign_21	1599833760340925	14
gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul	1599833760269322	636
bert/encoder/layer_4/attention/self/value/bias	-1	-1
add_584	1599833760318860	12
sub_203	1599833760332263	8
global_norm/L2Loss_137	1599833760089940	15
bert/encoder/layer_10/attention/self/key/bias	-1	-1
Mul_788	1599833760294572	39
gradients/bert/encoder/layer_5/output/dense/BiasAdd_grad/BiasAddGrad	1599833760139401	55
gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/mul_1	1599833760255798	281
Assign_367	1599833760295713	13
cls/predictions/transform/LayerNorm/batchnorm/mul_1	1599833759991001	12
clip_by_global_norm/mul_183	1599833760274986	4
Square_170	1599833760279235	4
clip_by_global_norm/mul_144	1599833760273299	39
clip_by_global_norm/mul_75	1599833760275493	4
Mul_941	1599833759809366	11
bert/encoder/layer_4/output/dense/MatMul	1599833759893511	2474
Mul_93	1599833760278751	4
mul_586	1599833760330249	16
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m	-1	-1
Mul_283	1599833760292537	4
gradients/bert/encoder/layer_9/attention/self/Reshape_1_grad/Reshape	-1	-1
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul	1599833759968587	39
mul_176	1599833759814597	45
mul_483	1599833760326452	8
gradients/cls/predictions/Neg_grad/Neg	1599833759828509	5
mul_14	1599833759812137	11
bert/encoder/layer_9/intermediate/dense/add_1	1599833759959758	197
gradients/AddN_26	1599833760072853	96
bert/encoder/layer_6/attention/self/value/bias/adam_m/read	-1	-1
bert/encoder/layer_11/attention/self/value/bias/adam_m/read	-1	-1
add_619	1599833760290671	9
Sqrt_182	1599833760312405	9
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760071926	51
gradients/bert/encoder/layer_9/attention/output/dropout/mul_grad/Mul	1599833760063834	73
bert/encoder/layer_8/attention/output/LayerNorm/moments/variance	1599833759941761	32
Assign_128	1599833760309711	12
mul_214	1599833760326303	8
Assign_556	1599833760301475	19
bert/encoder/layer_4/attention/self/key/bias/read	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760160171	71
Sqrt_63	1599833760314320	45
Sqrt_152	1599833760312021	11
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul	1599833759848101	39
gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul	1599833760152435	637
Assign_255	1599833760341775	13
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760049978	53
bert/encoder/layer_3/output/dense/bias/adam_m/read	-1	-1
Sqrt_88	1599833760309141	9
truediv_189	1599833760323816	5
add_245	1599833760304723	21
bert/encoder/layer_5/attention/self/key/kernel/read	-1	-1
gradients/AddN_25	1599833760072219	68
global_norm/L2Loss_173	1599833760040865	5
add_89	1599833760292155	22
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759834689	4
mul_569	1599833760329293	10
global_norm/L2Loss_79	1599833760173114	32
bert/encoder/layer_5/output/LayerNorm/gamma/adam_m/read	-1	-1
Mul_201	1599833759810535	4
add_533	1599833760300641	4
Square_153	1599833760279456	11
bert/encoder/layer_10/attention/output/dropout/random_uniform/RandomUniform	1599833759801167	46
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760006096	65
global_norm/L2Loss_154	1599833760067837	5
global_norm/L2Loss_165	1599833760049471	15
add_127	1599833760299126	4
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760085705	51
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760204467	73
gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul	1599833760041826	638
bert/encoder/layer_0/attention/output/LayerNorm/gamma	-1	-1
add_606	1599833760290306	9
Mul_656	1599833759816750	16
add_244	1599833760293513	21
mul_327	1599833759813386	19
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/sub	1599833760200503	98
bert/encoder/layer_8/attention/self/key/bias/adam_m/read	-1	-1
Mul_798	1599833759807124	4
bert/encoder/layer_4/attention/self/Softmax	1599833759886306	323
add_546	1599833760301140	9
bert/encoder/layer_10/intermediate/dense/mul	1599833759972069	194
gradients/bert/encoder/layer_8/attention/self/transpose_3_grad/transpose	1599833760087394	189
bert/encoder/layer_10/attention/self/value/bias/adam_m/read	-1	-1
Square_16	1599833760278745	4
gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/mul	1599833759851777	194
add_334	1599833760317387	42
gradients/bert/encoder/layer_9/output/dropout/mul_grad/Mul	1599833760050860	73
gradients/AddN_44	1599833760130057	95
Mul_727	1599833760281428	4
truediv_148	1599833760325357	5
gradients/cls/predictions/mul_1_grad/Mul_1	1599833759828465	4
Mul_140	1599833759811602	17
global_norm/L2Loss_48	1599833760217329	9
gradients/bert/encoder/layer_11/output/LayerNorm/moments/mean_grad/Tile	1599833760006034	27
mul_865	1599833760331782	40
Mul_385	1599833760292751	4
bert/encoder/layer_2/output/LayerNorm/beta/adam_m	-1	-1
Mul_186	1599833760282147	39
clip_by_global_norm/mul_116	1599833760273354	4
global_norm/L2Loss_103	1599833760138083	16
Mul_272	1599833760282822	39
bert/embeddings/LayerNorm/moments/mean	1599833759827104	32
clip_by_global_norm/mul_115	1599833760275943	4
gradients/bert/encoder/layer_11/intermediate/dense/mul_3_grad/Mul	1599833760011481	279
Mul_13	1599833760291987	9
mul_123	1599833760332659	15
gradients/AddN_50	1599833760151517	68
Assign_163	1599833760298305	4
bert/encoder/layer_6/output/dense/kernel	-1	-1
Assign_276	1599833760334140	4
bert/encoder/layer_1/intermediate/dense/add_1	1599833759852659	195
Assign_576	1599833760337035	10
Square_18	1599833760278724	4
bert/encoder/layer_9/attention/output/dense/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760151225	51
Assign_277	1599833760296382	13
bert/encoder/layer_5/intermediate/dense/MatMul	1599833759901966	2192
Assign_239	1599833760314836	41
bert/encoder/layer_5/attention/self/Reshape_2	-1	-1
bert/encoder/layer_9/attention/output/dropout/random_uniform	-1	-1
Mul_99	1599833759810465	40
bert/pooler/dense/kernel	-1	-1
add_572	1599833760290460	9
Assign_540	1599833760336764	4
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760019202	5
gradients/bert/encoder/layer_5/attention/self/transpose_1_grad/transpose	1599833760157176	189
Sqrt_35	1599833760309611	4
bert/encoder/layer_3/output/LayerNorm/beta/adam_m/read	-1	-1
clip_by_global_norm/mul_79	1599833760275498	4
sub_97	1599833760329972	8
bert/encoder/layer_5/attention/self/value/kernel/adam_m/read	-1	-1
bert/encoder/layer_4/attention/self/query/kernel	-1	-1
gradients/bert/encoder/layer_8/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760088567	55
sub_115	1599833760333634	4
bert/encoder/layer_8/intermediate/dense/kernel/adam_m	-1	-1
bert/encoder/layer_10/attention/self/key/kernel/read	-1	-1
gradients/bert/encoder/layer_2/intermediate/dense/mul_3_grad/Mul_1	1599833759864592	195
bert/encoder/layer_8/output/dropout/random_uniform/mul	-1	-1
Mul_319	1599833760277245	4
bert/encoder/layer_7/attention/output/dropout/random_uniform/RandomUniform	1599833759800976	46
mul_42	1599833760327469	4
bert/encoder/layer_2/attention/self/value/kernel/read	-1	-1
Sqrt_101	1599833760308423	12
clip_by_global_norm/mul_201	1599833760275271	4
bert/encoder/layer_5/intermediate/dense/mul_3	1599833759906621	280
Square_107	1599833760276729	12
gradients/bert/encoder/layer_10/output/dropout/mul_1_grad/Mul	1599833760028666	51
bert/encoder/layer_7/attention/self/query/kernel/adam_v/read	-1	-1
bert/encoder/layer_2/output/dense/kernel/adam_m	-1	-1
global_norm/L2Loss_69	1599833760182118	15
sub_171	1599833760331870	4
bert/encoder/layer_3/attention/self/value/bias/adam_v/read	-1	-1
sub_29	1599833760330083	4
sub_110	1599833760333076	7
bert/encoder/layer_1/output/dense/kernel/adam_v/read	-1	-1
truediv_65	1599833760320925	5
add_690	1599833760302822	10
gradients/cls/seq_relationship/mul_grad/Mul_1	1599833759821993	4
bert/encoder/layer_11/output/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_3/attention/output/dropout/mul_1	1599833759874618	73
bert/encoder/layer_7/attention/self/transpose_2	1599833759925871	189
Sqrt_52	1599833760315176	8
add_354	1599833760320133	16
Mul_125	1599833760280108	4
bert/embeddings/LayerNorm/batchnorm/mul_1	1599833759829161	73
add_91	1599833760319618	15
sub_195	1599833760332224	8
Assign_536	1599833760312088	11
add_429	1599833760320686	9
cls/predictions/transform/LayerNorm/beta	-1	-1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760107902	72
gradients/AddN_86	1599833760262728	96
truediv_116	1599833760321030	5
bert/encoder/layer_3/output/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_9/attention/output/dense/kernel/adam_v	-1	-1
bert/encoder/layer_7/attention/self/value/bias/read	-1	-1
gradients/AddN_80	1599833760248526	119
Square_58	1599833760277239	4
add_1	1599833760004668	4
clip_by_global_norm/mul_53	1599833760275672	4
Mul_616	1599833760294903	39
bert/encoder/layer_6/attention/self/value/kernel/adam_m/read	-1	-1
bert/encoder/layer_1/output/LayerNorm/moments/mean	1599833759856022	31
Mul_331	1599833759819682	10
bert/encoder/layer_11/output/LayerNorm/moments/mean	1599833759989888	31
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760173767	28
global_step/VarIsInitializedOp	-1	-1
sub_150	1599833760336144	17
gradients/bert/encoder/layer_8/attention/output/dropout/mul_1_grad/Mul	1599833760085928	51
bert/encoder/layer_3/attention/output/dropout/mul	1599833759826668	51
Mul_903	1599833759808070	14
gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul	1599833760048208	638
Mul_303	1599833759818139	18
bert/encoder/layer_5/attention/output/add	1599833759901458	72
Square_73	1599833760276779	12
Sqrt_22	1599833760313868	10
gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul	1599833760109726	159
bert/embeddings/LayerNorm/gamma	-1	-1
Sqrt_76	1599833760315786	9
Mul_855	1599833759807587	12
sub_155	1599833760331644	4
Mul_89	1599833760284688	39
Mul_62	1599833759808692	12
gradients/bert/encoder/layer_3/attention/self/dropout/mul_1_grad/Mul	1599833760198527	99
Mul_356	1599833760276489	40
cls/predictions/transform/LayerNorm/moments/variance	1599833759990855	6
gradients/bert/encoder/layer_1/attention/output/dropout/mul_1_grad/Mul	1599833760240709	51
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760005687	68
mul_735	1599833759816437	13
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760018803	32
cls/predictions/transform/dense/BiasAdd	1599833759990504	9
add_122	1599833760317742	8
bert/encoder/layer_9/attention/output/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v	-1	-1
Mul_539	1599833760276699	4
Mul_599	1599833759813817	12
clip_by_global_norm/mul_147	1599833760275871	4
add_279	1599833760304103	61
bert/encoder/layer_4/output/dense/bias/adam_v	-1	-1
bert/encoder/layer_9/output/LayerNorm/beta/adam_m/read	-1	-1
global_norm/L2Loss_101	1599833760137939	15
gradients/bert/encoder/layer_9/attention/self/transpose_1_grad/transpose	1599833760068782	189
bert/encoder/layer_1/attention/output/dropout/random_uniform/mul	-1	-1
global_norm/L2Loss_151	1599833760071762	16
gradients/bert/encoder/layer_11/output/dropout/mul_grad/Mul	1599833760006638	72
bert/encoder/layer_10/attention/self/dropout/random_uniform	-1	-1
bert/encoder/layer_10/attention/self/value/MatMul	1599833759964821	630
Assign_147	1599833760340388	41
mul_800	1599833760329690	4
bert/encoder/layer_6/attention/output/dense/kernel/adam_m	-1	-1
add_75	1599833760292259	21
Mul_455	1599833760282733	4
gradients/bert/encoder/layer_0/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760269266	54
mul_864	1599833759815845	46
mul_1106	1599833760332542	7
Mul_965	1599833759819651	12
add_12	1599833760300364	13
truediv_74	1599833760321468	18
Mul_342	1599833760282710	4
Mul_953	1599833760279921	4
bert/encoder/layer_0/attention/output/dropout/Cast	1599833759824103	36
bert/encoder/layer_2/intermediate/dense/mul_2	1599833759866260	195
Assign_54	1599833760336077	4
bert/encoder/layer_10/output/LayerNorm/gamma	-1	-1
gradients/bert/encoder/layer_6/intermediate/dense/mul_2_grad/Mul_1	1599833760122347	279
Mul_1041	1599833760292060	9
add_384	1599833760304593	9
truediv_16	1599833760323015	58
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760226838	68
gradients/bert/encoder/layer_2/output/LayerNorm/moments/mean_grad/truediv	1599833760205220	47
add_328	1599833760326601	59
bert/embeddings/LayerNorm/batchnorm/mul	1599833759828683	41
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760248940	68
bert/encoder/layer_3/attention/self/dropout/Cast	1599833759825031	68
bert/encoder/layer_7/output/LayerNorm/gamma/adam_v	-1	-1
Sqrt_33	1599833760307629	45
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759968582	4
bert/embeddings/GatherV2/axis	-1	-1
Assign_524	1599833760312329	11
mul_515	1599833760326082	5
bert/encoder/layer_4/attention/self/Reshape_1	-1	-1
bert/encoder/layer_0/attention/self/value/kernel/adam_v/read	-1	-1
bert/encoder/layer_6/attention/output/LayerNorm/moments/variance	1599833759915006	31
global_norm/L2Loss_140	1599833760087373	5
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/add	1599833759981964	5
Reshape_1	-1	-1
mul_311	1599833760328815	4
add_27	1599833760318511	4
Mul_801	1599833759807496	11
bert/encoder/layer_0/attention/self/Reshape_1	-1	-1
Mul_223	1599833759812029	10
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760041415	51
add_580	1599833760301113	4
global_norm/L2Loss_56	1599833760204322	5
bert/encoder/layer_4/intermediate/dense/bias/adam_m/read	-1	-1
Mul_858	1599833760284903	4
gradients/bert/encoder/layer_3/attention/self/transpose_2_grad/transpose	1599833760198628	190
bert/embeddings/assert_less_equal/Assert/Assert	-1	-1
add_62	1599833760284450	56
Mul_872	1599833760278876	39
edge_1728_cls/predictions/Reshape@@MemcpyHtoD	1599833759821049	13
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/add	1599833759901655	5
bert/encoder/layer_5/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_3/attention/output/dense/kernel/adam_v	-1	-1
add_306	1599833760297453	22
Mul_389	1599833759819098	17
Mul_632	1599833760294629	5
Mul_1033	1599833760279927	40
gradients/bert/encoder/layer_5/intermediate/dense/mul_1_grad/Mul_1	1599833760145035	194
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760129387	32
Mul_1083	1599833759811922	10
mul_333	1599833760325667	4
Mul_684	1599833760276076	4
Square_21	1599833760280113	13
bert/encoder/layer_8/attention/self/query/kernel/adam_v/read	-1	-1
mul_763	1599833760329050	7
truediv_177	1599833760323867	6
bert/encoder/layer_0/attention/self/query/BiasAdd	1599833759831552	61
Mul_782	1599833759806431	23
Mul_433	1599833760293139	44
Sqrt_49	1599833760308928	45
bert/encoder/layer_11/attention/output/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_7/attention/self/query/kernel/adam_m/read	-1	-1
mul_962	1599833760331823	39
mul_617	1599833759813874	45
add_599	1599833760291770	9
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
bert/encoder/layer_9/attention/self/MatMul_1	1599833759953780	243
Mul_783	1599833760293713	9
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760107410	54
Sqrt_188	1599833760313157	8
global_norm/L2Loss_110	1599833760129779	5
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_2/attention/self/key/kernel	-1	-1
Assign_393	1599833760333883	10
clip_by_global_norm/mul_88	1599833760273776	13
add_506	1599833760318587	4
clip_by_global_norm/mul_156	1599833760274640	13
bert/embeddings/dropout/mul_1	1599833759829571	73
Mul_614	1599833760281108	39
add_225	1599833760296564	62
global_step/cond/Merge	-1	-1
bert/encoder/layer_4/attention/self/value/bias/adam_m/read	-1	-1
mul_607	1599833760333767	43
add_608	1599833760318880	4
truediv_31	1599833760321511	8
gradients/bert/encoder/layer_1/attention/self/transpose_1_grad/transpose	1599833760245666	189
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760152025	51
gradients/bert/encoder/layer_1/intermediate/dense/Tanh_grad/TanhGrad	1599833760233217	274
bert/encoder/layer_1/intermediate/dense/bias/adam_v	-1	-1
Mul_678	1599833759815910	11
Mul_823	1599833759807921	11
bert/encoder/layer_5/attention/output/LayerNorm/moments/variance	1599833759901621	32
gradients/AddN_75	1599833760227640	95
truediv_158	1599833760323291	5
clip_by_global_norm/mul_27	1599833760275346	4
bert/encoder/Cast	1599833759797942	34
Assign_146	1599833760307270	4
bert/encoder/layer_4/output/add	1599833759896123	73
bert/encoder/layer_4/output/dropout/random_uniform	-1	-1
sub_199	1599833760332244	9
bert/encoder/layer_3/output/dropout/random_uniform/mul	-1	-1
Mul_578	1599833759814644	4
bert/encoder/layer_2/attention/output/dropout/random_uniform/mul	-1	-1
Sqrt_116	1599833760315694	5
Square_37	1599833760276645	12
bert/encoder/layer_0/output/dense/kernel/adam_m	-1	-1
add_475	1599833760306326	22
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
global_norm/L2Loss_30	1599833760240334	5
truediv_84	1599833760321728	6
add_598	1599833760328172	55
truediv_77	1599833760324916	9
bert/encoder/layer_1/output/LayerNorm/beta	-1	-1
sub_50	1599833760334882	60
Assign_213	1599833760341847	14
global_norm/L2Loss_53	1599833760204305	16
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760217852	32
add_544	1599833760327748	55
gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul_1	1599833760136009	611
gradients/bert/encoder/layer_7/attention/output/dropout/mul_grad/Mul	1599833760108126	73
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760261999	53
bert/embeddings/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_0/attention/self/value/kernel/adam_m	-1	-1
bert/encoder/layer_1/intermediate/dense/bias/read	-1	-1
bert/encoder/layer_0/attention/output/dropout/GreaterEqual	1599833759822124	38
Mul_905	1599833759808960	18
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760062681	72
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m	-1	-1
truediv_188	1599833760323701	17
sub_149	1599833760331633	4
add_365	1599833760316991	8
bert/encoder/layer_8/attention/self/MatMul_1	1599833759940402	232
bert/embeddings/dropout/Cast	1599833759824924	36
add_180	1599833760293565	9
gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul	1599833760145993	2499
mul_1069	1599833760332130	12
bert/encoder/layer_6/attention/self/value/bias/adam_m	-1	-1
Square_20	1599833760280065	4
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760182625	52
bert/encoder/layer_1/attention/self/value/bias/read	-1	-1
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum	1599833760089878	54
bert/encoder/layer_6/attention/output/dropout/GreaterEqual	1599833759822325	38
gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul	1599833760013312	2501
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760107649	47
Mul_740	1599833760293615	10
truediv_174	1599833760323430	4
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760173960	27
bert/encoder/layer_7/attention/output/dense/kernel/adam_v	-1	-1
bert/encoder/layer_1/intermediate/dense/bias/adam_m/read	-1	-1
Mul_320	1599833759818405	11
gradients/bert/encoder/layer_1/intermediate/dense/mul_1_grad/Mul_1	1599833760233492	194
sub_42	1599833760335063	20
Mul_594	1599833759815139	11
bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference	1599833759989920	55
bert/encoder/layer_4/attention/self/dropout/random_uniform/mul	-1	-1
add_464	1599833760293588	9
gradients/bert/encoder/layer_1/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760243343	55
bert/encoder/layer_8/output/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760107794	53
Square_183	1599833760279631	12
bert/encoder/layer_9/output/LayerNorm/batchnorm/sub	1599833759963423	55
bert/encoder/layer_10/attention/output/add	1599833759968379	73
bert/encoder/layer_6/attention/self/query/kernel	-1	-1
global_norm/L2Loss_161	1599833760056257	33
global_norm/L2Loss_62	1599833760196060	5
bert/encoder/layer_6/attention/output/dense/kernel/adam_v	-1	-1
gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/Tile	1599833760004761	8
global_norm/L2Loss_144	1599833760084637	9
Square_122	1599833760276071	4
bert/encoder/layer_11/attention/self/key/bias/adam_v/read	-1	-1
add_412	1599833760320308	8
bert/encoder/layer_4/output/LayerNorm/beta/adam_m/read	-1	-1
add_172	1599833760317625	42
mul_219	1599833759815782	19
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760173953	5
bert/encoder/layer_5/attention/self/value/bias/adam_v	-1	-1
bert/encoder/layer_8/attention/output/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_11/attention/self/Reshape_3_grad/Reshape	-1	-1
Mul_199	1599833759809926	4
gradients/bert/encoder/layer_4/attention/self/Mul_grad/Mul	1599833760178459	141
mul_1068	1599833759816626	17
bert/encoder/layer_1/attention/self/value/BiasAdd	1599833759845077	60
bert/encoder/layer_9/output/dense/kernel/adam_v/read	-1	-1
Assign_316	1599833760305930	13
bert/encoder/layer_11/attention/output/dropout/random_uniform	-1	-1
bert/encoder/layer_9/attention/self/key/kernel/adam_v/read	-1	-1
sub_191	1599833760332360	7
Sqrt_135	1599833760306681	157
add_60	1599833760300534	5
mul_709	1599833760329565	8
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760160517	52
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760107047	51
gradients/bert/encoder/layer_4/attention/self/dropout/mul_grad/Mul	1599833760176671	141
global_norm/L2Loss_21	1599833760248502	16
gradients/bert/encoder/layer_6/output/LayerNorm/moments/mean_grad/truediv	1599833760116758	47
Square_27	1599833760276219	12
global_norm/L2Loss_164	1599833760050434	5
sub_205	1599833760336998	5
cls/predictions/LogSoftmax	1599833759994897	837
bert/encoder/layer_3/attention/self/dropout/random_uniform/mul	-1	-1
Sqrt_205	1599833760313402	9
bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1	1599833759842792	74
add_25	1599833760284671	4
Assign_58	1599833760300552	4
gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1	1599833760176136	242
Sqrt_32	1599833760309898	9
truediv_168	1599833760323449	17
Mul_78	1599833759808723	11
bert/encoder/layer_6/output/dense/BiasAdd	1599833759922761	60
bert/encoder/layer_8/attention/self/Reshape	-1	-1
add_330	1599833760296432	9
add_5	1599833760318003	369
Mul_38	1599833759809082	11
bert/encoder/layer_7/attention/output/dropout/Cast	1599833759824595	36
Square_49	1599833760277146	40
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760240286	46
bert/encoder/layer_4/output/dense/bias/adam_m/read	-1	-1
Mul_447	1599833759804820	5
Mul_647	1599833759816988	13
bert/encoder/layer_5/output/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul_1	1599833760268652	610
Mul_352	1599833759818463	10
gradients/bert/encoder/layer_5/intermediate/dense/Tanh_grad/TanhGrad	1599833760144758	275
Sqrt_9	1599833760310901	12
add_455	1599833760320465	15
Mul_946	1599833759810540	39
cls/seq_relationship/LogSoftmax	1599833759990673	16
bert/encoder/layer_6/attention/output/dense/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760182279	71
bert/encoder/layer_4/attention/self/key/bias	-1	-1
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
gradients/bert/encoder/layer_7/attention/self/dropout/mul_grad/Mul	1599833760110421	141
Assign_414	1599833760339773	4
add_483	1599833760320329	7
add_145	1599833760317781	16
bert/encoder/layer_10/intermediate/dense/mul_3	1599833759973540	280
bert/encoder/layer_3/attention/self/query/BiasAdd	1599833759871764	61
Assign_399	1599833760341934	14
add_499	1599833760305740	9
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760085375	28
add_627	1599833760302538	4
mul_263	1599833760332741	43
add_518	1599833760290227	10
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760195668	32
bert/encoder/layer_5/attention/output/dropout/GreaterEqual	1599833759821949	43
bert/encoder/layer_1/attention/self/key/kernel/read	-1	-1
bert/encoder/layer_2/output/LayerNorm/batchnorm/mul	1599833759869562	38
Square_114	1599833760281348	4
clip_by_global_norm/mul_89	1599833760273870	4
bert/encoder/layer_8/output/dense/BiasAdd	1599833759949511	60
bert/encoder/layer_8/intermediate/dense/mul	1599833759945285	195
bert/encoder/layer_7/intermediate/dense/kernel	-1	-1
gradients/AddN_18	1599833760050073	69
bert/encoder/layer_5/attention/self/value/bias/read	-1	-1
bert/encoder/layer_7/attention/self/value/bias	-1	-1
sub_197	1599833760332526	4
bert/encoder/layer_2/attention/output/dense/bias/adam_v	-1	-1
bert/encoder/layer_10/attention/self/query/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1	1599833760134683	241
add_498	1599833760294955	4
bert/encoder/layer_9/attention/self/query/kernel	-1	-1
bert/encoder/layer_1/intermediate/dense/kernel/adam_m/read	-1	-1
Mul_379	1599833760282370	12
global_norm/L2Loss_71	1599833760182261	16
gradients/bert/encoder/layer_5/attention/self/Mul_grad/Mul	1599833760156369	141
gradients/AddN_56	1599833760173040	73
Assign_55	1599833760300518	4
gradients/bert/encoder/layer_10/attention/output/dropout/mul_1_grad/Mul	1599833760041639	51
gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul_1	1599833760269962	611
bert/encoder/layer_5/output/add	1599833759909514	72
bert/encoder/layer_7/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_7/output/dense/kernel/adam_v	-1	-1
add_54	1599833760318495	4
mul_268	1599833760325650	4
bert/encoder/layer_0/attention/output/dense/BiasAdd	1599833759834347	62
add_607	1599833760301370	9
bert/encoder/layer_0/attention/output/LayerNorm/beta/read	-1	-1
Mul_1043	1599833759806123	46
add_629	1599833760290385	21
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
add_282	1599833760293001	4
bert/encoder/layer_9/attention/output/dense/bias/adam_m	-1	-1
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v	-1	-1
bert/encoder/layer_4/attention/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_0/intermediate/dense/bias/adam_m	-1	-1
Assign_346	1599833760296127	4
bert/encoder/layer_3/attention/self/transpose	1599833759871951	190
Mul_986	1599833759819499	11
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/add	1599833759968575	5
Mul_428	1599833760292839	12
Assign_140	1599833760309761	11
bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2	1599833759896448	53
clip_by_global_norm/mul_40	1599833760273895	12
Assign_36	1599833760335984	4
mul_951	1599833760332284	43
Assign_245	1599833760314601	41
gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul_1	1599833760089267	610
Mul_310	1599833760293038	9
add_68	1599833760318563	4
sub_108	1599833760334421	21
add_640	1599833760291444	10
gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul_1	1599833760163749	2469
Mul_966	1599833760290284	9
bert/encoder/layer_3/output/add	1599833759882747	73
Sqrt_10	1599833760310736	4
bert/encoder/layer_9/attention/self/dropout/random_uniform/RandomUniform	1599833759801530	83
bert/encoder/layer_0/output/dropout/mul_1	1599833759842469	73
bert/encoder/layer_7/output/LayerNorm/beta/adam_v	-1	-1
mul_257	1599833760326680	8
add_585	1599833760327908	16
bert/encoder/layer_1/attention/self/Reshape_2	-1	-1
Mul_880	1599833760289644	13
bert/encoder/layer_0/attention/self/Softmax	1599833759832710	324
add_348	1599833760326169	16
gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul_1	1599833760224459	612
bert/encoder/layer_8/attention/self/query/bias/adam_m	-1	-1
bert/encoder/layer_9/attention/self/query/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul_1	1599833760097499	2471
Assign_190	1599833760303901	40
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760173421	68
gradients/bert/encoder/layer_7/output/LayerNorm/moments/variance_grad/Tile	1599833760094732	26
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760084825	73
add_11	1599833760284651	13
Assign_104	1599833760309272	13
gradients/AddN_84	1599833760261547	72
add_609	1599833760290970	9
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul_1	1599833759953536	100
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760129630	33
gradients/bert/encoder/layer_11/intermediate/dense/mul_1_grad/Mul_1	1599833760012353	195
gradients/bert/encoder/layer_7/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760101704	108
add_34	1599833760318442	4
add_238	1599833760297193	21
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759861495	4
Assign_549	1599833760341416	13
add_503	1599833760320808	4
bert/encoder/layer_4/attention/self/query/bias	-1	-1
gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul_1	1599833760053217	2469
bert/embeddings/Slice/size	-1	-1
Assign_198	1599833760337377	4
add_582	1599833760290202	21
add_693	1599833760303137	9
Assign_75	1599833760341557	14
add_292	1599833760304307	22
clip_by_global_norm/mul_20	1599833760274618	4
sub_43	1599833760330954	7
bert/encoder/layer_0/attention/self/query/kernel/read	-1	-1
Mul_321	1599833760282932	4
sub_156	1599833760336114	16
bert/encoder/layer_0/attention/output/dropout/random_uniform/RandomUniform	1599833759800351	46
bert/encoder/layer_8/intermediate/dense/kernel/read	-1	-1
Mul_948	1599833759811438	45
bert/encoder/layer_6/output/LayerNorm/batchnorm/add	1599833759923095	5
bert/encoder/layer_6/attention/self/query/bias	-1	-1
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760004555	15
global_norm/L2Loss_104	1599833760137956	5
Assign_206	1599833760314884	3
Assign_528	1599833760336903	9
bert/encoder/layer_10/output/LayerNorm/beta/adam_v/read	-1	-1
bert/encoder/layer_0/attention/self/dropout/random_uniform/RandomUniform	1599833759801870	83
gradients/bert/encoder/layer_5/attention/self/transpose_3_grad/transpose	1599833760153714	189
Assign_79	1599833760303553	12
gradients/AddN_79	1599833760240612	96
Sqrt_127	1599833760307175	45
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760041216	46
mul_816	1599833760327566	4
Assign_283	1599833760297504	10
gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul_1	1599833760022895	616
Assign_290	1599833760307334	12
Assign_465	1599833760341081	13
Assign_78	1599833760337174	8
Square_159	1599833760279102	39
Mul_159	1599833759811322	11
bert/encoder/layer_3/intermediate/dense/kernel/adam_m/read	-1	-1
Assign_328	1599833760304518	12
gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul_1	1599833760133499	612
clip_by_global_norm/mul_184	1599833760275069	12
Assign_20	1599833760310978	3
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Mul_922	1599833759809177	11
bert/encoder/layer_11/intermediate/dense/bias	-1	-1
bert/encoder/layer_0/attention/self/key/bias	-1	-1
add_416	1599833760329524	20
gradients/bert/encoder/layer_9/intermediate/dense/mul_1_grad/Mul_1	1599833760056567	194
clip_by_global_norm/mul_158	1599833760274768	4
bert/encoder/layer_1/attention/output/dense/kernel/adam_m	-1	-1
bert/encoder/layer_1/attention/self/dropout/mul_1	1599833759846539	141
bert/encoder/layer_2/attention/self/value/bias	-1	-1
Assign_402	1599833760339867	10
Mul_926	1599833760279320	12
bert/encoder/layer_10/attention/self/value/kernel/adam_m	-1	-1
global_norm/L2Loss_36	1599833760227361	5
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760227368	5
add_441	1599833760305873	56
bert/encoder/layer_2/output/dense/kernel	-1	-1
Sqrt_197	1599833760312633	12
cls/seq_relationship/output_bias/adam_m/read	-1	-1
bert/encoder/layer_9/intermediate/dense/bias/adam_m	-1	-1
Mul_1027	1599833759816735	4
gradients/bert/encoder/layer_8/output/LayerNorm/moments/variance_grad/Tile	1599833760072590	26
global_norm/L2Loss_194	1599833760011474	5
Mul_567	1599833759815293	11
add_426	1599833760320542	12
cls/predictions/transform/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_0/attention/self/Mul	1599833759832477	99
Sqrt_128	1599833760315404	9
Assign_121	1599833760298895	12
truediv_76	1599833760324762	22
bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1	1599833759936518	73
add_48	1599833760318490	4
bert/encoder/layer_9/attention/self/Reshape	-1	-1
bert/encoder/layer_11/attention/self/value/kernel/adam_m	-1	-1
add_558	1599833760290743	9
bert/encoder/layer_11/attention/self/Reshape	-1	-1
bert/encoder/layer_0/intermediate/dense/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_6/attention/self/Reshape_3_grad/Reshape	-1	-1
Mul_815	1599833760284848	4
Assign_30	1599833760335861	4
Mul_1108	1599833760279910	4
bert/embeddings/position_embeddings/adam_m	-1	-1
gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul	1599833760068104	243
gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul	1599833760178601	247
truediv_166	1599833760323228	17
clip_by_global_norm/mul_166	1599833760274720	12
add_420	1599833760281825	17
bert/encoder/layer_11/attention/self/Mul	1599833759979766	99
add_490	1599833760325472	59
gradients/bert/encoder/layer_3/attention/self/dropout/mul_grad/Mul	1599833760198819	141
Assign_410	1599833760315239	11
Square_146	1599833760281206	4
bert/embeddings/LayerNorm/beta/adam_m/read	-1	-1
Mul_1072	1599833759807256	11
sub_63	1599833760330510	19
sub_61	1599833760329990	8
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m	-1	-1
Assign_17	1599833760310887	13
clip_by_global_norm/mul_109	1599833760275622	4
clip_by_global_norm/mul_137	1599833760275678	4
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
Mul_286	1599833760280816	4
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760218012	53
truediv_3	1599833760322986	15
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760151150	73
mul_805	1599833760327560	4
Mul_772	1599833760294949	4
mul_1063	1599833760328349	4
bert/encoder/layer_8/attention/output/dense/kernel/adam_m/read	-1	-1
Mul_911	1599833759809054	10
mul_1004	1599833759816908	18
add_685	1599833760319121	4
Square_166	1599833760279230	4
bert/encoder/layer_9/output/dense/kernel/adam_m/read	-1	-1
Square_165	1599833760278971	11
bert/encoder/layer_4/attention/self/value/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760027689	73
Assign_4	1599833760303192	12
bert/encoder/layer_0/attention/self/value/kernel/adam_v	-1	-1
sub_146	1599833760339425	60
bert/encoder/layer_9/attention/self/key/kernel/adam_v	-1	-1
bert/encoder/layer_9/intermediate/dense/mul_1	1599833759959352	195
gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1	1599833760131975	242
bert/encoder/layer_5/intermediate/dense/kernel/read	-1	-1
Assign_205	1599833760304468	4
bert/encoder/layer_8/attention/self/key/bias	-1	-1
sub_198	1599833760336620	17
sub_84	1599833760330561	8
truediv_82	1599833760324345	63
clip_by_global_norm/mul_18	1599833760274439	39
sub_130	1599833760339694	60
Assign_184	1599833760297170	11
bert/encoder/layer_0/attention/self/Reshape	-1	-1
gradients/AddN_21	1599833760062504	72
add_454	1599833760305580	22
bert/encoder/layer_8/output/LayerNorm/gamma/adam_v	-1	-1
add_577	1599833760318697	12
Assign_530	1599833760313469	11
Mul_71	1599833759810454	4
Assign_448	1599833760300646	14
bert/encoder/layer_5/output/LayerNorm/beta/adam_v/read	-1	-1
Mul_603	1599833760281467	40
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760116413	32
bert/encoder/layer_3/output/LayerNorm/batchnorm/Rsqrt	1599833759882951	4
add_57	1599833760318522	40
gradients/bert/encoder/layer_7/attention/self/Reshape_2_grad/Reshape	-1	-1
bert/encoder/layer_7/intermediate/dense/mul_1	1599833759932578	195
gradients/bert/encoder/layer_2/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760221262	55
add_263	1599833760305452	9
mul_166	1599833760325776	4
Assign_24	1599833760335924	4
add_616	1599833760301424	22
Mul_840	1599833760278799	13
Mul_589	1599833759814412	11
Square_127	1599833760276352	39
bert/encoder/layer_10/output/LayerNorm/gamma/adam_v	-1	-1
bert/encoder/layer_5/attention/self/key/kernel/adam_v/read	-1	-1
clip_by_global_norm/mul_177	1599833760275219	4
Mul_1081	1599833759808899	11
sub_141	1599833760333212	11
add_76	1599833760303422	21
clip_by_global_norm/mul_14	1599833760274554	4
bert/encoder/layer_7/intermediate/dense/bias/read	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760160977	5
bert/encoder/layer_11/attention/self/query/MatMul	1599833759976943	633
sub_10	1599833760335966	16
Assign_498	1599833760336400	4
global_norm/L2Loss_192	1599833760018283	9
Mul_996	1599833760279827	4
bert/embeddings/token_type_embeddings/read	-1	-1
sub_165	1599833760332065	7
sub_23	1599833760332641	8
bert/embeddings/Reshape	-1	-1
Square_85	1599833760280592	12
add_694	1599833760319540	8
bert/encoder/layer_0/attention/self/MatMul_1	1599833759833280	240
add_211	1599833760282364	4
Mul_713	1599833760281600	4
bert/encoder/layer_4/attention/output/dense/kernel	-1	-1
bert/encoder/layer_7/intermediate/dense/add_1	1599833759932984	196
add_676	1599833760291618	22
Square_101	1599833760276974	12
gradients/bert/encoder/layer_11/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760019698	53
bert/encoder/layer_2/attention/self/Reshape	-1	-1
bert/encoder/layer_3/intermediate/dense/bias/read	-1	-1
bert/embeddings/word_embeddings/adam_v/read	-1	-1
Mul_508	1599833759814537	11
Mul_98	1599833760278436	40
truediv_25	1599833760321251	5
gradients/bert/encoder/layer_5/attention/self/Reshape_2_grad/Reshape	-1	-1
add_289	1599833760298348	12
sub_159	1599833760331664	4
bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1	1599833759990062	73
add_271	1599833760293185	56
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760195536	69
global_step/Initializer/zeros	-1	-1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760107713	27
bert/encoder/layer_9/attention/self/Reshape_2	-1	-1
Mul_41	1599833760284665	4
bert/encoder/layer_8/attention/output/dense/kernel/adam_v	-1	-1
Assign_443	1599833760316669	3
mul_628	1599833760325550	4
bert/encoder/layer_6/output/dense/bias/adam_v	-1	-1
sub_17	1599833760331621	5
global_norm/L2Loss_68	1599833760183077	5
global_norm/L2Loss_174	1599833760041264	5
bert/encoder/layer_7/intermediate/dense/kernel/adam_m	-1	-1
add_129	1599833760282259	17
gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul_1	1599833760141682	2470
add_548	1599833760284951	56
bert/encoder/layer_3/attention/output/dense/bias/adam_v/read	-1	-1
Assign_232	1599833760297841	12
mul_434	1599833759814272	40
Assign_191	1599833760314366	41
bert/encoder/layer_0/intermediate/dense/add	1599833759838570	279
add_436	1599833760325592	56
gradients/AddN_61	1599833760183353	96
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul	1599833759874901	38
gradients/bert/encoder/layer_0/intermediate/dense/Tanh_grad/TanhGrad	1599833760255325	275
bert/encoder/layer_7/attention/self/value/bias/adam_m/read	-1	-1
Mul_600	1599833760293412	9
bert/encoder/layer_1/attention/output/dense/kernel/adam_v	-1	-1
add_380	1599833760306402	56
clip_by_global_norm/mul_62	1599833760273607	4
bert/encoder/layer_9/output/dense/bias	-1	-1
bert/encoder/layer_0/attention/output/dense/bias/adam_v	-1	-1
bert/encoder/layer_0/attention/output/LayerNorm/gamma/read	-1	-1
sub_11	1599833760331440	4
sub_188	1599833760336667	17
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v/read	-1	-1
gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul	1599833760198136	160
cls/predictions/transform/LayerNorm/batchnorm/Rsqrt	1599833759990948	4
bert/encoder/layer_5/output/LayerNorm/gamma/adam_v/read	-1	-1
gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul	1599833760110764	637
Assign_183	1599833760334444	10
bert/encoder/layer_0/attention/self/MatMul	1599833759832313	163
global_norm/L2Loss_169	1599833760045651	15
Mul_266	1599833759817453	10
Sqrt_14	1599833760310941	4
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760027999	56
Assign_511	1599833760301118	4
Mul_1011	1599833759815463	13
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
bert/encoder/layer_10/attention/self/Reshape_1	-1	-1
Square_17	1599833760278395	39
Mul_920	1599833759811244	4
Mul_572	1599833759813328	13
Square_191	1599833760279968	39
gradients/bert/encoder/layer_8/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760079553	110
bert/encoder/layer_9/attention/output/dense/kernel/adam_m	-1	-1
Square_95	1599833760277428	40
sub_169	1599833760331960	4
bert/encoder/layer_9/output/LayerNorm/beta/read	-1	-1
Assign_590	1599833760313587	4
bert/pooler/dense/bias/adam_m	-1	-1
sub_125	1599833760333550	8
gradients/bert/encoder/layer_11/attention/self/Reshape_1_grad/Reshape	-1	-1
Mul_329	1599833759819313	11
bert/encoder/layer_1/intermediate/dense/kernel/adam_m	-1	-1
bert/encoder/layer_4/attention/output/dense/MatMul	1599833759887300	630
gradients/Reshape_2_grad/Reshape/tensor	1599833760005263	51
Mul_130	1599833760280154	11
Mul_627	1599833760281787	4
Square_88	1599833760277261	4
bert/encoder/layer_0/attention/self/add	1599833759832578	130
Assign_546	1599833760336546	4
clip_by_global_norm/mul_120	1599833760273360	13
bert/encoder/layer_5/attention/output/dense/bias/read	-1	-1
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
Assign_315	1599833760342034	13
bert/encoder/layer_4/attention/self/dropout/random_uniform/RandomUniform	1599833759802210	83
bert/encoder/layer_2/output/LayerNorm/batchnorm/sub	1599833759869732	55
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760239942	31
gradients/AddN_27	1599833760079184	364
sub_94	1599833760330181	15
add_400	1599833760306060	21
bert/encoder/layer_3/output/dense/bias/read	-1	-1
add_511	1599833760284842	4
Mul_914	1599833759808223	19
gradients/bert/encoder/layer_1/attention/self/Reshape_grad/Reshape	-1	-1
Square_43	1599833760280177	13
bert/encoder/layer_6/attention/self/value/kernel/read	-1	-1
bert/encoder/layer_2/attention/output/dropout/random_uniform/RandomUniform	1599833759800544	46
add_551	1599833760327624	56
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul	1599833760066277	143
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760005611	75
gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/Pow	1599833759851385	195
bert/encoder/layer_1/attention/self/value/bias/adam_v	-1	-1
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Square_8	1599833760278477	4
add_171	1599833760298195	60
clip_by_global_norm/mul_154	1599833760274971	13
add_46	1599833760284624	4
gradients/bert/encoder/layer_6/attention/self/transpose_3_grad/transpose	1599833760131623	189
clip_by_global_norm/mul_85	1599833760273865	4
global_norm/L2Loss_6	1599833760270575	5
mul_472	1599833760326040	8
bert/encoder/layer_11/output/dropout/mul	1599833759825980	52
add_391	1599833760306009	9
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760041278	27
Assign_168	1599833760337486	5
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760062808	74
bert/encoder/layer_5/output/LayerNorm/beta/adam_m/read	-1	-1
Assign_479	1599833760311636	41
gradients/AddN_10	1599833760027352	119
bert/encoder/layer_3/intermediate/dense/kernel/adam_v/read	-1	-1
Assign_274	1599833760298475	19
bert/encoder/layer_1/output/dropout/GreaterEqual	1599833759822164	38
add_28	1599833760284379	17
mul_48	1599833760331426	12
clip_by_global_norm/mul_58	1599833760273745	12
global_norm/L2Loss_52	1599833760205269	5
bert/encoder/layer_1/attention/output/dense/kernel	-1	-1
Mul_681	1599833760295361	9
gradients/bert/encoder/layer_7/attention/self/transpose_grad/transpose	1599833760112816	189
Assign_541	1599833760302533	4
Sqrt_201	1599833760312662	4
Assign_264	1599833760334970	12
gradients/AddN_59	1599833760182141	118
Mul_791	1599833759807466	12
Assign_141	1599833760341617	41
bert/encoder/layer_7/output/dense/BiasAdd	1599833759936133	61
bert/encoder/layer_5/attention/self/value/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760093994	73
gradients/bert/encoder/layer_8/attention/self/Reshape_1_grad/Reshape	-1	-1
bert/encoder/layer_5/attention/output/dense/bias/adam_m/read	-1	-1
bert/pooler/strided_slice	1599833759990342	5
Mul_699	1599833759805872	44
Square_97	1599833760276878	40
clip_by_global_norm/mul_1	1599833760274050	362
Assign_575	1599833760313539	41
gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul_1	1599833760148494	2466
Square_98	1599833760277125	4
add_692	1599833760291939	9
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum	1599833760200424	54
mul_1080	1599833760332400	17
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum	1599833760178279	55
bert/encoder/layer_4/attention/self/dropout/GreaterEqual	1599833759823797	73
Assign_50	1599833760311108	3
add_160	1599833760283305	9
mul_187	1599833759815353	46
bert/encoder/layer_0/attention/self/mul_1/y	-1	-1
bert/encoder/layer_5/attention/self/transpose_2	1599833759899101	189
Assign_180	1599833760334154	10
Mul_450	1599833760292995	4
Assign_170	1599833760314715	13
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m	-1	-1
Assign_83	1599833760307033	13
add_451	1599833760305351	4
edge_1780_Less@@MemcpyHtoD	1599833759824068	4
gradients/bert/encoder/layer_3/attention/self/Reshape_3_grad/Reshape	-1	-1
global_norm/L2Loss_188	1599833760021012	5
Assign_196	1599833760296628	41
Assign_347	1599833760307054	11
gradients/AddN_15	1599833760040904	69
truediv_30	1599833760321263	4
Assign_100	1599833760296841	41
bert/encoder/layer_0/attention/self/query/bias	-1	-1
Mul_820	1599833760289699	17
Mul_715	1599833759805485	12
bert/encoder/layer_2/output/LayerNorm/gamma/adam_v	-1	-1
add_169	1599833760316936	4
Mul_734	1599833760281623	12
Mul_545/x	-1	-1
gradients/bert/encoder/layer_9/output/LayerNorm/moments/variance_grad/truediv	1599833760050476	52
bert/encoder/layer_11/attention/output/LayerNorm/beta	-1	-1
Assign_46	1599833760300471	40
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760063260	32
bert/encoder/layer_9/attention/output/dropout/random_uniform/RandomUniform	1599833759801263	47
Mul_551	1599833760282631	12
Mul_66	1599833760278512	11
global_norm/L2Loss_179	1599833760027889	5
add_255	1599833760292814	10
Sqrt_126	1599833760306578	10
mul_262	1599833759814657	45
Square_39	1599833760277309	12
truediv_13	1599833760322974	5
bert/encoder/layer_1/output/dropout/random_uniform	-1	-1
add_657	1599833760301941	56
Assign_516	1599833760336517	4
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760040810	53
bert/encoder/layer_3/attention/self/Reshape_1	-1	-1
bert/encoder/layer_2/attention/self/value/kernel/adam_v/read	-1	-1
sub_144	1599833760333956	60
Mul_910	1599833760279415	4
add_228	1599833760292746	4
Assign_371	1599833760307150	19
cls/seq_relationship/output_bias	-1	-1
clip_by_global_norm/Minimum	1599833760273249	4
add_419	1599833760316686	8
edge_1721_bert/embeddings/Reshape_2@@MemcpyHtoD	1599833759797664	78
Assign_557	1599833760312193	20
gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul_1	1599833760175149	612
bert/encoder/layer_9/attention/self/value/bias/adam_v	-1	-1
bert/encoder/layer_2/attention/self/value/BiasAdd	1599833759858483	61
Mul_24	1599833759810986	4
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_8/attention/self/key/bias/read	-1	-1
add_128	1599833760317991	4
Assign_613	1599833760303110	11
Mul_302	1599833760280312	12
Sqrt_69	1599833760308002	17
Square_23	1599833760280140	12
Assign_608	1599833760312990	14
gradients/bert/encoder/Reshape_1_grad/Reshape	-1	-1
bert/encoder/layer_0/output/LayerNorm/gamma/adam_m	-1	-1
bert/encoder/layer_8/output/dense/kernel/adam_m	-1	-1
Assign_444	1599833760336098	4
Mul_129	1599833759808381	18
bert/encoder/layer_8/intermediate/dense/bias	-1	-1
Mul_1070	1599833759809425	11
Mul_862	1599833759808446	45
bert/encoder/layer_11/output/dense/bias/adam_v	-1	-1
Mul_366	1599833759818491	10
clip_by_global_norm/mul_50	1599833760273813	39
bert/encoder/layer_8/attention/self/dropout/mul	1599833759827187	99
global_norm/L2Loss_166	1599833760049464	5
Assign_329	1599833760314951	14
Mul_651	1599833759817233	11
sub_138	1599833760337788	20
Square_160	1599833760279263	4
clip_by_global_norm/mul_51	1599833760273929	4
bert/encoder/layer_8/attention/output/dropout/random_uniform	-1	-1
add_453	1599833760294792	21
truediv_24	1599833760324076	21
add_33	1599833760300229	4
Sqrt_82	1599833760314643	4
bert/embeddings/Reshape_4	-1	-1
bert/encoder/layer_8/output/dense/bias	-1	-1
sub_32	1599833760335242	60
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760107520	27
cls/predictions/transform/dense/bias/adam_m	-1	-1
add_200	1599833760326205	17
Mul_232	1599833759811484	4
gradients/bert/encoder/layer_9/attention/self/dropout/mul_1_grad/Mul	1599833760065842	98
bert/encoder/layer_7/attention/self/query/bias	-1	-1
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760195882	27
Assign_306	1599833760334858	12
Sqrt_15	1599833760310994	41
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760151918	51
add_189	1599833760317668	4
cls/predictions/output_bias	-1	-1
gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1	1599833760154066	228
bert/encoder/layer_11/output/dense/bias/adam_m	-1	-1
gradients/AddN_74	1599833760227001	68
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul_1	1599833759859835	100
truediv_75	1599833760324323	4
sub_16	1599833760336020	56
truediv_139	1599833760325153	4
add_517	1599833760327724	17
Mul_449	1599833759804936	11
bert/encoder/layer_10/intermediate/dense/kernel/adam_m	-1	-1
Assign_460	1599833760301679	23
bert/encoder/layer_0/output/add	1599833759842543	73
Square_151	1599833760279038	10
add_343	1599833760297074	10
gradients/bert/encoder/layer_2/output/LayerNorm/moments/variance_grad/truediv	1599833760205312	52
Assign_325	1599833760304473	4
clip_by_global_norm/mul_191	1599833760275177	4
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
gradients/bert/encoder/layer_0/output/dense/BiasAdd_grad/BiasAddGrad	1599833760249966	55
bert/encoder/layer_9/attention/self/key/kernel/read	-1	-1
Square_41	1599833760277336	13
Square_136	1599833760280827	4
Sqrt_192	1599833760313725	11
add_472	1599833760305969	4
bert/encoder/layer_6/intermediate/dense/mul	1599833759918534	195
bert/encoder/layer_3/attention/output/dense/kernel/adam_m/read	-1	-1
Assign_110	1599833760309885	3
Square_2	1599833760278601	9
bert/encoder/layer_6/attention/self/key/kernel/read	-1	-1
clip_by_global_norm/mul_66	1599833760273458	39
bert/encoder/layer_10/attention/self/dropout/random_uniform/RandomUniform	1599833759801785	83
bert/encoder/layer_9/attention/self/value/kernel	-1	-1
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub	1599833759955363	54
add_359	1599833760295141	17
add_695	1599833760328360	4
Assign_363	1599833760342064	13
bert/encoder/layer_6/attention/self/query/bias/adam_v/read	-1	-1
Mul_367	1599833760280576	4
bert/encoder/layer_9/attention/self/dropout/mul_1	1599833759953637	141
Assign_456	1599833760336463	4
bert/encoder/layer_6/attention/output/dropout/mul_1	1599833759914768	73
bert/encoder/layer_9/intermediate/dense/BiasAdd	1599833759957688	225
bert/encoder/layer_4/output/LayerNorm/batchnorm/Rsqrt	1599833759896326	4
bert/encoder/layer_11/attention/output/dropout/mul_1	1599833759981693	73
bert/encoder/layer_10/attention/self/dropout/mul	1599833759827691	99
bert/encoder/layer_9/intermediate/dense/Tanh	1599833759959548	208
Assign_6	1599833760341028	10
bert/encoder/layer_2/attention/self/query/kernel/adam_m	-1	-1
bert/encoder/layer_4/intermediate/dense/mul_3	1599833759893231	279
bert/encoder/layer_5/attention/self/value/bias/adam_m	-1	-1
mul_429	1599833760328734	4
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760049831	74
Mul_540	1599833759809024	11
Mul_868	1599833759807227	11
bert/encoder/layer_1/attention/self/key/MatMul	1599833759843687	632
truediv_154	1599833760323556	17
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2	1599833759982090	54
bert/encoder/layer_0/output/LayerNorm/batchnorm/add_1	1599833759842978	73
truediv_130	1599833760325072	61
Mul_1093	1599833759820264	11
bert/encoder/layer_11/attention/output/dense/bias/adam_v	-1	-1
Assign_212	1599833760314436	11
Mul_30	1599833760284354	4
add_496	1599833760320347	43
bert/encoder/layer_1/attention/self/query/kernel/adam_v	-1	-1
bert/encoder/layer_9/attention/output/dense/MatMul	1599833759954216	630
sub_26	1599833760337081	22
clip_by_global_norm/mul_44	1599833760275352	12
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760129600	28
add_29	1599833760300085	17
bert/encoder/layer_11/attention/self/query/kernel/adam_v/read	-1	-1
global_norm/L2Loss_35	1599833760226961	5
Mul_196	1599833759809395	11
mul_360	1599833760330032	44
Mul_839	1599833759805303	18
Mul_355	1599833759818733	44
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760204876	31
add_438	1599833760305061	9
bert/encoder/layer_6/output/LayerNorm/gamma/adam_v	-1	-1
Mul_425	1599833759819869	11
bert/encoder/layer_2/output/LayerNorm/beta/adam_v/read	-1	-1
add_515	1599833760300872	17
Mul_882	1599833759810594	4
clip_by_global_norm/mul_185	1599833760275165	4
clip_by_global_norm/mul_57	1599833760275556	4
bert/encoder/layer_3/attention/self/Mul	1599833759872684	100
bert/encoder/layer_10/attention/self/key/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_11/output/LayerNorm/moments/variance_grad/truediv	1599833760006253	52
bert/encoder/layer_1/output/dense/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_4/attention/self/dropout/mul_1_grad/Mul	1599833760176380	99
gradients/bert/encoder/layer_0/intermediate/dense/mul_3_grad/Mul	1599833760254731	279
Assign_93	1599833760340516	41
bert/encoder/layer_8/attention/self/Mul	1599833759939605	99
Mul_353	1599833760281637	4
Assign_301	1599833760297035	25
bert/encoder/layer_0/attention/self/value/kernel/adam_m/read	-1	-1
Mul_1107	1599833759807800	11
add_1/_2558	1599833760004704	6
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760151277	73
truediv_138	1599833760324720	21
bert/encoder/layer_9/attention/self/add	1599833759953082	129
bert/encoder/layer_9/attention/self/transpose	1599833759952247	189
Mul_959	1599833759817584	39
truediv_180	1599833760323676	4
Mul_454	1599833759819930	11
global_norm/L2Loss_117	1599833760115845	16
Mul_43	1599833759808659	19
Assign_525	1599833760341446	41
bert/encoder/layer_10/output/add	1599833759976435	73
add_677	1599833760302749	21
bert/encoder/layer_11/attention/output/dense/kernel/adam_m/read	-1	-1
clip_by_global_norm/mul_173	1599833760274960	4
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760160370	75
gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760270816	73
bert/encoder/layer_11/attention/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_3/intermediate/dense/mul_2	1599833759879658	194
bert/embeddings/dropout/truediv	-1	-1
bert/encoder/layer_0/attention/self/value/kernel	-1	-1
gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul_1	1599833760086758	614
bert/encoder/layer_3/attention/self/value/kernel/adam_m/read	-1	-1
add_157	1599833760283079	4
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760106903	68
bert/encoder/layer_6/output/dropout/Cast	1599833759824406	37
Assign_256	1599833760304331	20
bert/encoder/layer_8/output/dense/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_2/attention/output/dropout/mul_grad/Mul	1599833760218671	72
bert/encoder/layer_2/output/dropout/mul_1	1599833759869277	73
bert/encoder/layer_10/attention/output/dense/bias/read	-1	-1
gradients/bert/encoder/layer_0/output/dropout/mul_1_grad/Mul	1599833760249837	51
clip_by_global_norm/mul_188	1599833760275082	12
bert/encoder/layer_2/attention/self/dropout/random_uniform/RandomUniform	1599833759802295	83
bert/encoder/layer_8/intermediate/dense/kernel	-1	-1
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul	1599833760154730	141
bert/encoder/layer_2/output/LayerNorm/beta/adam_m/read	-1	-1
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m/read	-1	-1
bert/encoder/layer_11/output/dense/kernel	-1	-1
add_70	1599833760300546	4
bert/pooler/dense/kernel/adam_v	-1	-1
bert/encoder/layer_6/attention/self/key/bias/adam_m	-1	-1
gradients/cls/predictions/Sum_grad/Reshape	-1	-1
truediv_63	1599833760321716	4
bert/encoder/layer_0/attention/self/ExpandDims	-1	-1
Mul_511	1599833759805844	11
gradients/bert/encoder/layer_2/attention/self/transpose_grad/transpose	1599833760223375	189
bert/encoder/layer_9/attention/self/query/MatMul	1599833759950158	633
global_norm/L2Loss_133	1599833760093760	16
Square_158	1599833760278832	4
bert/encoder/layer_9/output/dropout/mul	1599833759826774	51
global_norm/L2Loss_86	1599833760160002	6
gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul	1599833760156511	231
cls/predictions/transform/LayerNorm/batchnorm/mul_2	1599833759991015	11
bert/encoder/layer_1/attention/self/dropout/random_uniform/mul	-1	-1
Assign_339	1599833760341949	40
sub_116	1599833760329873	4
bert/encoder/layer_5/output/dense/kernel/adam_v	-1	-1
bert/embeddings/token_type_embeddings/adam_m/read	-1	-1
Mul_761	1599833759805814	12
bert/encoder/layer_3/output/dense/kernel/adam_m	-1	-1
truediv_199	1599833760323733	4
Mul_1009	1599833760290722	10
Square_69	1599833760276753	12
add_305	1599833760282484	17
Sqrt_5	1599833760310872	13
mul_838	1599833760327571	4
Mul_248	1599833759811494	10
Assign_18	1599833760336009	4
Mul_463	1599833760280606	12
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
mul_96	1599833760327549	4
Mul_1054	1599833759805230	4
Assign_517	1599833760301605	12
add_410	1599833760294418	9
mul_1	1599833759822927	4
mul_128	1599833760328451	7
mul_650	1599833760329879	12
bert/encoder/layer_1/attention/output/dropout/random_uniform	-1	-1
Mul_423	1599833760282722	4
bert/encoder/layer_5/attention/self/query/MatMul	1599833759896632	634
add_116	1599833760282188	56
mul_3	1599833759825459	76
bert/encoder/layer_1/output/dense/bias/adam_v	-1	-1
Mul_938	1599833759808870	11
Assign_438	1599833760339652	6
Assign_252	1599833760334954	5
Sqrt_149	1599833760311248	16
truediv_23	1599833760324032	6
bert/embeddings/LayerNorm/batchnorm/add/y	-1	-1
Square_26	1599833760280166	4
Square_179	1599833760279620	4
truediv_83	1599833760324410	5
bert/encoder/layer_0/attention/output/dense/kernel/adam_v/read	-1	-1
global_norm/L2Loss_123	1599833760109522	12
Sqrt_19	1599833760311113	4
gradients/AddN_34	1599833760101329	367
add_560	1599833760319041	7
gradients/AddN_90/inputs_1	1599833760272154	331
bert/encoder/layer_1/output/LayerNorm/gamma/adam_v	-1	-1
add_204	1599833760292604	22
add_402	1599833760329505	18
mul_730	1599833760329586	7
Assign_89	1599833760307605	11
sub_179	1599833760331927	4
add_673	1599833760291191	8
add_41	1599833760318395	4
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/add_1	1599833759888502	73
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760049779	50
bert/encoder/layer_2/output/dropout/mul	1599833759826404	51
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760249315	32
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760072726	51
add_597	1599833760319263	43
Assign_265	1599833760298424	12
Assign_240	1599833760334509	9
add_291	1599833760293113	21
Mul_546	1599833760282320	4
cls/seq_relationship/Reshape	-1	-1
mul_655	1599833760329154	6
bert/encoder/layer_1/attention/output/LayerNorm/beta	-1	-1
Sqrt_179	1599833760312689	8
Assign_601	1599833760303083	12
bert/encoder/layer_5/attention/self/add	1599833759899555	127
mul_649	1599833759814194	18
Mul_147	1599833760280172	4
bert/encoder/layer_7/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_8/intermediate/dense/add	1599833759945677	280
bert/encoder/layer_0/attention/self/transpose_1	1599833759831930	189
clip_by_global_norm/mul_182	1599833760274926	12
bert/encoder/layer_0/output/dense/kernel	-1	-1
global_norm/L2Loss_58	1599833760200479	5
clip_by_global_norm/mul_149	1599833760274623	4
mul_402	1599833759814520	12
bert/encoder/layer_1/intermediate/dense/kernel/read	-1	-1
sub_73	1599833760333412	7
Mul_111	1599833760284820	4
Mul_573	1599833760295160	12
bert/encoder/layer_2/attention/self/Softmax	1599833759859511	323
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760195461	74
gradients/bert/encoder/layer_1/attention/self/transpose_2_grad/transpose	1599833760242862	189
Mul_4	1599833759810078	362
Mul_989	1599833759819898	18
add_655	1599833760319578	8
Mul_181	1599833760283442	4
gradients/bert/encoder/layer_9/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760063912	55
bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1	1599833759976684	73
Mul_95	1599833760284808	4
sub_71	1599833760332872	4
Sqrt_60	1599833760307358	9
bert/encoder/layer_6/intermediate/dense/add_1	1599833759919611	196
bert/encoder/layer_4/attention/output/dropout/random_uniform/RandomUniform	1599833759801215	46
cls/predictions/Const	-1	-1
bert/encoder/layer_8/output/dense/kernel/adam_v	-1	-1
mul_20	1599833759812241	9
add_125	1599833760317894	4
Mul_197	1599833760283193	4
mul_47	1599833759813239	13
Sqrt_177	1599833760311679	41
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Assign_81	1599833760340036	14
Mul_492	1599833759820499	10
gradients/bert/encoder/layer_4/attention/self/transpose_2_grad/transpose	1599833760176480	190
sub_158	1599833760331776	4
global_norm/L2Loss_141	1599833760085153	5
bert/encoder/layer_1/output/dense/BiasAdd	1599833759855811	61
Assign_247	1599833760304213	4
mul_833	1599833760332005	16
Mul_1029	1599833759816845	11
global_norm/L2Loss_9	1599833760266830	15
bert/encoder/layer_8/output/dense/bias/adam_m	-1	-1
add_628	1599833760319313	4
bert/encoder/layer_7/attention/self/key/MatMul	1599833759924038	631
Assign_87	1599833760334273	4
gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul	1599833760250023	2223
gradients/bert/encoder/layer_7/output/LayerNorm/moments/mean_grad/truediv	1599833760094670	47
bert/encoder/layer_8/attention/self/add	1599833759939706	127
gradients/bert/encoder/layer_2/output/dense/BiasAdd_grad/BiasAddGrad	1599833760205771	55
add_461	1599833760295907	23
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_1/attention/self/key/bias	-1	-1
bert/embeddings/LayerNorm/batchnorm/Rsqrt	1599833759828527	5
Assign_182	1599833760307378	11
global_norm/L2Loss_127	1599833760106869	33
bert/encoder/layer_4/attention/self/query/kernel/adam_m	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/gamma	-1	-1
Mul_973	1599833759819040	11
bert/encoder/layer_9/attention/output/LayerNorm/moments/variance	1599833759955149	32
Mul_281	1599833760280292	4
bert/encoder/layer_1/output/LayerNorm/batchnorm/Rsqrt	1599833759856151	4
Mul_444	1599833760292893	43
Mul_393	1599833759819809	12
gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul_1	1599833760180179	613
Square_199	1599833760279866	12
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760063450	51
sub_120	1599833760334040	17
add_505	1599833760300563	4
bert/encoder/layer_8/attention/output/dropout/Cast	1599833759824179	36
Mul_137	1599833759811311	4
bert/encoder/layer_0/output/dropout/Cast	1599833759824329	36
sub_166	1599833760336220	17
gradients/AddN_58	1599833760174223	95
add_17	1599833760318569	4
bert/encoder/layer_2/attention/output/dense/BiasAdd	1599833759861151	60
gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul_1	1599833760091775	612
Assign_49	1599833760300541	4
Square_40	1599833760277480	4
bert/encoder/layer_2/attention/self/query/BiasAdd	1599833759858358	60
bert/encoder/layer_7/attention/output/dense/bias	-1	-1
Square_68	1599833760280702	4
gradients/bert/encoder/layer_11/output/LayerNorm/moments/mean_grad/truediv	1599833760006163	47
mul_1036	1599833759817363	45
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760004538	15
bert/encoder/layer_5/attention/self/dropout/mul	1599833759827590	99
bert/encoder/layer_1/intermediate/dense/add	1599833759851973	281
bert/embeddings/dropout/random_uniform	-1	-1
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m/read	-1	-1
add_43	1599833760300240	17
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760041307	52
sub_64	1599833760337320	56
bert/encoder/layer_0/output/dense/bias/adam_m	-1	-1
Assign_459	1599833760341344	13
add_425	1599833760305797	9
bert/encoder/layer_11/attention/self/dropout/random_uniform/mul	-1	-1
Mul_410	1599833760280907	11
sub_66	1599833760334201	60
add_361	1599833760320608	12
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/Pow	1599833759931707	195
bert/encoder/layer_8/attention/output/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_9/attention/self/query/bias/adam_m	-1	-1
add_610	1599833760302053	9
truediv_204	1599833760323854	6
bert/encoder/layer_0/attention/self/query/bias/adam_v/read	-1	-1
bert/encoder/layer_9/attention/self/key/BiasAdd	1599833759952123	60
bert/encoder/layer_3/output/dropout/Cast	1599833759824671	36
bert/encoder/layer_2/attention/output/dropout/random_uniform	-1	-1
Mul_863	1599833760289832	40
add_295	1599833760282464	4
bert/encoder/layer_11/output/dense/kernel/adam_m	-1	-1
Mul_1075	1599833759809501	19
Mul_921	1599833760279241	4
sub_151	1599833760331639	4
mul_380	1599833759814720	18
bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference	1599833759842651	54
sub_123	1599833760329713	8
Square_19	1599833760278757	4
bert/encoder/layer_0/attention/self/key/bias/read	-1	-1
bert/encoder/layer_0/attention/output/dense/kernel/adam_m	-1	-1
bert/encoder/layer_0/attention/output/dense/kernel	-1	-1
Assign_257	1599833760314766	20
Assign_278	1599833760307289	11
sub_167	1599833760331864	4
Assign_65	1599833760313939	20
Mul_364	1599833760292740	4
global_norm/L2Loss_7	1599833760270726	16
add_84	1599833760319677	16
mul_552	1599833759813709	12
bert/encoder/layer_5/attention/output/dense/kernel/adam_m	-1	-1
Assign_589	1599833760303057	4
add_522	1599833760301707	21
bert/encoder/layer_9/intermediate/dense/kernel/read	-1	-1
mul_967	1599833760327932	4
Assign_273	1599833760340430	13
bert/encoder/layer_0/intermediate/dense/MatMul	1599833759834996	2193
gradients/bert/encoder/layer_8/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760091077	55
add_124	1599833760298870	4
bert/encoder/layer_9/attention/output/LayerNorm/gamma	-1	-1
bert/encoder/layer_1/attention/self/Reshape	-1	-1
add_66	1599833760284792	4
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760248664	71
truediv_170	1599833760323247	18
truediv_62	1599833760321424	6
sub_83	1599833760332964	4
Assign_403	1599833760306246	12
global_norm/L2Loss_108	1599833760131603	5
mul_542	1599833760325878	4
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
Mul_261	1599833760292433	44
Mul_808	1599833759805244	18
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum	1599833760067781	54
gradients/bert/encoder/layer_2/output/dropout/mul_grad/Mul	1599833760205696	73
Assign_349	1599833760305357	5
sub_92	1599833760334995	20
Mul_205	1599833760276659	12
bert/encoder/layer_2/attention/self/MatMul_1	1599833759860079	244
Sqrt_119	1599833760307078	17
gradients/Reshape_2_grad/Reshape	-1	-1
Mul_813	1599833760278778	4
sub_1	1599833760335315	519
gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul	1599833760045911	245
bert/encoder/layer_9/attention/self/key/bias/adam_m/read	-1	-1
bert/encoder/layer_11/attention/self/Reshape_1	-1	-1
bert/encoder/layer_0/intermediate/dense/kernel/adam_m	-1	-1
bert/encoder/layer_7/attention/self/key/kernel	-1	-1
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760151697	28
Mul_465	1599833760293087	17
global_norm/L2Loss_112	1599833760128865	9
clip_by_global_norm/mul_163	1599833760274748	4
bert/encoder/layer_1/attention/self/key/kernel	-1	-1
Mul_663	1599833760276066	4
truediv_108	1599833760321401	22
bert/encoder/layer_8/attention/self/key/kernel	-1	-1
Mul_86	1599833759811016	39
sub_160	1599833760336285	56
gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul_1	1599833760177665	613
Assign_282	1599833760334597	10
gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/Pow	1599833759864789	196
bert/encoder/layer_11/attention/self/transpose_1	1599833759979223	189
Assign_342	1599833760339778	4
Assign_507	1599833760341168	14
Mul_1103	1599833759807862	12
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/add	1599833759861489	5
Mul_888	1599833760279490	4
Assign_432	1599833760337767	10
bert/encoder/layer_2/intermediate/dense/kernel/adam_m/read	-1	-1
Assign_570	1599833760336787	10
bert/encoder/layer_5/attention/self/value/bias	-1	-1
clip_by_global_norm/mul_134	1599833760275810	12
bert/encoder/layer_1/output/LayerNorm/beta/adam_v/read	-1	-1
Assign_123	1599833760340460	13
Assign_611	1599833760313373	12
Square_141	1599833760280939	4
Assign_76	1599833760303343	20
bert/encoder/layer_2/intermediate/dense/BiasAdd	1599833759863999	224
global_norm/L2Loss_109	1599833760129380	5
bert/encoder/layer_3/attention/self/query/kernel/read	-1	-1
bert/encoder/layer_3/attention/self/query/bias/adam_v/read	-1	-1
bert/encoder/layer_0/attention/self/query/bias/adam_m/read	-1	-1
bert/encoder/layer_0/output/LayerNorm/batchnorm/Rsqrt	1599833759842746	4
bert/encoder/layer_4/attention/self/dropout/mul	1599833759828195	99
Assign_171	1599833760340357	14
bert/encoder/layer_8/attention/self/value/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_6/attention/self/transpose_2_grad/transpose	1599833760132318	189
truediv_92	1599833760321965	21
Mul_835	1599833760278789	4
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v	-1	-1
truediv_89	1599833760321945	8
Assign_332	1599833760315053	10
bert/encoder/layer_1/attention/output/LayerNorm/gamma	-1	-1
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul	1599833759955194	38
bert/encoder/layer_2/attention/self/value/kernel/adam_m/read	-1	-1
add_392	1599833760320627	7
cls/predictions/transform/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/add_1	1599833759875125	73
mul_977	1599833760328229	4
bert/encoder/layer_7/attention/output/dropout/random_uniform/mul	-1	-1
Assign_521	1599833760311805	10
truediv_11	1599833760322846	4
Mul_34	1599833759808626	18
Mul_942	1599833760279436	4
bert/encoder/layer_6/output/dropout/mul	1599833759826456	51
sub_90	1599833760334575	21
gradients/bert/encoder/layer_11/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760022196	55
Sqrt_85	1599833760314740	17
Sqrt_139	1599833760316532	15
Assign_579	1599833760341359	41
Assign_279	1599833760334376	4
bert/encoder/layer_3/attention/output/LayerNorm/moments/variance	1599833759874856	31
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760218095	33
Assign_470	1599833760311222	11
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760019345	51
bert/encoder/layer_7/output/dropout/GreaterEqual	1599833759822766	40
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760138445	53
Mul_11	1599833760280048	4
add_618	1599833760327937	17
gradients/AddN_23	1599833760063685	95
Sqrt_123	1599833760307127	17
Square_117	1599833760281367	11
gradients/cls/seq_relationship/LogSoftmax_grad/Exp	1599833759990699	5
mul_359	1599833759813662	46
sub_154	1599833760336528	17
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760182352	73
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760204615	52
Sqrt_180	1599833760313104	4
Mul_346	1599833759818000	45
Sqrt_124	1599833760316068	8
add_146	1599833760326540	20
Mul_737	1599833759805931	10
Assign_496	1599833760300780	13
mul_467	1599833760333000	16
bert/embeddings/position_embeddings/read	-1	-1
AssignVariableOp	-1	-1
Square_115	1599833760276252	4
bert/encoder/layer_8/attention/self/value/kernel	-1	-1
truediv_9	1599833760322911	5
bert/encoder/layer_5/attention/self/value/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_2/attention/self/dropout/mul_grad/Mul	1599833760220975	141
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760094726	5
clip_by_global_norm/mul_175	1599833760274966	4
Mul_189	1599833759811398	4
add_547	1599833760318835	4
Assign_463	1599833760300579	4
add_77	1599833760319657	15
bert/encoder/layer_8/output/LayerNorm/gamma/adam_v/read	-1	-1
Mul_237	1599833759811974	17
add_439	1599833760320269	8
add_565	1599833760289971	4
Assign_207	1599833760340240	14
sub_114	1599833760339589	56
gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Neg	1599833760270965	51
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul	1599833759928424	39
bert/encoder/layer_9/output/dense/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_8/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760092391	54
gradients/bert/encoder/layer_3/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760201619	55
Assign_117	1599833760340445	13
truediv_164	1599833760323443	4
Mul_625	1599833760276246	4
add_267	1599833760317486	8
bert/encoder/layer_4/attention/output/dense/BiasAdd	1599833759887932	60
Mul_469	1599833760276834	4
Sqrt_186	1599833760312449	11
gradients/bert/encoder/layer_0/attention/self/dropout/mul_grad/Mul	1599833760265165	141
Assign_533	1599833760311721	41
add_540	1599833760318617	4
add_379	1599833760295530	60
Mul_418	1599833760294696	10
mul_58	1599833759812266	19
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
clip_by_global_norm/mul_121	1599833760275729	4
bert/encoder/layer_5/attention/output/dense/kernel/adam_m/read	-1	-1
Mul_168	1599833760276823	4
bert/encoder/layer_7/intermediate/dense/Pow	1599833759931143	366
bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1	1599833759869602	74
bert/embeddings/LayerNorm/moments/variance	1599833759828477	31
bert/encoder/layer_11/attention/self/dropout/Cast	1599833759825461	68
add_389	1599833760329337	59
Sqrt_183	1599833760312734	17
bert/encoder/layer_10/attention/self/key/bias/adam_v	-1	-1
clip_by_global_norm/mul_26	1599833760275299	12
Assign_44	1599833760310947	3
Square_135	1599833760276113	11
Mul_527	1599833759809534	45
Mul_626	1599833759816658	11
add_574	1599833760318926	7
mul_242	1599833760332719	16
bert/encoder/layer_2/output/dropout/random_uniform/RandomUniform	1599833759800688	46
truediv_40	1599833760322009	21
bert/encoder/layer_10/attention/self/value/bias/read	-1	-1
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760217645	74
sub_41	1599833760330945	4
bert/encoder/layer_10/attention/output/LayerNorm/gamma	-1	-1
bert/encoder/layer_10/output/dense/kernel/read	-1	-1
mul_343	1599833760326223	4
clip_by_global_norm/mul_95	1599833760273671	4
gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul	1599833760131814	159
bert/encoder/layer_4/attention/self/value/bias/adam_m	-1	-1
gradients/bert/encoder/layer_2/attention/self/transpose_2_grad/transpose	1599833760220784	189
Square_32	1599833760277605	5
Square_105	1599833760281318	11
add_108	1599833760317307	10
gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul_1	1599833760093090	661
add_602	1599833760289914	56
bert/embeddings/token_type_embeddings/adam_v/read	-1	-1
bert/encoder/layer_0/attention/output/dense/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Const_2	-1	-1
add_388	1599833760320486	43
sub_49	1599833760329956	5
mul_526	1599833760325662	4
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/sub	1599833760112097	99
add_571	1599833760327890	16
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760085650	53
gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul_1	1599833760202319	663
bert/encoder/layer_2/attention/self/dropout/random_uniform	-1	-1
gradients/AddN_63	1599833760195155	73
gradients/cls/predictions/transform/dense/Pow_grad/Pow	1599833759990563	7
gradients/cls/seq_relationship/Sum_grad/Tile	-1	-1
add_257	1599833760319879	8
Assign_359	1599833760307101	20
add_480	1599833760320210	7
bert/encoder/layer_5/attention/self/dropout/mul_1	1599833759900108	141
Assign_523	1599833760301655	12
cls/seq_relationship/output_weights/read	-1	-1
mul_1105	1599833759805543	12
Assign_369	1599833760340066	14
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul	1599833760088421	141
Mul_121	1599833760292234	17
bert/encoder/layer_7/attention/output/LayerNorm/gamma	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760161120	51
bert/encoder/layer_6/attention/self/key/bias	-1	-1
bert/encoder/layer_3/output/LayerNorm/batchnorm/add	1599833759882945	5
Sqrt_96	1599833760307314	9
Assign_532	1599833760301058	41
bert/encoder/layer_3/attention/self/value/bias	-1	-1
bert/encoder/layer_0/output/LayerNorm/batchnorm/sub	1599833759842922	54
bert/encoder/layer_0/attention/self/transpose_3	1599833759833522	189
Square_149	1599833760278863	12
bert/encoder/layer_3/intermediate/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_9/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760057412	110
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760182844	53
truediv_110	1599833760324533	4
Mul_1007/x	-1	-1
bert/encoder/layer_4/output/LayerNorm/gamma/adam_v/read	-1	-1
gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul	1599833760019753	642
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum	1599833760134112	54
bert/encoder/layer_5/attention/self/dropout/Cast	1599833759825243	70
bert/encoder/layer_10/attention/self/value/kernel/adam_v	-1	-1
bert/encoder/layer_6/attention/self/value/bias/read	-1	-1
bert/encoder/layer_0/intermediate/dense/bias/read	-1	-1
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760196154	53
bert/encoder/layer_9/attention/output/dense/bias/adam_v/read	-1	-1
Mul_1088	1599833759817845	12
bert/encoder/layer_4/output/LayerNorm/gamma/adam_m/read	-1	-1
Square_205	1599833760279905	4
bert/encoder/layer_5/attention/output/dropout/Cast	1599833759823953	35
Mul_535	1599833759807725	4
Mul_358	1599833760282015	39
bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1	1599833759963293	73
bert/encoder/layer_5/output/LayerNorm/beta/adam_v	-1	-1
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760050636	72
Sqrt_93	1599833760307772	9
mul_698	1599833760329113	7
add_563	1599833760318683	12
add_637	1599833760302192	21
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m/read	-1	-1
truediv_64	1599833760324252	58
truediv_143	1599833760325048	5
Mul_643	1599833760281566	4
bert/encoder/layer_9/output/dense/kernel	-1	-1
bert/encoder/layer_8/attention/self/MatMul	1599833759939440	164
gradients/bert/encoder/layer_9/output/LayerNorm/moments/variance_grad/Tile	1599833760050447	26
bert/encoder/layer_2/output/LayerNorm/moments/variance	1599833759869516	32
truediv_128	1599833760321073	58
truediv_191	1599833760323823	5
gradients/AddN_36	1599833760107340	68
mul_322	1599833760326435	4
bert/encoder/layer_3/attention/self/query/bias/adam_m/read	-1	-1
add_117	1599833760296780	60
bert/embeddings/LayerNorm/batchnorm/sub	1599833759829442	52
bert/encoder/layer_1/attention/self/value/MatMul	1599833759844320	630
gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul_1	1599833760185857	2472
bert/encoder/layer_7/intermediate/dense/mul_3	1599833759933378	279
Mul_195	1599833760277474	4
mul_230	1599833759814381	18
Sqrt_190	1599833760313201	8
Mul_132	1599833760292282	17
gradients/bert/encoder/layer_1/output/LayerNorm/moments/mean_grad/truediv	1599833760227313	47
bert/encoder/layer_8/attention/self/transpose_2	1599833759939249	189
add_234	1599833760293248	4
bert/encoder/layer_3/output/dense/bias/adam_v	-1	-1
gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul_1	1599833760263656	611
bert/encoder/layer_11/attention/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_3/attention/self/key/kernel/adam_m	-1	-1
cls/predictions/Reshape	-1	-1
Mul_968	1599833759817970	11
truediv_123	1599833760320835	9
mul_101	1599833759815478	46
Assign_592	1599833760301999	13
Sqrt_111	1599833760316574	44
mul_1085	1599833760328355	4
bert/encoder/layer_2/output/dense/MatMul	1599833759866738	2475
bert/encoder/layer_5/attention/self/value/kernel/read	-1	-1
Assign_189	1599833760341690	41
bert/encoder/layer_11/intermediate/dense/Tanh	1599833759986321	208
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760249471	5
sub_51	1599833760330877	8
gradients/bert/encoder/layer_1/attention/self/Reshape_3_grad/Reshape	-1	-1
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760006306	53
mul_419	1599833760329275	8
bert/encoder/layer_0/attention/output/dropout/mul_1	1599833759834411	73
truediv_91	1599833760321768	9
Assign_422	1599833760315284	10
gradients/bert/encoder/layer_2/attention/self/dropout/mul_1_grad/Mul	1599833760220684	99
add_360	1599833760305945	17
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/moments/mean_grad/Tile	1599833760160792	28
Mul_729	1599833760295457	9
bert/encoder/layer_10/output/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_2/attention/output/dropout/mul_1_grad/Mul	1599833760218618	52
bert/encoder/layer_10/output/dropout/random_uniform	-1	-1
bert/encoder/layer_11/output/LayerNorm/gamma/adam_m	-1	-1
Sqrt_198	1599833760312926	8
bert/encoder/layer_0/attention/output/dense/bias	-1	-1
bert/encoder/layer_9/attention/self/query/BiasAdd	1599833759952061	60
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760116956	51
Mul_326	1599833760292581	18
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760151080	68
Reshape	-1	-1
Assign_571	1599833760302641	12
global_norm/L2Loss_20	1599833760249464	5
truediv_59	1599833760321921	4
bert/encoder/layer_8/attention/self/query/bias/adam_v	-1	-1
add_625	1599833760328114	16
Sqrt_38	1599833760308774	10
bert/encoder/layer_9/attention/output/LayerNorm/moments/mean	1599833759955058	32
Mul_533	1599833759810614	4
add_78	1599833760328472	20
bert/encoder/layer_8/output/LayerNorm/batchnorm/sub	1599833759950027	55
add_646	1599833760291493	10
bert/encoder/layer_7/intermediate/dense/kernel/adam_v	-1	-1
Assign_96	1599833760335309	4
bert/encoder/layer_6/attention/self/query/MatMul	1599833759910022	631
Square_171	1599833760279307	12
sub_46	1599833760330856	8
Assign_194	1599833760306906	11
bert/encoder/layer_8/attention/self/Reshape_1	-1	-1
Mul_52	1599833760284507	4
Mul_640	1599833759817704	4
bert/encoder/layer_0/output/dropout/random_uniform/RandomUniform	1599833759800640	46
bert/encoder/layer_5/attention/self/key/bias/adam_v/read	-1	-1
global_norm/stack	1599833760273205	13
bert/encoder/layer_6/attention/output/dense/bias/adam_m	-1	-1
Mul_796	1599833759806472	11
Mul_491	1599833760277109	4
gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul_1	1599833760219451	614
truediv_161	1599833760323436	5
gradients/AddN_68	1599833760205546	95
Square_94	1599833760276872	4
Mul_1019	1599833759814024	11
gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul	1599833760070351	640
Assign_143	1599833760314167	41
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760240219	66
add_528	1599833760284865	17
add_18	1599833760284360	4
bert/encoder/layer_5/intermediate/dense/bias/read	-1	-1
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/add_1	1599833759942031	73
bert/encoder/layer_7/output/LayerNorm/batchnorm/add_1	1599833759936703	72
bert/encoder/layer_11/attention/self/transpose	1599833759979033	189
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760028386	53
bert/encoder/layer_4/intermediate/dense/kernel/adam_v	-1	-1
add_65	1599833760327327	56
clip_by_global_norm/mul_80	1599833760275575	40
Sqrt_118	1599833760306470	13
gradients/bert/encoder/layer_0/output/LayerNorm/moments/mean_grad/truediv	1599833760249416	47
global_norm/L2Loss_175	1599833760040434	31
sub_145	1599833760333233	8
add_484	1599833760294944	4
bert/pooler/dense/bias	-1	-1
bert/encoder/layer_0/attention/self/query/kernel/adam_m/read	-1	-1
add_414	1599833760306115	22
bert/encoder/layer_5/attention/output/dropout/mul_1	1599833759901383	73
Square_162	1599833760279016	6
Mul_245	1599833759812110	10
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum	1599833760266767	55
mul_198	1599833760326661	4
bert/encoder/layer_9/attention/self/dropout/mul	1599833759827388	100
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760062953	53
gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul	1599833760021224	160
gradients/bert/encoder/layer_8/attention/self/dropout/mul_1_grad/Mul	1599833760087986	99
Assign_312	1599833760339541	10
Square_38	1599833760277114	4
Sqrt_113	1599833760315927	46
Assign_215	1599833760315149	20
Assign_593	1599833760312647	13
bert/encoder/layer_6/attention/self/value/kernel	-1	-1
bert/encoder/layer_11/attention/self/value/MatMul	1599833759978211	633
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760261726	72
Assign_167	1599833760314241	20
sub_182	1599833760336474	17
sub_12	1599833760335929	16
bert/encoder/layer_3/attention/self/key/kernel	-1	-1
Sqrt_108	1599833760314889	7
Sqrt_129	1599833760316123	45
Mul_290	1599833759818607	17
add_504	1599833760284831	4
bert/encoder/layer_4/output/dropout/random_uniform/mul	-1	-1
global_norm/L2Loss_72	1599833760182135	5
bert/encoder/layer_11/attention/output/LayerNorm/gamma	-1	-1
bert/encoder/layer_6/output/dense/kernel/adam_m	-1	-1
mul_1095	1599833760328166	4
bert/encoder/layer_3/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_9/output/LayerNorm/beta/adam_v/read	-1	-1
Mul_383	1599833760280451	4
bert/encoder/layer_6/output/LayerNorm/beta/adam_m/read	-1	-1
Mul_374	1599833760293243	4
Mul_22	1599833759810442	4
bert/encoder/layer_2/output/dense/bias/adam_v	-1	-1
gradients/bert/encoder/layer_6/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760130284	55
Mul_481	1599833759811219	4
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760085506	47
Mul_543	1599833759809580	4
bert/encoder/layer_9/attention/self/Softmax	1599833759953212	323
truediv_133	1599833760324892	5
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m	-1	-1
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760094813	53
Mul_63	1599833760284397	4
Square_87	1599833760277076	12
add_467	1599833760293739	22
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2	1599833759875015	53
Mul_581	1599833759814814	18
bert/encoder/layer_3/attention/output/LayerNorm/beta/read	-1	-1
clip_by_global_norm/mul_19	1599833760274601	4
Mul_887	1599833759808064	4
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760094603	65
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760129128	51
truediv_121	1599833760324786	8
clip_by_global_norm/mul_6	1599833760274499	12
Sqrt_61	1599833760307957	9
Square_198	1599833760279713	4
gradients/bert/encoder/layer_7/output/dropout/mul_grad/Mul	1599833760095145	73
bert/encoder/layer_10/intermediate/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_10/attention/self/MatMul	1599833759966212	162
bert/encoder/layer_11/attention/output/LayerNorm/moments/mean	1599833759981841	32
bert/encoder/layer_8/attention/output/LayerNorm/beta	-1	-1
Assign_504	1599833760336512	4
bert/encoder/layer_2/output/dense/kernel/adam_m/read	-1	-1
Mul_169	1599833759810523	4
bert/encoder/layer_3/attention/self/value/bias/read	-1	-1
Mul_211	1599833760277120	4
gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul_1	1599833760070996	614
bert/encoder/layer_7/attention/self/key/kernel/adam_m/read	-1	-1
Mul_590	1599833760293264	9
add_372	1599833760320066	4
bert/encoder/layer_6/attention/self/query/kernel/adam_m/read	-1	-1
bert/embeddings/LayerNorm/beta/read	-1	-1
bert/encoder/layer_7/output/dense/bias/adam_v	-1	-1
Mul_775	1599833760276137	39
gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/mul	1599833759878579	194
add_593	1599833760301629	9
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_10/output/dense/kernel/adam_m/read	-1	-1
bert/embeddings/word_embeddings/adam_m	-1	-1
bert/encoder/layer_8/intermediate/dense/Pow	1599833759944524	365
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760239681	52
global_norm/L2Loss_142	1599833760085555	5
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760049705	72
Mul_1092	1599833760279724	4
Mul_767	1599833760294489	9
add_131	1599833760317134	13
Mul_271	1599833759817657	45
bert/encoder/layer_4/attention/self/transpose_1	1599833759885533	189
bert/encoder/layer_9/attention/self/key/bias/read	-1	-1
add_531	1599833760327577	16
gradients/cls/predictions/Sum_1_grad/Reshape	-1	-1
clip_by_global_norm/mul_148	1599833760276048	4
bert/encoder/layer_10/attention/self/query/bias/adam_v/read	-1	-1
Mul_235	1599833760283237	9
bert/encoder/layer_0/intermediate/dense/bias	-1	-1
bert/encoder/layer_3/intermediate/dense/kernel	-1	-1
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m	-1	-1
Assign_209	1599833760308025	20
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760040741	68
Mul_502	1599833760276410	4
Mul_170	1599833760282447	4
bert/encoder/layer_1/output/LayerNorm/batchnorm/sub	1599833759856327	54
gradients/bert/embeddings/LayerNorm/moments/variance_grad/Tile	1599833760271634	31
Mul_804	1599833760284837	4
Assign_433	1599833760304931	12
global_norm/L2Loss_45	1599833760217845	5
bert/encoder/layer_2/attention/self/value/bias/adam_m	-1	-1
Assign_160	1599833760297709	14
truediv_109	1599833760324527	4
gradients/AddN_65	1599833760196336	96
global_norm/L2Loss_49	1599833760211090	33
bert/encoder/layer_6/attention/output/dense/bias/read	-1	-1
truediv_165	1599833760323587	4
bert/encoder/layer_2/attention/output/dense/kernel/adam_m/read	-1	-1
global_norm/L2Loss_14	1599833760262451	5
bert/encoder/layer_3/intermediate/dense/bias/adam_v/read	-1	-1
Assign_616	1599833760302876	12
Assign_581	1599833760312590	41
Assign_610	1599833760302799	11
Assign_604	1599833760302038	4
clip_by_global_norm/mul_92	1599833760273876	12
add_591	1599833760318829	4
mul_208	1599833759814783	19
bert/encoder/layer_3/attention/self/query/bias	-1	-1
Square_102	1599833760277141	4
gradients/AddN_14	1599833760040360	72
bert/encoder/layer_3/attention/self/dropout/mul_1	1599833759873342	141
sub_192	1599833760336927	59
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759928419	4
bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference	1599833759968487	54
Sqrt_175	1599833760313016	44
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/add_1	1599833759915277	73
clip_by_global_norm/mul_21	1599833760275294	4
global_step/cond/Read/ReadVariableOp	-1	-1
Assign_164	1599833760309022	3
Mul_105	1599833759811291	4
truediv_187	1599833760323580	5
global_norm/L2Loss_28	1599833760242159	5
Assign_157	1599833760304803	12
Square_109	1599833760280730	4
bert/encoder/layer_8/attention/self/key/kernel/adam_m/read	-1	-1
mul_408	1599833760328729	4
add_299	1599833760297924	22
bert/encoder/layer_5/attention/self/transpose_1	1599833759898910	190
Mul_341	1599833759819342	11
clip_by_global_norm/mul_24	1599833760275332	12
add_457	1599833760295435	9
mul_294	1599833759814703	13
add_523	1599833760318982	18
Sqrt_46	1599833760309740	9
bert/encoder/layer_6/attention/output/dense/MatMul	1599833759914073	632
truediv_10	1599833760322955	17
sub_6	1599833760335948	17
gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul	1599833760035379	2500
Sqrt_48	1599833760307264	5
gradients/cls/predictions/transform/dense/mul_3_grad/Mul	1599833760004837	14
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
Square_174	1599833760279431	4
add_174	1599833760283090	4
Assign_91	1599833760297373	12
gradients/AddN_28	1599833760084647	73
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759955188	4
Mul_1094	1599833760291238	8
bert/encoder/layer_10/output/LayerNorm/gamma/adam_m/read	-1	-1
add_206	1599833760319802	12
Assign_490	1599833760301162	15
gradients/bert/encoder/layer_5/attention/self/transpose_grad/transpose	1599833760156986	189
bert/encoder/layer_5/intermediate/dense/kernel/adam_v/read	-1	-1
gradients/AddN_35	1599833760106795	72
cls/seq_relationship/output_weights/adam_m	-1	-1
Assign_321	1599833760340225	13
Square_187	1599833760279672	11
gradients/AddN_82	1599833760249741	95
sub_148	1599833760333811	4
bert/encoder/layer_5/intermediate/dense/bias/adam_m	-1	-1
Sqrt_104	1599833760315830	8
gradients/AddN_1	1599833760004809	18
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub	1599833759888447	54
bert/encoder/layer_9/attention/self/dropout/GreaterEqual	1599833759823204	73
sub_136	1599833760333906	21
Mul_497	1599833759805021	18
gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1	1599833760021386	229
Sqrt_167	1599833760311924	16
gradients/bert/encoder/layer_9/attention/self/Mul_grad/Mul	1599833760067960	142
add_368	1599833760317207	15
Mul_72	1599833760278585	4
truediv_72	1599833760324610	21
bert/encoder/layer_2/output/LayerNorm/gamma/adam_m/read	-1	-1
sub_80	1599833760337526	59
gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul_1	1599833760130984	617
bert/encoder/layer_2/attention/self/value/bias/adam_m/read	-1	-1
Mul_771	1599833759807048	11
truediv_135	1599833760325250	8
global_norm/L2Loss_195	1599833760005812	5
Assign_445	1599833760300558	4
bert/encoder/layer_9/attention/output/dense/bias/read	-1	-1
mul_1090	1599833760328103	4
bert/encoder/layer_6/output/dense/bias	-1	-1
add_442	1599833760320564	42
add_261	1599833760329133	20
gradients/bert/encoder/layer_4/attention/self/Reshape_3_grad/Reshape	-1	-1
Assign_338	1599833760315000	11
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760005483	73
add_630	1599833760301498	21
Assign_35	1599833760310858	13
gradients/bert/encoder/layer_3/intermediate/dense/mul_3_grad/Mul_1	1599833759877988	196
Mul_501	1599833759811224	4
add_398	1599833760320397	4
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760129929	51
Mul_604	1599833759814928	44
truediv_190	1599833760323720	4
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m/read	-1	-1
global_norm/L2Loss_39	1599833760226544	16
bert/encoder/layer_11/attention/self/key/bias/adam_v	-1	-1
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1	1599833759955234	73
clip_by_global_norm/mul_172	1599833760274901	13
bert/encoder/layer_9/attention/self/key/bias	-1	-1
global_norm/L2Loss_91	1599833760153700	13
bert/encoder/layer_6/attention/self/value/bias/adam_v/read	-1	-1
Assign	1599833760340559	365
bert/encoder/layer_5/attention/self/transpose_3	1599833759900495	189
gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul	1599833760073137	2219
clip_by_global_norm/mul_5	1599833760274413	4
mul_682	1599833760329546	8
bert/encoder/layer_11/attention/output/dropout/Cast	1599833759824028	36
Assign_400	1599833760305552	23
add_405	1599833760316674	4
Mul_106	1599833760284786	4
sub_109	1599833760333068	4
add_139	1599833760326513	21
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760183083	5
Assign_389	1599833760316186	48
add_179	1599833760319782	4
bert/encoder/layer_1/attention/self/key/bias/adam_m	-1	-1
clip_by_global_norm/mul_129	1599833760275708	4
Mul_939	1599833760289994	4
add_142	1599833760317899	4
Assign_80	1599833760314036	11
bert/encoder/layer_2/output/LayerNorm/beta/read	-1	-1
Assign_249	1599833760334750	7
mul_247	1599833760325867	4
mul_558	1599833760326349	8
gradients/bert/encoder/layer_6/attention/output/dropout/mul_grad/Mul	1599833760130207	72
bert/encoder/layer_0/attention/self/key/kernel/adam_m	-1	-1
Mul_528	1599833760276919	39
sub_168	1599833760336427	16
Mul_164	1599833759809335	12
clip_by_global_norm/mul_140	1599833760275993	12
bert/encoder/layer_5/output/dense/bias/adam_v	-1	-1
Assign_417	1599833760342079	14
bert/encoder/layer_2/attention/self/value/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_2/intermediate/dense/mul	1599833759864986	194
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760239880	53
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760072779	73
bert/encoder/layer_10/intermediate/dense/kernel/adam_m/read	-1	-1
clip_by_global_norm/mul_111	1599833760275639	4
gradients/cls/predictions/transform/dense/mul_2_grad/Mul_1	1599833760004853	15
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760262494	52
Sqrt_193	1599833760312544	45
clip_by_global_norm/mul_101	1599833760273587	4
Mul_806	1599833759808349	17
Sqrt_99	1599833760307816	9
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v	-1	-1
bert/encoder/layer_0/attention/self/mul_1	1599833759798195	17
Assign_376	1599833760306143	13
Mul_113	1599833759809787	12
clip_by_global_norm/Select	1599833760273255	4
add_31	1599833760327303	16
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760248811	51
add_603	1599833760301001	56
Assign_28	1599833760300326	13
sub_153	1599833760331922	4
add_669	1599833760290899	17
bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference	1599833759896231	55
add_614	1599833760319307	4
Assign_41	1599833760310989	4
global_norm/L2Loss_158	1599833760063409	5
clip_by_global_norm/mul_150	1599833760274666	13
Assign_482	1599833760311852	11
Assign_457	1599833760301320	11
Assign_395	1599833760306652	11
add_268	1599833760292864	10
global_norm/L2Loss_119	1599833760115989	16
bert/pooler/dense/BiasAdd	1599833759990571	5
Mul_297	1599833760277234	4
bert/encoder/layer_7/intermediate/dense/BiasAdd	1599833759930919	222
sub_206	1599833760332440	8
Mul_636	1599833759816331	18
Mul_546/x	-1	-1
bert/encoder/layer_10/attention/self/value/BiasAdd	1599833759965578	60
mul_919	1599833760331743	13
bert/encoder/layer_11/attention/self/key/MatMul	1599833759977578	632
Mul_87	1599833760278683	39
bert/encoder/layer_7/intermediate/dense/bias/adam_m	-1	-1
gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul	1599833760087585	158
bert/encoder/layer_1/output/LayerNorm/beta/adam_m/read	-1	-1
add_253	1599833760317268	15
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760019290	53
gradients/bert/encoder/layer_0/attention/self/transpose_grad/transpose	1599833760267568	189
Mul_878	1599833760279010	4
bert/encoder/layer_1/output/LayerNorm/beta/adam_v	-1	-1
add_397	1599833760305374	9
Mul_382	1599833759820322	11
add_199	1599833760317459	13
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760249071	32
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760116691	65
bert/encoder/layer_1/output/LayerNorm/batchnorm/add	1599833759856145	5
Assign_596	1599833760312944	10
Assign_135	1599833760335084	9
clip_by_global_norm/mul_162	1599833760274680	39
bert/encoder/layer_1/attention/self/transpose	1599833759845141	189
mul_672	1599833760329893	15
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/add	1599833759888264	5
Assign_66	1599833760337105	10
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760138506	32
gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Sum/reduction_indices	-1	-1
global_norm/L2Loss_168	1599833760049488	5
bert/pooler/dense/bias/read	-1	-1
Mul_720	1599833759806101	13
bert/encoder/layer_11/intermediate/dense/MatMul	1599833759982276	2191
global_norm/L2Loss_82	1599833760166220	5
Mul_285	1599833759818094	22
Square_100	1599833760276709	4
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v	-1	-1
Assign_558	1599833760336552	4
sub_161	1599833760331882	5
Assign_67	1599833760303397	12
global_norm/L2Loss_105	1599833760134175	15
Mul_251	1599833760283074	4
add_678	1599833760319398	16
bert/encoder/layer_8/output/LayerNorm/gamma	-1	-1
add_529	1599833760300605	17
cls/seq_relationship/output_weights	-1	-1
mul_913	1599833760327974	4
sub_78	1599833760330540	7
gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul	1599833760066479	638
Square_91	1599833760277271	13
Square_143	1599833760276178	39
bert/encoder/layer_8/attention/output/dense/MatMul	1599833759940827	630
bert/encoder/layer_6/output/dense/bias/adam_m/read	-1	-1
mul_413	1599833759813469	18
Mul_826	1599833760290252	9
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760072289	54
gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul	1599833760108261	636
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2	1599833759928539	53
bert/encoder/layer_7/output/LayerNorm/gamma/adam_m	-1	-1
Mul_943	1599833759810529	4
Assign_441	1599833760339973	4
Mul_67	1599833759809693	18
bert/encoder/layer_3/output/dropout/GreaterEqual	1599833759822726	38
mul_693	1599833760329912	42
bert/encoder/layer_1/intermediate/dense/mul_1	1599833759852255	195
Sqrt_27	1599833760307019	12
add_278	1599833760292938	56
Square_133	1599833760281094	12
bert/encoder/layer_4/attention/output/dense/bias/adam_v	-1	-1
gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul	1599833760050995	2220
add_4	1599833760299180	523
gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul_1	1599833760181498	611
cls/predictions/Reshape_1	-1	-1
add_564	1599833760327681	17
add_319	1599833760282289	7
Assign_263	1599833760308690	20
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub	1599833759982146	54
bert/encoder/layer_3/output/dropout/random_uniform	-1	-1
Mul_705	1599833759806929	11
Assign_495	1599833760341153	13
add_370	1599833760293254	4
add_362	1599833760329476	17
add_181	1599833760304779	9
bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2	1599833759856271	55
Mul_512	1599833760276867	4
clip_by_global_norm/mul_56	1599833760275412	12
bert/encoder/layer_9/output/LayerNorm/batchnorm/Rsqrt	1599833759963247	4
Square_60	1599833760276426	4
Assign_40	1599833760300407	4
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2	1599833759955309	53
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760182555	69
mul_810	1599833759816105	18
bert/encoder/layer_8/attention/output/dense/bias/read	-1	-1
gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul	1599833760112341	244
Sqrt_195	1599833760313246	8
bert/encoder/layer_3/output/LayerNorm/beta/adam_m	-1	-1
truediv_156	1599833760323118	17
Mul_387	1599833759818795	17
bert/encoder/layer_10/attention/self/Mul	1599833759966376	99
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1	1599833759982015	74
Square_184	1599833760279822	4
mul_488	1599833759813786	18
Square_15	1599833760278642	40
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760240045	54
bert/encoder/layer_3/output/LayerNorm/batchnorm/mul	1599833759882957	39
bert/encoder/layer_11/intermediate/dense/mul	1599833759985453	195
Assign_108	1599833760335304	4
Assign_319	1599833760296512	11
sub_163	1599833760331757	4
bert/encoder/layer_10/attention/self/Reshape_3	-1	-1
truediv_195	1599833760323726	5
add_26	1599833760300390	4
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760227211	33
Mul_883	1599833760279268	4
bert/encoder/layer_11/attention/self/dropout/mul	1599833759827893	99
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760085029	69
Assign_522	1599833760336522	4
bert/encoder/layer_7/output/LayerNorm/batchnorm/mul	1599833759936479	38
bert/encoder/layer_9/output/dense/bias/read	-1	-1
Assign_569	1599833760312854	10
mul_1037	1599833760332483	42
mul_274	1599833760330676	43
gradients/bert/encoder/layer_4/output/dropout/mul_1_grad/Mul	1599833760161344	51
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1	1599833759861541	74
gradients/bert/encoder/layer_3/attention/self/Reshape_1_grad/Reshape	-1	-1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760107245	53
gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul	1599833760242370	159
bert/encoder/layer_8/intermediate/dense/mul_1	1599833759945958	194
mul_875	1599833759816782	45
add_312	1599833760282979	17
mul_827	1599833760327926	4
add_632	1599833760327955	17
bert/encoder/layer_6/output/LayerNorm/batchnorm/mul	1599833759923107	39
global_norm/L2Loss_26	1599833760244720	5
add_259	1599833760305089	22
bert/encoder/layer_3/attention/self/Reshape	-1	-1
Mul_777	1599833760281707	39
bert/encoder/layer_9/intermediate/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v/read	-1	-1
Mul_390	1599833760293492	17
Assign_425	1599833760315569	13
add_144	1599833760298604	25
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum	1599833760156188	54
bert/encoder/layer_5/output/LayerNorm/gamma/adam_v	-1	-1
gradients/cls/predictions/Sum_grad/Tile/multiples	-1	-1
gradients/bert/encoder/layer_6/attention/self/Mul_grad/Mul	1599833760134293	141
clip_by_global_norm/mul_139	1599833760275938	4
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760173797	33
bert/encoder/layer_8/intermediate/dense/bias/adam_m	-1	-1
Mul_109	1599833760278762	4
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760182960	66
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760116281	68
Mul_7	1599833760283464	365
gradients/bert/encoder/layer_3/output/LayerNorm/moments/mean_grad/Tile	1599833760182898	27
sub_119	1599833760329696	5
bert/encoder/layer_2/output/dropout/random_uniform	-1	-1
bert/encoder/layer_5/output/dense/kernel/read	-1	-1
bert/encoder/layer_9/attention/self/dropout/Cast	1599833759825104	68
add_113	1599833760283448	5
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760063014	32
sub_18	1599833760335867	56
sub_31	1599833760330347	7
add_80	1599833760303371	10
bert/encoder/layer_2/attention/output/LayerNorm/gamma	-1	-1
add_358	1599833760320444	10
gradients/bert/encoder/layer_4/attention/self/Reshape_grad/Reshape	-1	-1
bert/encoder/layer_6/attention/output/dense/bias/adam_v/read	-1	-1
Mul_399	1599833760276793	11
Square_113	1599833760281149	39
Assign_554	1599833760313131	10
Mul_629	1599833759816829	4
bert/encoder/layer_5/output/LayerNorm/moments/mean	1599833759909588	31
sub_126	1599833760333689	8
Assign_586	1599833760302694	12
bert/encoder/layer_11/attention/output/dropout/mul	1599833759825926	51
truediv_119	1599833760320814	8
Assign_150	1599833760335105	10
Assign_487	1599833760300846	4
Square_200	1599833760280020	4
bert/embeddings/LayerNorm/moments/mean/reduction_indices	-1	-1
Assign_358	1599833760296142	20
Assign_424	1599833760305189	12
Assign_607	1599833760302399	12
Assign_577	1599833760303216	10
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v	-1	-1
bert/encoder/layer_9/attention/self/value/kernel/adam_v	-1	-1
bert/encoder/layer_4/output/LayerNorm/beta/adam_v	-1	-1
Mul_258	1599833759817876	44
add_626	1599833760291416	12
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
bert/encoder/layer_8/attention/self/query/BiasAdd	1599833759938680	62
bert/encoder/layer_9/attention/output/dense/kernel/read	-1	-1
bert/encoder/layer_1/attention/self/key/bias/adam_v	-1	-1
bert/encoder/layer_0/attention/output/dense/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760050201	53
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760004719	16
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760107465	53
Mul_750	1599833759806171	4
mul_724	1599833759817185	13
Mul_395	1599833759820117	11
bert/encoder/layer_9/intermediate/dense/kernel/adam_m	-1	-1
Mul_45	1599833759809112	18
Assign_34	1599833760300259	13
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub	1599833759915221	55
Mul_396	1599833760294646	8
bert/encoder/layer_6/attention/self/query/BiasAdd	1599833759911922	61
bert/encoder/layer_10/attention/output/dropout/random_uniform/mul	-1	-1
add_659	1599833760328025	57
Assign_103	1599833760298528	12
Mul_437	1599833760276812	4
Mul_829	1599833760279442	12
clip_by_global_norm/mul_205	1599833760275277	4
bert/encoder/ones/shape_as_tensor	-1	-1
bert/encoder/layer_9/output/LayerNorm/gamma/adam_v/read	-1	-1
Assign_559	1599833760301786	11
bert/encoder/layer_8/output/dense/bias/read	-1	-1
bert/encoder/layer_0/attention/output/LayerNorm/moments/mean	1599833759834560	31
add_417	1599833760281571	4
gradients/bert/encoder/layer_6/attention/self/transpose_grad/transpose	1599833760134925	190
clip_by_global_norm/mul_13	1599833760274527	4
global_norm/L2Loss_33	1599833760233183	33
Assign_606	1599833760336701	4
add_367	1599833760297124	22
gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul	1599833760179534	643
clip_by_global_norm/mul_67	1599833760275481	4
global_norm/L2Loss_60	1599833760197925	5
bert/encoder/layer_11/intermediate/dense/kernel/adam_m/read	-1	-1
Assign_318	1599833760334178	10
add_6	1599833760326774	520
bert/encoder/layer_1/attention/self/transpose_1	1599833759845331	189
Square_120	1599833760280925	4
gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul	1599833760114578	641
bert/encoder/layer_4/output/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_3/attention/self/transpose_1_grad/transpose	1599833760201427	189
add_519	1599833760301346	9
gradients/bert/encoder/layer_8/attention/self/Reshape_3_grad/Reshape	-1	-1
bert/encoder/layer_0/intermediate/dense/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760028056	53
Sqrt_94	1599833760308290	9
Mul_702	1599833760295100	40
gradients/bert/encoder/layer_10/attention/self/Reshape_2_grad/Reshape	-1	-1
Mul_899	1599833760279224	4
Assign_383	1599833760307222	41
Assign_378	1599833760333860	10
Assign_488	1599833760311468	11
mul_145	1599833760332620	15
Mul_280	1599833759817482	10
truediv_131	1599833760325223	14
sub_104	1599833760337659	21
truediv_19	1599833760323074	5
bert/embeddings/LayerNorm/gamma/adam_m	-1	-1
sub_45	1599833760330176	4
cls/predictions/transform/dense/Pow	1599833759990542	19
bert/encoder/layer_0/attention/self/key/bias/adam_m	-1	-1
bert/encoder/layer_6/attention/self/value/MatMul	1599833759911288	632
clip_by_global_norm/mul_132	1599833760273277	4
Mul_1052	1599833760291168	10
bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference	1599833759928323	55
Square_33	1599833760276563	39
Mul_1077	1599833759810992	12
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v	-1	-1
add_476	1599833760320745	16
bert/encoder/layer_7/attention/self/Reshape	-1	-1
bert/encoder/layer_0/attention/self/sub	1599833759798144	24
Square_128	1599833760280887	4
global_norm/L2Loss_76	1599833760175762	5
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
clip_by_global_norm/mul_49	1599833760273430	5
sub_40	1599833760335040	21
add_381	1599833760320764	42
cls/predictions/transform/LayerNorm/beta/adam_v	-1	-1
add_443	1599833760329416	59
add_109	1599833760283373	56
Mul_955	1599833760291745	9
bert/encoder/layer_5/output/dropout/mul	1599833759826191	51
clip_by_global_norm/mul_55	1599833760273854	4
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760249231	52
bert/encoder/layer_10/attention/self/transpose_3	1599833759967416	189
bert/encoder/layer_2/output/LayerNorm/gamma	-1	-1
add_351	1599833760317603	11
bert/embeddings/Reshape_3	-1	-1
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760129256	69
truediv_144	1599833760320948	62
Mul_250	1599833759811789	10
gradients/bert/encoder/layer_2/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760225076	54
add_352	1599833760293437	23
add_39	1599833760284403	4
Mul_970	1599833759818702	11
Assign_430	1599833760295987	48
clip_by_global_norm/mul_82	1599833760275504	39
cls/seq_relationship/output_bias/adam_m	-1	-1
clip_by_global_norm/truediv	1599833760273243	5
gradients/AddN_12	1599833760028568	96
Square_148	1599833760278773	4
bert/encoder/layer_6/output/dense/kernel/adam_v/read	-1	-1
gradients/AddN_29	1599833760085193	69
add_344/y	-1	-1
mul_435	1599833760333018	42
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760028296	5
cls/predictions/mul	1599833759996042	433
Mul_803	1599833759807893	11
sub_60	1599833760337302	17
bert/encoder/layer_11/output/dense/kernel/read	-1	-1
add_525	1599833760284854	4
gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/Tile/multiples	-1	-1
global_step/add/_2560	1599833759828411	3
truediv_160	1599833760323297	58
Mul_506	1599833759812701	12
Assign_311	1599833760315100	20
bert/encoder/layer_6/intermediate/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_2/attention/output/dropout/GreaterEqual	1599833759822285	38
bert/encoder/layer_6/attention/self/dropout/random_uniform	-1	-1
bert/encoder/layer_9/attention/self/query/bias/adam_v/read	-1	-1
cls/predictions/transform/LayerNorm/batchnorm/sub	1599833759991028	11
Mul_770	1599833760281190	4
add_79	1599833760292208	11
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760106973	72
Assign_447	1599833760341097	13
bert/encoder/layer_8/output/dense/bias/adam_v	-1	-1
truediv_169	1599833760323537	4
add_214	1599833760282716	4
add_530	1599833760318603	12
Sqrt_168	1599833760312218	8
gradients/bert/encoder/layer_4/attention/self/transpose_grad/transpose	1599833760179093	189
Assign_348	1599833760339493	4
Mul_452	1599833759819575	4
bert/encoder/layer_0/attention/self/Reshape/shape	-1	-1
cls/predictions/transform/dense/kernel/adam_v	-1	-1
Assign_62	1599833760313792	10
add_355	1599833760328968	20
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v/read	-1	-1
gradients/bert/encoder/layer_6/intermediate/dense/mul_3_grad/Mul	1599833760122066	279
clip_by_global_norm/mul_130	1599833760275883	40
global_norm/L2Loss_15	1599833760261621	32
mul_391	1599833759813630	19
Assign_280	1599833760296932	13
add_205	1599833760303788	22
bert/encoder/layer_9/attention/self/Mul	1599833759952981	99
bert/pooler/dense/bias/adam_m/read	-1	-1
Assign_285	1599833760340475	40
truediv_21	1599833760323987	8
gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/mul_1	1599833760145230	280
gradients/bert/encoder/layer_7/output/dense/BiasAdd_grad/BiasAddGrad	1599833760095221	55
gradients/AddN_73	1599833760226423	119
Assign_392	1599833760316472	10
mul_59	1599833760331521	13
bert/encoder/layer_7/output/dense/kernel/read	-1	-1
bert/encoder/layer_9/attention/self/value/bias/adam_v/read	-1	-1
sub_88	1599833760334770	21
Assign_220	1599833760297272	20
Mul_126	1599833759811815	10
bert/encoder/layer_11/attention/self/value/bias/read	-1	-1
Mul_598	1599833760280747	4
Mul_1003	1599833760290412	17
Mul_363	1599833759818259	11
bert/encoder/layer_2/attention/output/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_10/attention/output/LayerNorm/moments/variance	1599833759968542	32
bert/encoder/layer_8/attention/self/Reshape_2	-1	-1
Assign_494	1599833760312518	10
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v/read	-1	-1
mul_537	1599833760326328	8
Mul_992	1599833760290993	16
add_14	1599833760327455	12
Assign_387	1599833760341991	41
Mul_394	1599833760281053	4
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760262654	73
Assign_435	1599833760341892	41
bert/encoder/layer_7/output/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_5/intermediate/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_1/attention/self/query/BiasAdd	1599833759844952	62
bert/encoder/layer_6/attention/self/Reshape_2	-1	-1
bert/encoder/layer_3/output/LayerNorm/beta/adam_v	-1	-1
bert/encoder/layer_10/attention/self/query/bias	-1	-1
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m	-1	-1
clip_by_global_norm/mul_42	1599833760273909	12
sub_106	1599833760339755	16
bert/encoder/layer_5/attention/self/dropout/random_uniform/RandomUniform	1599833759801700	84
add_49	1599833760284682	4
Assign_428	1599833760316022	3
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760116351	53
Assign_299	1599833760307837	12
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/sub	1599833760134191	100
bert/encoder/layer_2/attention/output/dense/bias	-1	-1
Mul_802	1599833760278767	4
gradients/bert/encoder/layer_9/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760068976	54
bert/encoder/layer_10/intermediate/dense/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul_1	1599833759980319	100
clip_by_global_norm/mul_74	1599833760273626	12
Mul_240	1599833760292384	16
mul_779	1599833760329815	43
sub_82	1599833760337413	60
gradients/bert/encoder/layer_8/intermediate/dense/Tanh_grad/TanhGrad	1599833760078430	274
clip_by_global_norm/mul_106	1599833760275924	12
Mul_246	1599833760282278	4
Square_51	1599833760280286	4
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m	-1	-1
add_421	1599833760296220	22
gradients/Reshape_2_grad/Reshape/strided_slice	-1	-1
Mul_706	1599833760281417	4
truediv_2	1599833760323953	5
global_norm/L2Loss_107	1599833760131609	12
Square_181	1599833760279355	12
bert/embeddings/one_hot/on_value	-1	-1
gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul_1	1599833760192673	2470
Assign_572	1599833760313222	11
Assign_474	1599833760336134	4
Mul_179	1599833760277611	4
Assign_233	1599833760308596	11
Mul_1097	1599833760279892	5
gradients/bert/encoder/layer_4/attention/self/Reshape_2_grad/Reshape	-1	-1
Mul_504	1599833760281968	4
add_516	1599833760318716	12
gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul	1599833760177016	642
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760151587	54
mul_446	1599833760332920	42
bert/encoder/ones	-1	-1
bert/encoder/layer_5/attention/output/dropout/random_uniform/RandomUniform	1599833759800140	65
bert/encoder/layer_3/attention/self/key/MatMul	1599833759870499	630
bert/encoder/layer_6/attention/self/transpose_2	1599833759912489	189
Mul_301	1599833759817781	17
bert/encoder/layer_6/output/dropout/random_uniform	-1	-1
Assign_246	1599833760337475	4
bert/encoder/layer_8/output/LayerNorm/beta/adam_v/read	-1	-1
add_52	1599833760284635	4
Square_65	1599833760276448	39
bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2	1599833759842868	52
Mul_1002	1599833759810600	13
add_203	1599833760317674	4
Assign_106	1599833760298875	4
Sqrt_6	1599833760310972	4
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v/read	-1	-1
gradients/bert/encoder/layer_7/attention/self/transpose_2_grad/transpose	1599833760110230	189
Square_176	1599833760279915	4
gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul	1599833760220278	161
bert/encoder/layer_1/attention/output/dense/bias/adam_v	-1	-1
bert/encoder/layer_11/attention/output/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul_1	1599833760247881	612
truediv_205	1599833760323946	5
bert/encoder/layer_8/intermediate/dense/BiasAdd	1599833759944300	222
global_norm/L2Loss_63	1599833760195229	32
Assign_162	1599833760334943	4
sub_55	1599833760330721	4
Assign_391	1599833760306195	13
Assign_598	1599833760302772	20
Mul_218	1599833760283009	12
Assign_573	1599833760341504	41
bert/encoder/layer_3/attention/output/dense/MatMul	1599833759873925	630
gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul_1	1599833760044974	612
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub	1599833759901835	55
bert/encoder/layer_8/intermediate/dense/bias/read	-1	-1
Mul_817	1599833759806808	18
gradients/bert/encoder/layer_10/intermediate/dense/mul_3_grad/Mul	1599833760033550	278
bert/encoder/layer_0/attention/output/dropout/random_uniform	-1	-1
clip_by_global_norm/mul_125	1599833760275877	4
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760027636	51
mul_940	1599833760327873	4
bert/encoder/layer_6/attention/output/dropout/mul	1599833759826297	52
add_508	1599833760300662	16
bert/encoder/layer_9/attention/self/value/kernel/adam_m	-1	-1
bert/encoder/layer_5/attention/self/dropout/random_uniform	-1	-1
bert/encoder/layer_4/output/dense/kernel/read	-1	-1
mul_811	1599833760331675	12
Mul_473	1599833759819959	17
Assign_262	1599833760297947	20
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760129490	54
Mul_1059	1599833759807140	12
bert/encoder/layer_10/attention/self/query/kernel/read	-1	-1
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m	-1	-1
mul_521	1599833760330896	42
clip_by_global_norm/mul_94	1599833760273576	4
bert/encoder/layer_4/output/dense/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/mul	1599833759972265	195
gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul	1599833760234448	2502
Square_66	1599833760280435	4
bert/encoder/layer_2/intermediate/dense/add	1599833759865380	279
add_154	1599833760282283	4
gradients/bert/encoder/layer_3/attention/self/Reshape_grad/Reshape	-1	-1
Assign_39	1599833760336014	4
global_norm/L2Loss_152	1599833760071636	5
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760116153	51
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760160578	31
sub_172	1599833760336445	17
Assign_472	1599833760300891	4
add_543	1599833760318735	40
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760041271	5
bert/encoder/layer_7/attention/output/LayerNorm/beta/read	-1	-1
mul_489	1599833760330385	15
Sqrt_23	1599833760313964	20
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760173494	52
bert/encoder/layer_5/attention/output/dense/bias	-1	-1
add_526	1599833760300585	4
bert/encoder/layer_0/output/LayerNorm/gamma	-1	-1
gradients/AddN_32	1599833760094360	69
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760027895	32
gradients/bert/encoder/layer_2/attention/self/Reshape_1_grad/Reshape	-1	-1
Assign_467	1599833760311170	13
global_norm/L2Loss_193	1599833760012043	33
add_541	1599833760289770	60
add_635	1599833760319023	8
Sqrt_148	1599833760311123	4
mul_133	1599833759813314	13
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum	1599833760045588	54
bert/encoder/layer_9/output/dropout/random_uniform	-1	-1
bert/encoder/layer_1/attention/self/key/bias/adam_v/read	-1	-1
add_115	1599833760317997	4
bert/encoder/layer_5/attention/output/dense/kernel/adam_v	-1	-1
Square_202	1599833760279718	4
Assign_552	1599833760336770	4
gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul	1599833760046842	639
Sqrt_95	1599833760309522	45
gradients/AddN_5	1599833760006488	96
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/add_1	1599833759834922	73
Mul_543/x	-1	-1
gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1	1599833760178849	243
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1	1599833759928465	73
Mul_568	1599833760294770	9
bert/encoder/layer_5/attention/self/query/kernel/adam_m	-1	-1
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/add	1599833759941794	5
bert/encoder/layer_7/attention/self/key/bias/read	-1	-1
bert/encoder/layer_6/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_0/attention/output/dense/bias/adam_m/read	-1	-1
Square_164	1599833760279495	4
global_norm/L2Loss_128	1599833760106785	9
bert/encoder/layer_3/attention/output/dropout/random_uniform	-1	-1
bert/encoder/layer_11/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v	-1	-1
PolynomialDecay/Cast_2	1599833759812608	12
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_0/attention/self/dropout/Shape	-1	-1
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760227565	73
add_335	1599833760326101	66
gradients/AddN_71	1599833760217885	69
bert/encoder/layer_8/attention/self/value/bias/read	-1	-1
bert/encoder/layer_10/attention/output/dropout/random_uniform	-1	-1
bert/encoder/layer_8/attention/self/value/bias	-1	-1
bert/encoder/layer_11/attention/output/dense/bias/adam_m/read	-1	-1
Mul_153	1599833759809817	18
cls/predictions/BiasAdd	1599833759994552	343
gradients/bert/encoder/layer_0/intermediate/dense/mul_2_grad/Mul_1	1599833760255011	278
bert/encoder/layer_10/output/LayerNorm/batchnorm/Rsqrt	1599833759976638	4
Mul_990	1599833760279645	12
clip_by_global_norm/mul_160	1599833760274773	40
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760139099	73
Mul_1032	1599833759815666	45
truediv_71	1599833760324317	4
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760160822	33
gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul	1599833760212358	2500
add_331	1599833760316952	7
Mul_1022	1599833759813846	12
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760205472	72
Mul_1086	1599833759818377	11
bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference	1599833759976542	55
bert/encoder/layer_0/intermediate/dense/BiasAdd	1599833759837191	224
bert/encoder/layer_7/intermediate/dense/kernel/adam_v/read	-1	-1
Sqrt_3	1599833760311091	4
mul_725	1599833760333469	15
bert/encoder/layer_0/output/dense/MatMul	1599833759839933	2473
Mul_755	1599833759806687	18
Mul_25	1599833760284797	4
bert/encoder/layer_1/attention/self/query/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760041468	73
gradients/AddN_40	1599833760117083	96
Assign_216	1599833760339498	8
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760117009	73
bert/encoder/layer_0/attention/self/dropout/Cast	1599833759825388	70
add_520	1599833760318874	4
bert/encoder/layer_6/attention/self/value/bias	-1	-1
add_102	1599833760317059	4
Mul_118	1599833759811063	12
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760161065	53
gradients/cls/seq_relationship/LogSoftmax_grad/sub	1599833759990744	4
Mul_27	1599833759807402	17
bert/encoder/layer_9/intermediate/dense/MatMul	1599833759955494	2193
add_303	1599833760298401	9
bert/encoder/layer_7/output/dense/bias	-1	-1
mul_477	1599833759815230	18
add_502	1599833760306465	4
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Reshape/shape	-1	-1
Square_172	1599833760279420	4
add_242	1599833760303954	4
add_148	1599833760298922	9
mul_1079	1599833759805121	20
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
truediv_54	1599833760321677	18
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_7/attention/output/dense/MatMul	1599833759927447	630
bert/encoder/layer_0/attention/output/dropout/mul	1599833759826033	51
Assign_7	1599833760300379	10
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760129664	66
bert/encoder/layer_8/output/LayerNorm/moments/mean	1599833759949722	31
Square_196	1599833760280015	4
Less	-1	-1
Assign_222	1599833760337388	4
Mul_657	1599833760281380	12
global_norm/L2Loss_198	1599833759990926	5
bert/encoder/layer_7/attention/self/query/BiasAdd	1599833759925303	62
gradients/bert/encoder/layer_0/attention/self/Reshape_2_grad/Reshape	-1	-1
Assign_3	1599833760341552	4
gradients/bert/encoder/layer_6/output/dropout/mul_1_grad/Mul	1599833760117181	51
Square_112	1599833760280741	4
Square_80	1599833760276806	4
clip_by_global_norm/mul_7	1599833760274549	4
Mul_474	1599833760277090	12
mul_139	1599833760325765	4
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m	-1	-1
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum	1599833760271471	32
Assign_500	1599833760311770	4
add_386	1599833760294842	59
bert/encoder/layer_0/attention/output/LayerNorm/beta	-1	-1
Mul_550	1599833759812638	18
gradients/bert/encoder/layer_4/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760167945	109
truediv_132	1599833760320879	8
Mul_1065	1599833760279583	12
mul_1053	1599833760328155	4
Assign_137	1599833760309441	12
bert/encoder/layer_0/attention/self/value/bias	-1	-1
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760261655	69
Assign_304	1599833760297657	13
bert/encoder/layer_7/attention/self/value/bias/adam_m	-1	-1
mul_692	1599833759817144	40
clip_by_global_norm/mul_190	1599833760275096	4
Sqrt_13	1599833760310983	4
truediv_12	1599833760322918	17
Sqrt_43	1599833760314062	16
bert/encoder/layer_0/intermediate/dense/kernel/read	-1	-1
cls/predictions/add	1599833759821104	18
Square_89	1599833760276839	13
bert/encoder/layer_6/attention/self/Reshape	-1	-1
Mul_68	1599833760284519	12
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760005409	73
Assign_499	1599833760301101	4
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760129731	47
cls/predictions/transform/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_2/attention/self/query/bias/adam_m	-1	-1
Sqrt_165	1599833760311388	12
Mul_893	1599833760278957	13
Sqrt_125	1599833760316418	8
mul_574	1599833759816311	13
add_22	1599833760300274	17
gradients/bert/encoder/layer_10/attention/self/Mul_grad/Mul	1599833760045768	141
Square_4	1599833760278347	4
bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2	1599833759990137	53
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760271505	65
Assign_436	1599833760305239	47
gradients/bert/encoder/layer_5/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760145884	108
Sqrt_30	1599833760308147	9
Square_67	1599833760280571	4
clip_by_global_norm/mul_181	1599833760275160	4
bert/encoder/layer_3/attention/self/transpose_3	1599833759873734	189
gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/mul_1	1599833760233688	279
truediv_80	1599833760324458	62
Assign_200	1599833760314415	4
bert/encoder/layer_11/attention/self/key/BiasAdd	1599833759978908	61
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum	1599833760222579	54
bert/encoder/layer_0/attention/output/dense/kernel/read	-1	-1
bert/encoder/layer_2/output/dropout/Cast	1599833759824367	37
add_72	1599833760292105	9
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760085099	53
bert/encoder/layer_6/attention/self/key/kernel/adam_m	-1	-1
add_32	1599833760284513	4
gradients/bert/encoder/layer_9/intermediate/dense/Tanh_grad/TanhGrad	1599833760056291	275
Assign_609	1599833760336856	11
mul_8	1599833759812756	367
Assign_138	1599833760335218	10
bert/embeddings/token_type_embeddings/adam_m	-1	-1
add_8	1599833760303161	9
add_554	1599833760318711	4
Mul_369	1599833760293007	4
bert/encoder/layer_2/attention/self/Reshape_1	-1	-1
Mul_315	1599833760282677	13
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760129821	52
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759848095	4
gradients/AddN_42	1599833760128876	72
bert/encoder/layer_4/attention/self/query/bias/adam_v	-1	-1
add_185	1599833760317445	12
gradients/bert/encoder/layer_9/attention/self/Reshape_2_grad/Reshape	-1	-1
Mul_264	1599833759817409	4
bert/encoder/layer_10/intermediate/dense/kernel/adam_v	-1	-1
add_382	1599833760329625	64
truediv_196	1599833760323829	5
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
Assign_512	1599833760311781	3
cls/predictions/transform/dense/mul_2	1599833759990711	7
Square_157	1599833760279055	4
add_246	1599833760320151	15
bert/encoder/layer_4/intermediate/dense/mul_1	1599833759892434	194
bert/encoder/layer_1/attention/output/dense/bias/adam_m	-1	-1
Mul_1096	1599833759814327	5
Mul_979	1599833760279369	12
clip_by_global_norm/mul_32	1599833760273998	39
mul_90	1599833759813273	40
gradients/AddN_70	1599833760217339	72
bert/encoder/layer_4/output/LayerNorm/gamma/adam_v	-1	-1
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760085264	55
Assign_520	1599833760301129	4
gradients/bert/embeddings/MatMul_grad/MatMul_1	1599833760272048	104
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/encoder/layer_1/output/dropout/mul_grad/Mul	1599833760227789	73
truediv_126	1599833760325203	8
bert/encoder/layer_9/output/dense/MatMul	1599833759960432	2474
bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference	1599833759981875	55
global_norm/L2Loss_70	1599833760182111	5
global_norm/L2Loss_124	1599833760109515	5
bert/encoder/layer_1/attention/self/value/kernel/adam_v/read	-1	-1
cls/predictions/output_bias/read	-1	-1
bert/encoder/layer_1/attention/self/add	1599833759845973	130
gradients/AddN_52	1599833760160033	119
bert/encoder/layer_7/attention/output/dense/BiasAdd	1599833759928078	61
Mul_686	1599833760281588	4
Square_31	1599833760277512	39
gradients/bert/encoder/layer_4/intermediate/dense/mul_1_grad/Mul_1	1599833760167098	195
add_674	1599833760302348	10
bert/embeddings/GatherV2	1599833759813125	78
Mul_318	1599833759818173	11
clip_by_global_norm/mul_141	1599833760275683	4
add_636	1599833760291044	22
Sqrt_200	1599833760313592	8
gradients/bert/encoder/layer_0/attention/self/Mul_grad/Mul	1599833760266947	141
gradients/bert/encoder/layer_1/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760234337	110
add_176	1599833760317827	8
Sqrt_39	1599833760309303	16
Mul_135	1599833759810512	4
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760072124	53
Assign_181	1599833760296484	12
gradients/bert/pooler/strided_slice_grad/StridedSliceGrad	1599833759990954	31
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760249559	53
Square_47	1599833760280204	40
add_395	1599833760316844	4
mul_424	1599833760326229	4
Assign_297	1599833760334382	5
clip_by_global_norm/mul_90	1599833760273657	12
gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul	1599833760223817	639
Mul_991	1599833759819223	17
sub_177	1599833760332459	8
Mul_426	1599833760280473	4
global_norm/L2Loss_156	1599833760065229	5
bert/encoder/layer_2/attention/self/query/kernel/adam_m/read	-1	-1
bert/encoder/layer_10/output/dense/bias/read	-1	-1
Sqrt_64	1599833760306884	10
Mul_958	1599833760279184	39
bert/encoder/layer_4/output/dense/kernel/adam_v/read	-1	-1
Mul_234	1599833759811763	11
add_385	1599833760320096	7
bert/encoder/layer_7/output/LayerNorm/beta/adam_v/read	-1	-1
bert/encoder/layer_4/intermediate/dense/bias/adam_v	-1	-1
Sqrt_142	1599833760316016	4
Square_154	1599833760278794	4
clip_by_global_norm/mul_176	1599833760275119	39
mul_284	1599833760328617	4
add_555	1599833760290069	9
bert/encoder/layer_5/attention/self/query/kernel	-1	-1
Assign_136	1599833760298682	12
add_3	1599833760283832	521
bert/encoder/layer_2/attention/output/LayerNorm/beta	-1	-1
add_220	1599833760328660	56
Mul_890	1599833760290771	9
bert/encoder/layer_10/attention/self/value/kernel/adam_v/read	-1	-1
bert/encoder/layer_8/output/dropout/random_uniform/RandomUniform	1599833759800880	46
mul_171	1599833760326021	8
gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul	1599833760134436	246
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760027833	54
bert/encoder/layer_4/attention/self/value/kernel/adam_v/read	-1	-1
bert/encoder/layer_4/attention/output/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m	-1	-1
Assign_542	1599833760313110	3
cls/predictions/output_bias/adam_m	-1	-1
bert/encoder/layer_11/attention/self/transpose_3	1599833759980809	189
Mul_645	1599833759815752	17
gradients/bert/encoder/layer_9/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760066422	55
sub_69	1599833760333062	4
mul_117	1599833760328405	7
add_363	1599833760282004	4
add_666	1599833760291897	4
Mul_55	1599833760278566	11
truediv_186	1599833760323518	18
PolynomialDecay/truediv	1599833759822933	4
add_275	1599833760282441	5
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760063118	55
Sqrt_72	1599833760315741	9
bert/encoder/layer_5/output/dropout/random_uniform	-1	-1
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760239735	74
gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul	1599833760243399	640
clip_by_global_norm/mul_206	1599833760275208	4
bert/encoder/layer_1/attention/self/dropout/Cast	1599833759825605	69
bert/encoder/layer_0/attention/self/key/kernel/adam_m/read	-1	-1
mul_499	1599833759814052	18
add_237	1599833760282384	17
bert/encoder/layer_1/attention/self/query/bias/adam_v/read	-1	-1
bert/encoder/layer_4/attention/self/Reshape_2	-1	-1
add_281	1599833760328743	59
Mul_94	1599833759808752	11
Mul_695	1599833760280882	4
mul_752	1599833760329494	4
gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul_1	1599833760026708	612
add_317	1599833760296369	4
gradients/bert/encoder/layer_7/attention/self/transpose_3_grad/transpose	1599833760109536	189
gradients/bert/encoder/layer_5/attention/output/dropout/mul_1_grad/Mul	1599833760152249	52
Mul_904	1599833760279279	13
sub_135	1599833760333727	10
Mul_906	1599833760290157	17
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760085439	66
Mul_378	1599833759820085	17
add_248	1599833760294666	8
global_norm/L2Loss_202	1599833760004749	4
bert/encoder/layer_9/intermediate/dense/kernel/adam_m/read	-1	-1
Square_25	1599833760280075	12
Mul_146	1599833759808414	12
gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul_1	1599833760214860	2467
bert/encoder/layer_5/attention/self/key/bias	-1	-1
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul_1	1599833759900007	99
Mul_269	1599833759817537	45
global_norm/L2Loss_34	1599833760232615	5
clip_by_global_norm/mul_168	1599833760274887	13
bert/encoder/layer_9/attention/self/Reshape_3	-1	-1
gradients/bert/encoder/layer_6/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760135310	55
bert/encoder/layer_8/attention/self/query/kernel	-1	-1
Assign_16	1599833760300293	13
bert/encoder/layer_1/attention/output/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_9/attention/self/query/kernel/read	-1	-1
sub_47	1599833760330974	8
bert/encoder/layer_9/output/dropout/Cast	1599833759824633	36
bert/encoder/layer_0/attention/self/value/bias/adam_v	-1	-1
clip_by_global_norm/mul_164	1599833760274881	4
clip_by_global_norm/mul_142	1599833760275734	4
Square_74	1599833760280456	4
bert/encoder/layer_8/intermediate/dense/MatMul	1599833759942105	2193
Assign_114	1599833760334813	10
cls/predictions/transform/LayerNorm/gamma/adam_v	-1	-1
Mul_74	1599833760284618	4
gradients/bert/encoder/layer_0/output/LayerNorm/moments/variance_grad/truediv	1599833760249506	51
Mul_847	1599833759807558	11
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
Mul_641	1599833760276054	4
bert/encoder/layer_9/output/dense/BiasAdd	1599833759962908	61
Mul_685	1599833759817069	10
bert/encoder/layer_2/intermediate/dense/mul_1	1599833759865661	195
Mul_524	1599833759808993	12
global_norm/L2Loss_182	1599833760027321	5
bert/encoder/layer_10/attention/output/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_1/attention/self/value/bias	-1	-1
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760196011	47
add_656	1599833760290796	60
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760028441	52
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760151727	32
Assign_377	1599833760316435	10
bert/embeddings/ExpandDims	-1	-1
Mul_892	1599833759808928	18
bert/encoder/layer_7/attention/output/dropout/random_uniform	-1	-1
bert/encoder/layer_11/intermediate/dense/Pow	1599833759984692	366
bert/encoder/layer_11/attention/output/dropout/random_uniform/RandomUniform	1599833759800255	46
Assign_63	1599833760341573	14
bert/encoder/layer_1/intermediate/dense/BiasAdd	1599833759850594	222
clip_by_global_norm/mul_96	1599833760273934	39
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
gradients/AddN_62	1599833760189689	364
bert/encoder/layer_8/output/LayerNorm/beta/read	-1	-1
gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul_1	1599833760031073	2469
bert/encoder/layer_8/attention/self/dropout/mul_1	1599833759940259	141
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759874895	4
clip_by_global_norm/mul_16	1599833760274560	39
Mul_60	1599833759808258	11
cls/predictions/truediv	1599833760004638	5
bert/encoder/layer_4/output/dropout/mul	1599833759826562	51
add_316	1599833760281974	4
gradients/bert/encoder/layer_5/output/dropout/mul_grad/Mul	1599833760139323	73
add_235	1599833760304462	4
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub	1599833759928594	54
Assign_77	1599833760313840	20
bert/encoder/layer_8/output/dropout/GreaterEqual	1599833759822567	37
add_208	1599833760281998	4
Sqrt_162	1599833760311449	9
bert/encoder/layer_1/output/dense/kernel/read	-1	-1
add_118	1599833760317090	42
Mul_18	1599833759809598	14
clip_by_global_norm/mul_10	1599833760274513	12
Mul_287	1599833759818347	11
add_549	1599833760300723	56
Mul_998	1599833760291404	4
bert/encoder/layer_1/attention/self/MatMul_1	1599833759846682	243
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759981970	4
sub_37	1599833760331036	4
global_norm/L2Loss_22	1599833760248495	5
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
mul_854	1599833760327743	4
Assign_454	1599833760300857	13
cls/predictions/transform/LayerNorm/moments/SquaredDifference	1599833759990821	12
gradients/bert/embeddings/dropout/mul_grad/Mul	1599833760270743	71
add_45	1599833760327390	16
Mul_620	1599833760281342	4
bert/encoder/layer_10/attention/self/key/kernel/adam_v/read	-1	-1
bert/encoder/layer_9/output/add	1599833759963044	73
Assign_134	1599833760307752	7
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760085321	53
bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference	1599833759882854	56
add_130	1599833760296884	17
Mul_330	1599833760276432	4
Assign_426	1599833760339647	4
bert/encoder/layer_6/attention/self/key/BiasAdd	1599833759911984	60
add_219	1599833760319815	42
add_566	1599833760301107	4
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v/read	-1	-1
bert/encoder/layer_4/intermediate/dense/bias/adam_m	-1	-1
clip_by_global_norm/mul_84	1599833760273770	4
Mul_65	1599833759809144	19
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760072428	32
Sqrt_160	1599833760311828	13
add_697	1599833760302850	10
Mul_40	1599833759809630	12
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760085562	5
add_175	1599833760298709	9
mul_316	1599833759814441	18
Assign_483	1599833760341111	40
gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul	1599833760161528	2219
Assign_119	1599833760309327	22
mul_1026	1599833760328150	4
add_269	1599833760304051	9
Mul_613	1599833759813502	44
bert/encoder/layer_0/attention/self/query/kernel/adam_v	-1	-1
cls/predictions/Sum_2	1599833759813266	5
Assign_118	1599833760298580	19
bert/encoder/layer_3/output/LayerNorm/gamma/adam_m	-1	-1
clip_by_global_norm/mul_136	1599833760273283	9
gradients/bert/encoder/layer_7/attention/self/transpose_1_grad/transpose	1599833760113007	190
Square_147	1599833760281554	4
bert/encoder/layer_11/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_3/attention/self/key/kernel/adam_v/read	-1	-1
bert/encoder/layer_11/output/dense/bias	-1	-1
global_norm/L2Loss_186	1599833760023568	5
bert/encoder/layer_4/attention/self/value/kernel	-1	-1
bert/encoder/layer_6/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_11/attention/self/query/bias/adam_m/read	-1	-1
mul_789	1599833759815168	45
bert/encoder/layer_9/output/dropout/mul_1	1599833759962970	73
Mul_739	1599833759806331	20
clip_by_global_norm/mul_194	1599833760275003	40
gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1	1599833760242531	229
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760094868	51
truediv_105	1599833760324940	8
gradients/bert/encoder/layer_11/intermediate/dense/mul_3_grad/Mul_1	1599833759985060	195
add_196	1599833760319977	7
add_290	1599833760317684	7
Assign_617	1599833760313421	14
gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/mul	1599833759918731	194
bert/encoder/layer_6/attention/output/dropout/random_uniform/RandomUniform	1599833759800592	46
global_norm/L2Loss_43	1599833760220074	12
bert/encoder/layer_2/attention/self/transpose_3	1599833759860325	189
gradients/bert/encoder/layer_11/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760026007	55
gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul	1599833760113258	642
add_243	1599833760319865	4
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1	1599833759848142	73
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760183279	73
clip_by_global_norm/mul_48	1599833760275365	39
bert/encoder/layer_11/attention/self/query/kernel/adam_v	-1	-1
global_norm/L2Loss_116	1599833760116806	5
gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul	1599833760205828	2222
bert/encoder/layer_7/attention/self/key/kernel/adam_v/read	-1	-1
bert/encoder/layer_3/attention/output/dropout/GreaterEqual	1599833759822606	39
bert/encoder/layer_11/attention/self/query/bias/adam_v/read	-1	-1
Sqrt_68	1599833760314878	4
bert/encoder/layer_8/attention/self/dropout/Cast	1599833759824962	68
bert/encoder/layer_4/attention/self/query/kernel/adam_v	-1	-1
Mul_648	1599833760281811	12
bert/encoder/layer_0/attention/self/dropout/GreaterEqual	1599833759823501	71
cls/seq_relationship/Mean	1599833759990814	5
Mul_19	1599833760284640	9
bert/encoder/layer_3/attention/output/dropout/random_uniform/RandomUniform	1599833759800928	46
Mul_165	1599833760282136	4
Sqrt_166	1599833760311764	4
add_462	1599833760316741	20
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference	1599833759923006	55
bert/encoder/layer_8/attention/output/LayerNorm/gamma/read	-1	-1
add_691	1599833760319416	10
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul	1599833760044133	142
Mul_215	1599833759811184	12
Assign_11	1599833760311096	3
add_258	1599833760294386	22
sub_132	1599833760329751	8
Assign_450	1599833760336104	4
bert/encoder/layer_9/intermediate/dense/bias/adam_v	-1	-1
Mul_32	1599833759808191	18
gradients/AddN_48	1599833760145512	366
bert/encoder/layer_8/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_11/attention/self/query/BiasAdd	1599833759978846	61
add_58	1599833760327480	56
bert/encoder/layer_5/output/dropout/Cast	1599833759824216	36
mul_9	1599833760331051	366
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v	-1	-1
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760182479	74
Mul_742	1599833759806988	18
bert/encoder/layer_7/attention/self/dropout/GreaterEqual	1599833759823649	72
add_136	1599833760283022	17
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul	1599833760198962	141
Mul_901	1599833760289977	4
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760249010	53
Mul_728	1599833759805515	10
Assign_156	1599833760337701	9
Cast/ReadVariableOp	-1	-1
bert/encoder/layer_7/attention/output/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_5/attention/self/key/bias/adam_v	-1	-1
bert/encoder/layer_7/attention/output/dense/kernel	-1	-1
bert/encoder/layer_9/attention/output/add	1599833759954984	73
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/sub	1599833760089957	101
Mul_357	1599833759819372	44
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760018401	68
Assign_355	1599833760295644	23
Assign_505	1599833760301548	18
Mul_830	1599833759806839	18
gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul	1599833760263013	637
Mul_634	1599833759813922	13
gradients/bert/embeddings/LayerNorm/moments/variance_grad/truediv	1599833760271666	51
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
bert/encoder/layer_0/intermediate/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_8/output/dropout/random_uniform	-1	-1
bert/encoder/layer_6/output/LayerNorm/moments/variance	1599833759923062	31
Mul_288	1599833760293540	9
bert/encoder/layer_3/attention/self/add	1599833759872785	129
sub_186	1599833760336493	17
bert/encoder/layer_1/attention/self/query/MatMul	1599833759843052	633
Mul_278	1599833760283085	4
add_15	1599833760284803	4
add_254	1599833760325978	18
sub_68	1599833760332969	4
bert/encoder/layer_2/attention/self/query/kernel/adam_v	-1	-1
bert/encoder/layer_9/attention/self/value/bias	-1	-1
truediv_95	1599833760321577	7
add_42	1599833760284533	17
Square_173	1599833760279246	4
bert/encoder/layer_0/attention/self/query/MatMul	1599833759829645	639
bert/encoder/layer_5/attention/self/transpose	1599833759898719	189
gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul_1	1599833760259067	2468
bert/encoder/layer_6/attention/self/dropout/mul_1	1599833759913493	141
bert/encoder/layer_10/output/LayerNorm/beta/read	-1	-1
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760240101	53
bert/encoder/layer_1/output/dropout/mul_1	1599833759855874	73
Mul_308	1599833760280587	4
bert/encoder/layer_8/attention/self/query/bias	-1	-1
bert/encoder/layer_6/output/dense/bias/adam_m	-1	-1
Assign_71	1599833760313989	20
gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul_1	1599833760170558	2470
bert/encoder/layer_11/output/dense/kernel/adam_v	-1	-1
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/add	1599833759928412	5
Sqrt_176	1599833760313447	9
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760072584	5
bert/encoder/layer_8/attention/output/dropout/mul	1599833759826139	51
Assign_407	1599833760306855	22
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v	-1	-1
add_664	1599833760302722	9
bert/encoder/layer_10/attention/self/transpose	1599833759965639	190
bert/encoder/layer_7/attention/output/LayerNorm/moments/mean	1599833759928290	31
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760218258	27
Mul_834	1599833759807169	11
add_422	1599833760316871	16
gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul_1	1599833760155573	614
bert/encoder/layer_8/output/LayerNorm/batchnorm/add	1599833759949845	5
gradients/bert/encoder/layer_10/attention/self/transpose_grad/transpose	1599833760046400	190
bert/encoder/layer_0/output/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_11/output/LayerNorm/beta/adam_m/read	-1	-1
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760063230	28
add_340	1599833760296985	9
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760050441	5
bert/encoder/layer_4/intermediate/dense/kernel/read	-1	-1
gradients/AddN_90	1599833760272511	516
global_norm/L2Loss_163	1599833760050033	5
add_36	1599833760300307	17
bert/encoder/layer_1/attention/self/MatMul	1599833759845714	157
truediv_147	1599833760325055	4
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m	-1	-1
mul_403	1599833760330305	19
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760085597	51
bert/encoder/layer_10/attention/self/value/bias/adam_m	-1	-1
gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul_1	1599833760037881	2467
mul_193	1599833760326495	8
Assign_32	1599833760310742	3
bert/encoder/layer_4/attention/output/dense/bias/adam_m	-1	-1
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v	-1	-1
gradients/bert/encoder/layer_5/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760158686	54
gradients/bert/encoder/layer_2/intermediate/dense/mul_1_grad/Mul_1	1599833760211400	195
bert/encoder/layer_1/attention/self/Softmax	1599833759846105	331
bert/pooler/strided_slice/stack_2	-1	-1
add_512	1599833760300574	4
sub_81	1599833760330326	8
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760004799	8
gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul_1	1599833760015814	2467
bert/encoder/layer_2/attention/output/dense/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760151828	47
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760084755	69
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
clip_by_global_norm/mul_9	1599833760274479	4
bert/encoder/layer_9/output/dense/bias/adam_m	-1	-1
Mul_787	1599833759807077	45
clip_by_global_norm/mul_45	1599833760273570	4
Square_53	1599833760276987	12
gradients/bert/encoder/layer_2/output/LayerNorm/moments/variance_grad/Tile	1599833760205282	28
add_88	1599833760317053	5
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760240156	28
bert/encoder/layer_2/intermediate/dense/kernel	-1	-1
bert/encoder/layer_7/attention/self/key/kernel/adam_m	-1	-1
bert/encoder/layer_6/output/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_1/intermediate/dense/bias	-1	-1
bert/encoder/layer_2/attention/output/dropout/mul_1	1599833759861213	76
add_74	1599833760319599	8
global_norm/L2Loss_65	1599833760188902	33
Assign_597	1599833760341488	14
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
Mul_174	1599833759811349	47
bert/encoder/layer_9/output/LayerNorm/gamma/read	-1	-1
cls/seq_relationship/Sum	1599833759990725	4
clip_by_global_norm/mul_93	1599833760273436	4
Square_57	1599833760277015	13
Mul_690	1599833759817098	44
bert/encoder/layer_3/attention/output/dense/bias/read	-1	-1
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760240347	27
truediv_159	1599833760323137	4
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760239810	68
global_norm/L2Loss_4	1599833760271620	5
truediv_52	1599833760324208	5
sub_140	1599833760339890	21
mul_252	1599833760326562	9
bert/encoder/layer_10/output/dropout/mul_1	1599833759976361	73
sub_157	1599833760331669	4
Assign_253	1599833760298374	11
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760218066	27
gradients/bert/encoder/layer_7/attention/output/dropout/mul_1_grad/Mul	1599833760108073	51
add_688	1599833760319241	8
add_260	1599833760320290	16
gradients/bert/encoder/layer_1/attention/self/Mul_grad/Mul	1599833760244843	141
Mul_148	1599833759808840	10
add_430	1599833760281583	4
bert/encoder/layer_3/attention/self/query/kernel/adam_v	-1	-1
Square_54	1599833760277228	4
Mul_401	1599833760282403	12
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760240376	52
Mul_212	1599833759809944	11
bert/encoder/layer_8/attention/self/value/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul_1	1599833760244045	616
gradients/bert/pooler/dense/BiasAdd_grad/BiasAddGrad	1599833759990864	8
gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/mul	1599833759985649	194
add_569	1599833760301242	21
add_573	1599833760301526	9
bert/encoder/layer_1/attention/output/dropout/mul	1599833759826509	51
add_150	1599833760292406	21
global_norm/L2Loss_97	1599833760144724	33
bert/encoder/layer_2/attention/self/key/kernel/adam_v/read	-1	-1
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m/read	-1	-1
bert/encoder/layer_10/attention/output/dense/bias/adam_v	-1	-1
bert/encoder/layer_9/attention/self/query/kernel/adam_m	-1	-1
Square_71	1599833760280783	12
Mul_12	1599833759807742	12
Mul_464	1599833759804964	18
add_353	1599833760304699	22
bert/encoder/layer_4/output/dense/kernel/adam_v	-1	-1
Mul_952	1599833759811212	5
ConstantFolding/PolynomialDecay/truediv_recip	-1	-1
Square_134	1599833760281434	4
clip_by_global_norm/mul_138	1599833760275694	12
ReadVariableOp	-1	-1
gradients/bert/encoder/layer_8/output/dropout/mul_grad/Mul	1599833760073003	73
Mul_707	1599833759805722	12
gradients/AddN_11	1599833760027929	69
add_286	1599833760297869	8
add_488	1599833760296036	56
bert/encoder/layer_6/intermediate/dense/add	1599833759918927	280
clip_by_global_norm/mul_34	1599833760273515	39
global_norm/L2Loss_18	1599833760254722	7
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/sub	1599833760222658	99
add_342	1599833760282315	4
Assign_98	1599833760309923	12
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760226907	52
add_301	1599833760326261	20
bert/encoder/layer_8/attention/self/query/MatMul	1599833759936777	635
bert/encoder/layer_6/intermediate/dense/mul_1	1599833759919209	194
add_165	1599833760319738	43
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760195827	53
bert/encoder/layer_3/attention/self/value/bias/adam_v	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760160738	53
gradients/bert/encoder/layer_1/output/dense/BiasAdd_grad/BiasAddGrad	1599833760227867	55
bert/encoder/layer_3/output/dropout/mul_1	1599833759882673	73
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760195263	69
bert/encoder/layer_1/attention/self/query/kernel	-1	-1
Sqrt_169	1599833760311417	12
bert/encoder/layer_11/attention/output/dropout/GreaterEqual	1599833759822042	40
Mul_344	1599833759819710	44
global_norm/L2Loss_54	1599833760204299	5
cls/seq_relationship/one_hot	1599833759821942	9
bert/encoder/layer_0/attention/self/value/kernel/read	-1	-1
gradients/bert/encoder/layer_5/attention/self/transpose_2_grad/transpose	1599833760154396	189
add_98	1599833760316830	12
bert/encoder/layer_5/attention/output/dropout/random_uniform/mul	-1	-1
Sqrt_156	1599833760311203	9
bert/encoder/layer_9/attention/self/query/kernel/adam_v	-1	-1
Mul_46	1599833760284365	12
bert/encoder/layer_0/attention/self/dropout/mul	1599833759827792	99
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760071780	71
add_82	1599833760292307	21
Mul_987	1599833760290647	9
clip_by_global_norm/mul_98	1599833760273676	39
add_479	1599833760304907	9
Mul_347	1599833760292637	44
Assign_446	1599833760311129	3
cls/seq_relationship/output_weights/adam_v	-1	-1
Assign_535	1599833760301396	12
gradients/bert/encoder/layer_11/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760024696	53
bert/encoder/layer_9/output/dense/kernel/adam_v	-1	-1
add_681	1599833760303068	4
Sqrt_66	1599833760314408	4
Mul_662	1599833759815574	12
bert/encoder/layer_1/attention/self/value/kernel/adam_m/read	-1	-1
bert/encoder/layer_11/attention/self/MatMul	1599833759979605	160
truediv_203	1599833760323739	4
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1	1599833759941845	73
gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul	1599833760196620	639
truediv_6	1599833760322936	18
Sqrt_79	1599833760314790	45
gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul_1	1599833760042466	613
Mul_175	1599833760283328	44
bert/encoder/layer_5/attention/self/Reshape_1	-1	-1
gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul	1599833760174506	637
Square_29	1599833760276553	4
Mul_664	1599833759816139	12
bert/encoder/layer_0/attention/self/value/bias/adam_m	-1	-1
Assign_152	1599833760309493	12
Square_137	1599833760280869	11
bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference	1599833759936378	54
bert/encoder/layer_1/attention/output/LayerNorm/moments/mean	1599833759847967	32
clip_by_global_norm/mul_28	1599833760273340	13
gradients/bert/encoder/layer_3/intermediate/dense/mul_2_grad/Mul_1	1599833760188619	281
add_96	1599833760281762	17
Mul_304	1599833760292548	12
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m	-1	-1
gradients/bert/encoder/layer_11/attention/self/transpose_2_grad/transpose	1599833760021716	189
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul	1599833759981976	38
Mul_786	1599833760280944	40
add_686	1599833760291262	9
truediv_173	1599833760323543	5
sub_100	1599833760330206	10
gradients/cls/predictions/transform/dense/Tanh_grad/TanhGrad	1599833760004870	11
global_norm/L2Loss_31	1599833760239504	32
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/sub	1599833760178359	98
bert/encoder/layer_11/attention/self/value/kernel/read	-1	-1
bert/encoder/layer_6/attention/output/dense/BiasAdd	1599833759914706	60
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/add_1	1599833759982201	73
bert/encoder/layer_2/intermediate/dense/bias/adam_m	-1	-1
bert/encoder/layer_1/attention/self/query/bias/adam_m/read	-1	-1
mul_188	1599833760330116	44
bert/encoder/layer_3/intermediate/dense/BiasAdd	1599833759877396	223
sub_98	1599833760334622	60
Mul_610	1599833759813938	11
Mul_335	1599833760276748	4
bert/encoder/layer_7/attention/self/dropout/Cast	1599833759825533	70
Mul_852	1599833759807198	11
gradients/bert/encoder/layer_4/output/dense/BiasAdd_grad/BiasAddGrad	1599833760161472	55
bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2	1599833759883072	53
global_norm/L2Loss_191	1599833760018367	33
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/sub	1599833760271719	53
bert/encoder/layer_4/output/LayerNorm/beta/adam_m	-1	-1
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760116657	32
bert/encoder/layer_8/attention/self/key/BiasAdd	1599833759938743	61
Mul_700	1599833760281222	39
add_50	1599833760300402	4
Assign_56	1599833760311086	3
truediv_22	1599833760324053	22
gradients/bert/encoder/layer_8/output/LayerNorm/moments/mean_grad/truediv	1599833760072529	47
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760249666	73
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760019209	27
gradients/bert/encoder/layer_5/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760154875	54
add_170	1599833760282863	56
add_357	1599833760305528	9
bert/encoder/layer_7/intermediate/dense/kernel/read	-1	-1
add_101	1599833760296682	4
bert/encoder/layer_3/attention/self/query/kernel/adam_m/read	-1	-1
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760004791	7
PolynomialDecay/Minimum/y	-1	-1
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760041031	53
bert/encoder/layer_0/attention/self/key/BiasAdd	1599833759831615	60
Mul_56	1599833759807830	17
add_288	1599833760282949	4
Assign_13	1599833760300079	4
bert/encoder/layer_8/intermediate/dense/mul_2	1599833759946558	194
mul_370	1599833760328809	4
gradients/AddN_6	1599833760012831	364
bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference	1599833759888175	54
global_norm/L2Loss_172	1599833760043083	5
sub_76	1599833760339332	29
Assign_469	1599833760300635	4
bert/encoder/layer_6/attention/output/dense/kernel	-1	-1
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760204815	53
clip_by_global_norm/mul_107	1599833760273452	4
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul	1599833759941806	38
truediv_141	1599833760324677	8
gradients/cls/predictions/transform/dense/MatMul_grad/MatMul_1	1599833760005110	144
Square_140	1599833760280838	4
Assign_478	1599833760300902	40
truediv_179	1599833760323493	4
bert/encoder/layer_2/intermediate/dense/bias/adam_m/read	-1	-1
Mul_183	1599833759809879	45
cls/predictions/transform/LayerNorm/beta/adam_v/read	-1	-1
gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/mul_1	1599833760034618	279
gradients/AddN_49	1599833760150972	73
Mul_256	1599833760283284	9
bert/encoder/layer_2/attention/self/transpose_1	1599833759858736	189
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760019146	47
bert/encoder/layer_4/attention/self/query/bias/read	-1	-1
truediv_97	1599833760321146	5
Mul_114	1599833760280070	4
gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul_1	1599833760221966	611
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Mul	1599833760271328	54
add_44	1599833760318447	13
bert/encoder/layer_5/attention/output/LayerNorm/beta/read	-1	-1
Mul_873	1599833759808022	40
add_56	1599833760300413	56
bert/encoder/layer_8/attention/self/value/kernel/adam_m	-1	-1
Mul_1035	1599833760291855	40
bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference	1599833759914949	55
truediv_185	1599833760323810	4
Sqrt_87	1599833760308667	17
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/add	1599833759955182	5
gradients/bert/encoder/layer_7/intermediate/dense/mul_3_grad/Mul_1	1599833759931510	195
bert/encoder/layer_7/attention/self/Softmax	1599833759926455	323
truediv_201	1599833760323940	4
bert/encoder/layer_7/attention/self/query/bias/adam_m/read	-1	-1
bert/encoder/layer_6/intermediate/dense/mul_2	1599833759919808	195
gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul	1599833760190169	2502
Square_104	1599833760281075	4
bert/encoder/layer_10/attention/self/query/MatMul	1599833759963553	631
Assign_423	1599833760339400	11
gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul_1	1599833760119589	2469
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m	-1	-1
gradients/bert/encoder/layer_8/intermediate/dense/mul_3_grad/Mul	1599833760077835	280
bert/pooler/dense/kernel/read	-1	-1
Sqrt_37	1599833760307719	12
bert/encoder/layer_11/output/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_11/output/dense/BiasAdd	1599833759989677	60
gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1	1599833760264643	229
bert/embeddings/LayerNorm/batchnorm/mul_2	1599833759829235	54
bert/encoder/layer_0/output/LayerNorm/beta	-1	-1
bert/encoder/layer_4/attention/self/query/kernel/read	-1	-1
bert/encoder/layer_10/attention/self/key/BiasAdd	1599833759965515	61
bert/encoder/layer_8/attention/output/dense/bias	-1	-1
bert/encoder/layer_6/attention/output/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul_1	1599833760208051	2468
add_562	1599833760300795	17
gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul_1	1599833760064614	613
truediv_57	1599833760324422	4
bert/encoder/layer_0/output/LayerNorm/beta/read	-1	-1
Mul_652	1599833760280920	4
bert/encoder/layer_3/attention/self/Softmax	1599833759872916	323
gradients/bert/encoder/layer_10/attention/self/Reshape_grad/Reshape	-1	-1
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760018963	53
Sqrt_77	1599833760308576	9
gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul	1599833760265510	638
Square_56	1599833760280582	4
Mul_812	1599833759806274	11
add_652	1599833760328292	56
sub_93	1599833760329962	4
bert/encoder/layer_1/intermediate/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_1/attention/output/dense/BiasAdd	1599833759847756	61
mul_338	1599833760325937	6
truediv_146	1599833760324829	61
Mul_856	1599833760278826	4
Mul_559	1599833759812670	19
gradients/bert/encoder/layer_10/output/LayerNorm/moments/mean_grad/Tile	1599833760028111	27
bert/encoder/layer_11/attention/self/value/kernel/adam_v/read	-1	-1
Mul_674	1599833760281211	4
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760138610	55
Mul_1018	1599833760279833	4
sub_77	1599833760333430	8
sub_4	1599833760331615	4
bert/encoder/layer_4/attention/self/query/bias/adam_v/read	-1	-1
mul_907	1599833759816495	19
Mul_851	1599833760279050	4
bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference	1599833759949755	55
global_norm/L2Loss_180	1599833760028290	5
sub_101	1599833760330227	8
Square_106	1599833760276443	4
bert/encoder/layer_8/attention/self/transpose_3	1599833759940636	189
Mul_530	1599833760282514	39
bert/encoder/layer_5/output/dense/kernel/adam_v/read	-1	-1
add_187	1599833760282926	4
bert/encoder/layer_10/output/LayerNorm/batchnorm/add_1	1599833759976869	73
clip_by_global_norm/mul_47	1599833760273992	4
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759901661	4
Assign_303	1599833760340327	13
bert/encoder/layer_1/output/dropout/mul	1599833759826086	51
add_644	1599833760302242	9
bert/encoder/layer_2/intermediate/dense/kernel/adam_v	-1	-1
bert/encoder/layer_3/attention/output/dense/kernel	-1	-1
gradients/bert/embeddings/LayerNorm/moments/mean_grad/truediv	1599833760271572	47
Assign_19	1599833760300396	4
bert/pooler/Squeeze	-1	-1
truediv_61	1599833760321152	4
truediv_151	1599833760323105	5
bert/encoder/layer_7/attention/self/transpose_3	1599833759927256	189
add_134	1599833760298024	9
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
global_norm/L2Loss_73	1599833760178343	14
clip_by_global_norm/Const_1	-1	-1
bert/encoder/layer_5/attention/self/key/MatMul	1599833759897267	630
global_norm/L2Loss_19	1599833760249064	5
Square_1	1599833760280042	4
bert/encoder/layer_9/output/dropout/GreaterEqual	1599833759822687	38
Assign_585	1599833760336811	9
Assign_43	1599833760300359	4
Assign_510	1599833760336405	4
Assign_416	1599833760316273	12
gradients/bert/encoder/layer_9/attention/self/Reshape_3_grad/Reshape	-1	-1
add_325	1599833760283096	56
cls/predictions/transform/LayerNorm/gamma/adam_v/read	-1	-1
bert/encoder/layer_1/output/dropout/Cast	1599833759824141	37
gradients/AddN_24	1599833760071642	118
bert/encoder/layer_3/attention/self/query/bias/adam_m	-1	-1
gradients/bert/encoder/layer_10/intermediate/dense/mul_3_grad/Mul_1	1599833759971674	196
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760226968	31
Assign_406	1599833760295875	22
global_norm/L2Loss_81	1599833760166788	32
gradients/AddN_39	1599833760116446	69
Assign_587	1599833760313269	11
bert/encoder/layer_11/attention/self/dropout/random_uniform	-1	-1
add_465	1599833760304854	9
add_570	1599833760318846	12
Assign_281	1599833760307792	11
Mul_398	1599833759820351	17
gradients/bert/encoder/layer_10/attention/self/dropout/mul_1_grad/Mul	1599833760043699	99
bert/encoder/layer_10/attention/self/key/kernel	-1	-1
cls/predictions/transform/dense/MatMul	1599833759990352	151
add_155	1599833760296918	5
mul_91	1599833760331568	40
truediv_103	1599833760321833	9
bert/encoder/layer_2/attention/self/transpose_2	1599833759858927	189
bert/encoder/layer_8/attention/self/value/bias/adam_m	-1	-1
add_497	1599833760329190	62
Assign_476	1599833760311190	3
gradients/AddN_7	1599833760018293	72
Mul_738	1599833760280822	4
bert/encoder/layer_3/attention/self/key/bias	-1	-1
Mul_83	1599833759810460	4
Mul_51	1599833759811005	4
Square_163	1599833760279274	4
edge_690_IteratorGetNext@@MemcpyHtoD	1599833759792487	30
add_366	1599833760282326	17
Mul_776	1599833759806013	45
mul_305	1599833759815124	12
sub_201	1599833760332532	4
Assign_493	1599833760301841	12
global_norm/L2Loss_102	1599833760137932	5
bert/encoder/layer_0/attention/output/add	1599833759834485	73
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760094430	55
gradients/bert/encoder/layer_0/attention/self/Reshape_1_grad/Reshape	-1	-1
truediv	-1	-1
bert/encoder/layer_0/output/LayerNorm/gamma/adam_m/read	-1	-1
Assign_568	1599833760302218	13
Sqrt_53	1599833760308452	12
bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2	1599833759949972	54
add_71	1599833760318581	4
Mul_545	1599833759813233	4
Assign_223	1599833760304023	12
bert/encoder/layer_1/attention/self/value/kernel	-1	-1
add_338	1599833760317585	8
Mul_468	1599833759805051	11
Mul_985	1599833760279474	4
clip_by_global_norm/mul_76	1599833760275714	13
bert/encoder/layer_10/attention/output/dense/BiasAdd	1599833759968242	60
bert/encoder/layer_7/output/LayerNorm/beta/read	-1	-1
gradients/bert/encoder/layer_0/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760267951	55
global_norm/L2Loss_178	1599833760033544	5
mul_494	1599833760326285	8
add_675	1599833760319223	8
add_151	1599833760303582	21
gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Sum	1599833760271017	74
gradients/AddN_38	1599833760115869	119
bert/encoder/layer_11/output/dropout/random_uniform	-1	-1
Mul_340	1599833760277048	4
bert/encoder/layer_2/attention/output/dense/kernel/adam_m	-1	-1
bert/encoder/layer_3/output/dense/kernel/adam_m/read	-1	-1
Mul_615	1599833759813966	45
gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul	1599833760135367	638
Assign_294	1599833760334838	10
bert/encoder/layer_2/attention/self/dropout/mul_1	1599833759859936	141
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760004644	7
bert/encoder/layer_4/attention/output/dropout/Cast	1599833759824784	36
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760041115	32
Mul_556	1599833759812729	10
Mul_716	1599833760281026	4
bert/encoder/layer_10/attention/self/query/kernel	-1	-1
Square_180	1599833760279811	4
Assign_385	1599833760305037	13
add_534	1599833760318623	4
bert/encoder/layer_10/output/dropout/random_uniform/RandomUniform	1599833759800207	46
bert/encoder/layer_10/intermediate/dense/mul_2	1599833759973345	194
bert/encoder/layer_10/attention/self/key/bias/adam_m	-1	-1
sub_204	1599833760332420	9
Mul_54	1599833759811230	13
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul	1599833759915050	39
bert/encoder/layer_6/attention/self/query/kernel/adam_v/read	-1	-1
Assign_86	1599833760307561	11
mul_85	1599833760327450	4
gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul	1599833760136679	636
gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/mul	1599833759891958	194
bert/encoder/layer_11/attention/self/key/bias/adam_m/read	-1	-1
bert/encoder/layer_4/attention/self/MatMul_1	1599833759886875	232
bert/encoder/layer_11/attention/self/query/bias/read	-1	-1
bert/encoder/layer_2/attention/self/Mul	1599833759859279	99
mul_134	1599833760332680	16
Mul_814	1599833759806502	11
Mul_376	1599833759819777	17
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760027764	68
add_119	1599833760325787	60
bert/encoder/layer_10/attention/self/value/bias	-1	-1
add_191	1599833760303749	17
Mul_688	1599833759816941	45
mul_591	1599833760328912	4
Square_13	1599833760278631	4
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/add	1599833759834682	5
sub_39	1599833760330620	7
gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/mul_1	1599833760101047	280
Assign_421	1599833760304880	12
gradients/bert/encoder/layer_8/attention/output/dropout/mul_grad/Mul	1599833760085981	72
global_norm/L2Loss_40	1599833760226417	5
bert/encoder/layer_11/attention/self/value/BiasAdd	1599833759978970	61
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1	1599833759888317	73
Sqrt_121	1599833760316376	15
gradients/bert/encoder/layer_0/intermediate/dense/mul_1_grad/Mul_1	1599833760255602	194
Assign_177	1599833760341675	13
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760160446	69
bert/encoder/layer_6/attention/output/dense/bias/adam_m/read	-1	-1
Mul_576	1599833759814352	12
Assign_267	1599833760340271	13
gradients/bert/encoder/layer_3/output/dropout/mul_grad/Mul	1599833760183503	73
Square_5	1599833760278525	12
Assign_38	1599833760310936	4
add_356	1599833760294744	9
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul_1	1599833759913392	99
mul_1047	1599833759815941	45
Sqrt_81	1599833760314555	44
bert/encoder/layer_9/attention/self/dropout/random_uniform/mul	-1	-1
Square_132	1599833760281032	4
gradients/bert/encoder/layer_7/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760114522	55
Mul_1045	1599833759805661	45
global_norm/L2Loss_106	1599833760134168	5
gradients/bert/encoder/layer_0/attention/self/transpose_2_grad/transpose	1599833760264974	189
add_140	1599833760283211	10
bert/encoder/layer_7/attention/self/Reshape_3	-1	-1
add_159	1599833760317799	17
add_538	1599833760284897	4
global_norm/L2Loss_130	1599833760099971	5
bert/encoder/layer_11/attention/self/Softmax	1599833759979996	321
bert/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_1	1599833759824865	57
Mul_496	1599833760277286	12
bert/encoder/layer_2/attention/output/dense/MatMul	1599833759860516	633
Square_129	1599833760281263	40
truediv_15	1599833760322980	5
bert/encoder/layer_7/attention/self/query/bias/read	-1	-1
bert/encoder/layer_4/attention/output/dense/bias	-1	-1
bert/encoder/layer_7/attention/self/MatMul_1	1599833759927024	231
truediv_50	1599833760321853	61
add_293	1599833760319997	16
add_213	1599833760317229	8
bert/encoder/layer_9/attention/self/MatMul	1599833759952820	159
bert/encoder/layer_5/output/LayerNorm/batchnorm/add_1	1599833759909948	72
sub_38	1599833760334353	16
bert/encoder/layer_3/output/dense/bias/adam_m	-1	-1
gradients/bert/encoder/layer_10/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760046786	55
gradients/cls/predictions/MatMul_grad/MatMul_1	1599833760001108	3405
Mul_498	1599833760282965	12
Mul_405	1599833760280462	4
bert/encoder/layer_11/output/dense/MatMul	1599833759987202	2473
Assign_259	1599833760297425	12
add_215	1599833760297762	9
mul_935	1599833760327980	4
global_norm/L2Loss_157	1599833760063007	5
group_deps_1	-1	-1
add_350	1599833760298165	9
Mul_984	1599833759818579	10
gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul	1599833760244986	244
add_302	1599833760282960	4
mul_704	1599833760333568	45
Mul_194	1599833759811897	10
bert/pooler/strided_slice/stack	-1	-1
mul_365	1599833760328717	4
bert/encoder/layer_6/attention/self/transpose	1599833759912107	189
Assign_188	1599833760308552	11
sub_70	1599833760334465	21
add_265	1599833760282727	4
add_466	1599833760320192	7
Assign_202	1599833760304229	12
gradients/AddN_51	1599833760152152	95
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/sub	1599833760045667	99
gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul_1	1599833760104315	2468
bert/encoder/layer_2/output/dense/bias/adam_m/read	-1	-1
Mul_764	1599833759805985	11
bert/encoder/layer_4/output/dense/bias/adam_m	-1	-1
bert/encoder/layer_4/attention/self/value/BiasAdd	1599833759885281	59
clip_by_global_norm/mul_71	1599833760275487	4
Assign_560	1599833760312471	12
bert/encoder/layer_10/output/LayerNorm/moments/variance	1599833759976599	31
gradients/bert/encoder/layer_3/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760190059	109
sub_193	1599833760332581	9
Mul_222	1599833760277485	4
global_norm/L2Loss_74	1599833760178336	5
gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul_1	1599833760048850	613
sub_52	1599833760332786	4
gradients/bert/encoder/layer_0/attention/self/transpose_1_grad/transpose	1599833760267758	189
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760062883	68
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760195607	53
mul_972	1599833760328108	4
sub_5	1599833760331420	4
Assign_68	1599833760313892	11
gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul_1	1599833760153079	613
bert/encoder/layer_7/output/dense/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/sub	1599833760244743	99
mul_580	1599833760325684	8
sub_30	1599833760330089	10
Assign_370	1599833760296196	20
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760028174	65
bert/encoder/layer_11/attention/self/query/kernel/adam_m	-1	-1
Mul_1007	1599833760279479	4
bert/encoder/layer_1/output/LayerNorm/beta/read	-1	-1
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
bert/encoder/layer_1/attention/self/value/kernel/adam_m	-1	-1
add_662	1599833760319202	8
sub_33	1599833760331042	5
Assign_27	1599833760341013	14
gradients/bert/encoder/layer_4/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760176960	55
bert/encoder/layer_5/intermediate/dense/bias	-1	-1
gradients/bert/encoder/layer_2/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760212248	109
bert/encoder/layer_10/attention/self/value/kernel	-1	-1
bert/encoder/layer_7/attention/output/LayerNorm/gamma/read	-1	-1
Square_182	1599833760279468	4
Mul_809	1599833760284938	12
Mul_136	1599833760276536	4
bert/encoder/layer_0/intermediate/dense/mul/x	-1	-1
gradients/cls/predictions/LogSoftmax_grad/mul	1599833759996477	309
bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference	1599833759834593	55
Square_111	1599833760281508	39
Mul_411	1599833759819467	18
cls/seq_relationship/Neg	1599833759990750	4
Assign_59	1599833760311118	3
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul_1	1599833759833036	99
Assign_47	1599833760311036	41
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760040538	73
gradients/bert/encoder/layer_10/attention/self/transpose_1_grad/transpose	1599833760046592	189
bert/encoder/layer_10/attention/self/dropout/Cast	1599833759825314	68
bert/encoder/layer_4/attention/output/LayerNorm/beta/read	-1	-1
Mul_762	1599833760293663	9
bert/encoder/layer_7/output/dropout/mul_1	1599833759936196	73
bert/encoder/layer_6/attention/self/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_2/attention/self/key/BiasAdd	1599833759858420	61
bert/encoder/layer_4/attention/self/key/MatMul	1599833759883889	630
Mul_592	1599833759814846	12
gradients/bert/embeddings/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760271627	5
bert/encoder/layer_9/intermediate/dense/Pow	1599833759957914	366
bert/encoder/layer_2/attention/self/query/kernel/read	-1	-1
Mul_1034	1599833759816266	44
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760262219	53
gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/Pow	1599833759904949	196
bert/encoder/layer_7/intermediate/dense/bias	-1	-1
bert/encoder/layer_7/attention/self/key/bias/adam_m	-1	-1
bert/encoder/layer_10/attention/self/query/kernel/adam_m/read	-1	-1
Mul_377	1599833760276767	11
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760094121	74
mul_746	1599833759817199	18
Mul_799	1599833760295638	4
gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul_1	1599833760252248	2472
add_448	1599833760295851	9
bert/encoder/layer_1/output/LayerNorm/moments/variance	1599833759856112	31
gradients/bert/encoder/layer_11/attention/self/transpose_grad/transpose	1599833760024310	189
Mul_667	1599833759816169	18
bert/encoder/layer_3/attention/self/value/kernel	-1	-1
bert/encoder/layer_3/output/dense/kernel/adam_v	-1	-1
truediv_20	1599833760323093	4
Square_35	1599833760277469	4
bert/encoder/layer_2/attention/self/key/MatMul	1599833759857094	631
bert/encoder/layer_5/intermediate/dense/Tanh	1599833759906017	209
Mul_571	1599833760281304	12
Sqrt_26	1599833760314016	9
sub_21	1599833760332602	8
Mul_119	1599833760280127	12
bert/encoder/layer_4/output/dropout/GreaterEqual	1599833759822527	39
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760271383	52
bert/encoder/layer_8/output/dense/kernel	-1	-1
Sqrt_86	1599833760308192	9
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/add	1599833759915038	5
truediv_48	1599833760324148	58
Mul_518	1599833759808096	44
bert/encoder/layer_5/output/dense/bias	-1	-1
Assign_418	1599833760306297	20
mul_618	1599833760333490	44
sub_89	1599833760330755	8
global_norm/L2Loss_200	1599833760005256	5
clip_by_global_norm/mul_104	1599833760275644	12
Assign_485	1599833760311346	40
clip_by_global_norm/mul_70	1599833760273612	13
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760226562	71
bert/encoder/layer_7/attention/self/MatMul	1599833759926062	160
bert/encoder/layer_1/attention/self/key/bias/adam_m/read	-1	-1
bert/encoder/layer_4/intermediate/dense/kernel/adam_m/read	-1	-1
mul_505	1599833760325656	4
mul_300	1599833760326429	4
Assign_480	1599833760336416	4
global_norm/L2Loss_47	1599833760217413	32
Mul_1101	1599833759806657	12
bert/encoder/layer_4/attention/self/dropout/mul_1	1599833759886732	142
gradients/cls/predictions/transform/LayerNorm/moments/mean_grad/Tile	1599833760004697	9
bert/encoder/layer_9/output/LayerNorm/beta/adam_v	-1	-1
add_440	1599833760295039	59
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760129875	53
bert/encoder/layer_4/attention/output/dense/bias/read	-1	-1
Mul_879	1599833759809973	12
bert/encoder/layer_1/output/dense/kernel/adam_m/read	-1	-1
Mul_658	1599833759815542	18
sub_176	1599833760336707	56
_SOURCE	-1	-1
Mul_323	1599833759818669	18
Assign_333	1599833760342094	40
gradients/bert/encoder/layer_11/attention/output/dropout/mul_grad/Mul	1599833760019621	72
global_norm/L2Loss_46	1599833760218245	5
Mul_932	1599833760279426	4
Mul_480	1599833760277266	4
bert/encoder/layer_11/attention/self/value/kernel/adam_v	-1	-1
Sqrt_174	1599833760312310	9
Mul_1038	1599833759805573	11
bert/encoder/layer_8/attention/self/value/bias/adam_v	-1	-1
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760027562	72
Sqrt_8	1599833760310832	4
bert/encoder/layer_0/intermediate/dense/Tanh	1599833759839047	208
mul_75	1599833760327444	4
clip_by_global_norm/mul_118	1599833760275949	12
bert/encoder/layer_0/attention/self/key/kernel/read	-1	-1
Mul_565	1599833759814986	11
add_513	1599833760318592	4
Mul_659	1599833760295310	17
bert/encoder/layer_9/output/LayerNorm/gamma	-1	-1
Mul_1084	1599833760291908	4
bert/encoder/layer_3/output/dense/BiasAdd	1599833759882609	62
bert/encoder/layer_9/attention/output/dropout/GreaterEqual	1599833759822887	38
bert/encoder/layer_3/output/dense/kernel	-1	-1
Mul_927	1599833759811547	17
gradients/bert/encoder/layer_1/attention/self/Reshape_2_grad/Reshape	-1	-1
bert/encoder/layer_10/output/LayerNorm/beta	-1	-1
bert/encoder/layer_7/attention/output/add	1599833759928215	73
Mul_944	1599833760290546	10
bert/encoder/layer_10/output/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_8/attention/self/transpose	1599833759938868	189
clip_by_global_norm/mul_153	1599833760274915	4
gradients/AddN_81	1599833760249104	69
Assign_148	1599833760298257	40
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul	1599833760221118	141
bert/encoder/layer_7/intermediate/dense/mul_2	1599833759933182	194
bert/encoder/layer_8/attention/self/key/bias/adam_v/read	-1	-1
bert/encoder/layer_6/attention/output/dense/bias/adam_v	-1	-1
global_norm/L2Loss_111	1599833760128950	32
Assign_295	1599833760298103	20
bert/encoder/layer_6/attention/self/query/bias/read	-1	-1
sub_9	1599833760331487	4
Mul_513	1599833759807315	11
add_489	1599833760316786	42
gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul	1599833760268008	638
sub_164	1599833760331888	4
Sqrt_31	1599833760309790	45
gradients/bert/encoder/layer_11/attention/self/transpose_1_grad/transpose	1599833760024501	189
gradients/AddN_60	1599833760182719	68
Sqrt_25	1599833760313819	16
gradients/AddN_76	1599833760233969	365
Assign_394	1599833760295822	12
Mul_1012	1599833760279658	12
gradients/bert/encoder/layer_1/attention/output/dropout/mul_grad/Mul	1599833760240762	73
edge_13_bert/embeddings/assert_less_equal/All@@MemcpyDtoH	1599833759813235	6
add_314	1599833760317722	16
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760160243	73
gradients/bert/encoder/layer_10/intermediate/dense/mul_1_grad/Mul_1	1599833760034423	194
bert/encoder/layer_1/output/LayerNorm/gamma/adam_m	-1	-1
truediv_37	1599833760322278	5
bert/encoder/layer_6/intermediate/dense/kernel/adam_m	-1	-1
Sqrt_110	1599833760315033	8
bert/encoder/layer_11/attention/self/key/kernel/adam_m	-1	-1
Mul_39	1599833760278626	4
bert/encoder/layer_9/attention/self/value/kernel/adam_v/read	-1	-1
global_norm/L2Loss_16	1599833760261537	8
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760107307	32
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v	-1	-1
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum	1599833760004708	10
bert/encoder/layer_7/output/dropout/random_uniform/RandomUniform	1599833759801119	46
gradients/bert/encoder/layer_3/intermediate/dense/Tanh_grad/TanhGrad	1599833760188936	275
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760173831	65
Square_194	1599833760279702	4
bert/encoder/layer_1/output/dropout/random_uniform/mul	-1	-1
add_670	1599833760302013	17
bert/encoder/layer_10/output/dense/kernel/adam_v/read	-1	-1
Assign_411	1599833760341862	14
bert/encoder/layer_2/attention/self/key/bias/adam_v	-1	-1
Assign_105	1599833760335191	4
global_norm/L2Loss_77	1599833760173548	5
Square_96	1599833760276421	4
add_612	1599833760291399	4
truediv_124	1599833760321055	17
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_1/attention/self/query/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760063415	5
truediv_87	1599833760321531	8
gradients/bert/encoder/layer_8/output/dense/BiasAdd_grad/BiasAddGrad	1599833760073080	55
Square_77	1599833760277054	4
sub_178	1599833760336343	55
Mul_312	1599833759817813	18
global_norm/L2Loss_121	1599833760112080	15
Mul_587	1599833759814165	11
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760227070	55
mul_2	1599833759828471	4
truediv_51	1599833760322074	8
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760138248	51
bert/encoder/layer_2/attention/self/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_9/attention/self/transpose_2_grad/transpose	1599833760065942	189
clip_by_global_norm/mul_174	1599833760274869	4
add_167	1599833760281962	5
bert/encoder/layer_10/intermediate/dense/add	1599833759972461	280
bert/encoder/layer_9/attention/self/value/MatMul	1599833759951425	634
Assign_127	1599833760298945	12
bert/encoder/layer_11/output/LayerNorm/batchnorm/add_1	1599833759990247	72
Assign_185	1599833760307978	11
add_592	1599833760290569	8
Mul_577	1599833760276437	4
bert/encoder/layer_1/attention/output/dropout/random_uniform/RandomUniform	1599833759800784	46
Mul_915	1599833760278998	11
truediv_7	1599833760323003	4
bert/encoder/layer_8/attention/self/key/kernel/adam_m	-1	-1
Square_142	1599833760281195	4
Sqrt_163	1599833760311879	8
gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1	1599833760198297	228
add_650	1599833760303000	56
bert/encoder/layer_4/intermediate/dense/mul	1599833759891762	194
Mul_544	1599833760276704	4
gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul	1599833760117369	2218
Assign_26	1599833760310838	3
Sqrt_92	1599833760307276	4
Assign_8	1599833760310963	8
clip_by_global_norm/mul_65	1599833760273293	4
Assign_111	1599833760340209	14
bert/encoder/layer_10/attention/output/dropout/Cast	1599833759824747	36
Mul_797	1599833760281548	4
add_272	1599833760304358	60
Assign_88	1599833760296753	11
Sqrt_114	1599833760316296	7
gradients/bert/encoder/layer_4/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760174450	55
gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul	1599833760199163	642
global_step/cond/Identity	-1	-1
mul_575	1599833760333614	12
bert/embeddings/assert_less_equal/All	-1	-1
Sqrt_141	1599833760315550	9
Mul_793	1599833759806245	11
bert/encoder/layer_10/intermediate/dense/kernel	-1	-1
add_224	1599833760282056	56
add_542	1599833760300943	56
bert/encoder/layer_2/output/dropout/GreaterEqual	1599833759822405	39
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760094569	33
add_298	1599833760282759	17
bert/encoder/layer_8/attention/output/LayerNorm/moments/mean	1599833759941671	32
bert/encoder/layer_0/attention/output/dense/bias/adam_m	-1	-1
bert/encoder/layer_1/attention/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/add_1	1599833759928650	73
truediv_106	1599833760325134	17
bert/embeddings/Slice	-1	-1
global_norm/L2Loss_115	1599833760116406	5
mul_844	1599833760331650	12
global_norm/L2Loss_167	1599833760049615	16
truediv_85	1599833760321927	4
clip_by_global_norm/mul_192	1599833760275225	39
bert/encoder/layer_10/attention/self/key/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_3/output/LayerNorm/moments/variance_grad/Tile	1599833760183090	27
bert/encoder/layer_7/attention/self/query/bias/adam_m	-1	-1
gradients/bert/encoder/layer_11/intermediate/dense/mul_2_grad/Mul_1	1599833760011762	279
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760173988	52
bert/encoder/layer_2/intermediate/dense/Pow	1599833759864224	367
bert/encoder/layer_10/output/LayerNorm/beta/adam_m/read	-1	-1
Assign_169	1599833760304280	12
sub_162	1599833760336163	55
add_207	1599833760328641	18
Assign_154	1599833760303744	4
truediv_26	1599833760324010	20
bert/encoder/layer_11/attention/self/value/kernel/adam_m/read	-1	-1
Mul_507	1599833760276682	4
mul_1111	1599833760328281	4
bert/encoder/layer_7/attention/self/dropout/random_uniform/mul	-1	-1
Assign_70	1599833760303498	20
sub_200	1599833760336833	21
bert/encoder/layer_8/attention/self/key/kernel/adam_v	-1	-1
mul_747	1599833760333254	16
bert/encoder/layer_10/intermediate/dense/bias/adam_m	-1	-1
bert/encoder/layer_4/attention/self/key/bias/adam_m	-1	-1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760107705	5
gradients/bert/pooler/Squeeze_grad/Reshape	-1	-1
gradients/bert/encoder/layer_10/intermediate/dense/mul_2_grad/Mul_1	1599833760033830	280
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760173345	74
bert/encoder/layer_5/attention/output/dropout/mul	1599833759825820	52
bert/encoder/layer_4/attention/output/dense/kernel/adam_v/read	-1	-1
sub_105	1599833760333451	7
gradients/bert/encoder/layer_10/output/dense/BiasAdd_grad/BiasAddGrad	1599833760028796	55
bert/encoder/layer_4/output/LayerNorm/batchnorm/add_1	1599833759896557	73
Assign_566	1599833760313176	10
bert/encoder/layer_6/attention/output/dropout/Cast	1599833759824292	36
bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference	1599833759941704	55
bert/encoder/layer_3/attention/self/dropout/random_uniform	-1	-1
Sqrt_84	1599833760309091	9
sub_24	1599833760337152	21
clip_by_global_norm/mul_203	1599833760275113	4
bert/encoder/layer_11/intermediate/dense/bias/read	-1	-1
bert/encoder/layer_9/attention/self/value/kernel/adam_m/read	-1	-1
add_67	1599833760300512	4
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub	1599833759875070	54
bert/encoder/layer_6/attention/self/key/bias/adam_v	-1	-1
mul_466	1599833759814313	12
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760151761	66
bert/encoder/layer_3/output/dense/kernel/read	-1	-1
mul_531	1599833759814084	46
gradients/bert/encoder/Reshape_13_grad/Reshape	-1	-1
bert/encoder/layer_8/attention/self/value/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul_1	1599833760108900	614
Assign_235	1599833760304074	12
add_307	1599833760317349	15
bert/encoder/layer_6/intermediate/dense/Tanh	1599833759919404	205
Sqrt_181	1599833760312116	16
Assign_468	1599833760336139	4
bert/encoder/layer_0/intermediate/dense/kernel	-1	-1
Mul_486	1599833759819990	19
bert/encoder/layer_8/attention/self/query/kernel/adam_m/read	-1	-1
bert/encoder/layer_7/output/LayerNorm/moments/variance	1599833759936434	31
Assign_125	1599833760309388	21
Assign_266	1599833760309164	13
mul_993	1599833759816576	12
global_norm/L2Loss_2	1599833760272487	14
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760004607	12
Assign_133	1599833760296924	4
Square_82	1599833760280560	4
gradients/cls/predictions/Sum_grad/Reshape/shape	-1	-1
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760151883	5
sub_190	1599833760332202	10
add_537	1599833760318729	4
Mul_538	1599833759808142	4
gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul	1599833760079665	2500
Sqrt_71	1599833760315127	17
gradients/bert/encoder/layer_2/output/dropout/mul_1_grad/Mul	1599833760205643	51
Assign_201	1599833760337480	4
truediv_172	1599833760323468	17
Sqrt_90	1599833760308716	14
bert/encoder/layer_3/attention/self/value/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/mul_1	1599833760012550	279
gradients/cls/predictions/transform/dense/mul_3_grad/Mul_1	1599833760004828	7
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760152078	73
global_norm/L2Loss_162	1599833760055688	5
mul_36	1599833759813205	13
add_620	1599833760301735	9
global_norm/L2Loss_29	1599833760239935	5
cls/seq_relationship/output_bias/adam_v	-1	-1
Mul_723	1599833760294820	17
bert/encoder/layer_11/output/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_0/output/LayerNorm/batchnorm/add	1599833759842740	5
sub_36	1599833760330939	4
bert/encoder/layer_7/attention/output/dense/bias/adam_m	-1	-1
Sqrt_143	1599833760306931	44
gradients/bert/encoder/layer_8/attention/self/transpose_1_grad/transpose	1599833760090882	190
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760019238	51
bert/encoder/layer_11/attention/output/dense/bias/adam_m	-1	-1
bert/encoder/layer_6/attention/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_3/intermediate/dense/bias	-1	-1
mul_849	1599833760327600	4
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760041360	53
cls/predictions/one_hot/depth	-1	-1
Assign_300	1599833760334400	10
gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/mul_1	1599833760189408	280
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2	1599833759861617	53
bert/encoder/layer_8/attention/self/query/kernel/adam_v	-1	-1
add_287	1599833760317503	10
add_576	1599833760300813	17
clip_by_global_norm/mul_43	1599833760273987	4
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760226709	51
bert/encoder/layer_9/intermediate/dense/bias/adam_v/read	-1	-1
Mul_427	1599833759820173	11
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_10/attention/output/dense/bias	-1	-1
bert/embeddings/dropout/random_uniform/RandomUniform	1599833759801312	47
Square	1599833760277617	365
bert/encoder/layer_11/intermediate/dense/bias/adam_m	-1	-1
Assign_197	1599833760307492	41
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760262458	5
bert/encoder/layer_8/attention/self/value/kernel/adam_m/read	-1	-1
mul_231	1599833760330835	16
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760217956	55
bert/encoder/layer_1/attention/self/value/kernel/read	-1	-1
gradients/bert/encoder/layer_2/attention/self/Reshape_grad/Reshape	-1	-1
Sqrt_42	1599833760309686	9
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760006219	5
Mul_749	1599833760281331	4
Mul_866	1599833759806900	12
bert/encoder/layer_7/attention/self/key/bias	-1	-1
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760218339	53
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760094486	53
Assign_305	1599833760308437	13
bert/encoder/layer_9/attention/self/transpose_1	1599833759952438	189
gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul_1	1599833760060024	2468
gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul	1599833760088623	638
gradients/bert/encoder/layer_0/attention/output/dropout/mul_1_grad/Mul	1599833760262826	51
gradients/AddN_57	1599833760173587	69
gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul	1599833760245918	643
truediv_112	1599833760325296	58
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760094196	69
gradients/bert/encoder/layer_3/output/dense/BiasAdd_grad/BiasAddGrad	1599833760183579	55
Mul_509	1599833760282298	4
Assign_84	1599833760334268	4
global_norm/L2Loss_37	1599833760226400	15
truediv_45	1599833760321355	4
Square_6	1599833760278620	4
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/Rsqrt	1599833759915045	4
bert/embeddings/one_hot	1599833759797863	69
Assign_45	1599833760341039	41
bert/encoder/layer_6/output/dense/MatMul	1599833759920286	2474
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760050285	32
bert/encoder/layer_9/attention/output/dropout/mul	1599833759827039	52
Sqrt_1	1599833760313682	8
Square_145	1599833760280985	39
clip_by_global_norm/mul_4	1599833760274606	4
gradients/bert/encoder/layer_10/output/LayerNorm/moments/variance_grad/Tile	1599833760028303	28
bert/embeddings/token_type_embeddings/adam_v	-1	-1
cls/predictions/transform/dense/kernel/adam_m	-1	-1
Mul_416	1599833760281064	4
Mul_668	1599833760276284	12
bert/encoder/layer_6/output/dropout/random_uniform/RandomUniform	1599833759800736	46
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760261853	74
Mul_794	1599833760294966	12
gradients/bert/encoder/layer_4/intermediate/dense/Tanh_grad/TanhGrad	1599833760166822	275
bert/encoder/layer_9/output/LayerNorm/moments/variance	1599833759963208	31
bert/encoder/layer_8/output/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_7/intermediate/dense/bias/adam_v	-1	-1
bert/encoder/layer_9/attention/output/LayerNorm/gamma/read	-1	-1
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760006414	73
bert/encoder/layer_9/output/dense/bias/adam_v/read	-1	-1
Square_195	1599833760279861	4
gradients/AddN_87	1599833760270606	118
bert/encoder/layer_5/attention/self/key/bias/read	-1	-1
bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2	1599833759909837	53
Mul_116	1599833760292084	9
gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/Pow	1599833759891564	196
add_557	1599833760318841	4
bert/encoder/layer_5/intermediate/dense/mul_1	1599833759905820	195
gradients/bert/encoder/layer_10/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760041769	55
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760218394	51
gradients/AddN	1599833760004653	13
gradients/bert/encoder/layer_11/attention/self/dropout/mul_1_grad/Mul	1599833760021616	99
add_217	1599833760292682	56
bert/encoder/layer_1/output/dense/MatMul	1599833759853334	2476
mul_1031	1599833760328245	4
bert/encoder/layer_9/attention/output/LayerNorm/beta	-1	-1
bert/encoder/layer_1/attention/self/key/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_4/attention/output/dropout/mul_grad/Mul	1599833760174373	72
bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1	1599833759949897	73
bert/encoder/layer_11/attention/self/value/kernel	-1	-1
gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul_1	1599833760020399	611
bert/encoder/layer_3/intermediate/dense/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul_1	1599833760113906	612
Square_186	1599833760279484	4
add_643	1599833760291094	9
Mul_228	1599833759811198	13
bert/encoder/layer_7/output/dense/kernel/adam_m	-1	-1
Mul_457	1599833759820248	4
mul_552/x	-1	-1
global_norm/L2Loss_50	1599833760210521	5
clip_by_global_norm/mul_108	1599833760273592	13
mul_671	1599833759813604	13
bert/encoder/layer_11/attention/self/key/kernel/read	-1	-1
Mul_785	1599833759806747	44
clip_by_global_norm/mul_193	1599833760275288	4
bert/encoder/layer_2/attention/output/LayerNorm/gamma/read	-1	-1
Mul_525	1599833760281986	4
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760262602	51
bert/encoder/layer_9/attention/self/value/kernel/read	-1	-1
cls/predictions/transform/dense/kernel	-1	-1
Mul_430	1599833759820382	46
add_494	1599833760294511	59
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760129546	53
bert/encoder/layer_10/output/dense/bias/adam_v/read	-1	-1
truediv_90	1599833760321554	21
Assign_193	1599833760295960	13
gradients/bert/encoder/layer_2/attention/self/transpose_1_grad/transpose	1599833760223566	189
Mul_157	1599833759810518	4
gradients/bert/encoder/layer_5/output/LayerNorm/moments/mean_grad/Tile	1599833760138720	27
add_427	1599833760295331	9
gradients/bert/encoder/layer_5/attention/self/Reshape_3_grad/Reshape	-1	-1
bert/encoder/layer_10/output/dense/bias/adam_v	-1	-1
bert/encoder/layer_0/intermediate/dense/add_1	1599833759839257	195
Assign_161	1599833760308465	14
gradients/bert/encoder/layer_1/output/LayerNorm/moments/mean_grad/Tile	1599833760227182	27
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760196066	5
add_450	1599833760294612	4
bert/encoder/layer_8/attention/output/dropout/random_uniform/mul	-1	-1
global_norm/L2Loss_171	1599833760043089	13
bert/encoder/layer_0/output/dropout/random_uniform	-1	-1
clip_by_global_norm/mul_60	1599833760275426	13
bert/encoder/layer_11/output/LayerNorm/batchnorm/mul	1599833759990022	39
bert/encoder/layer_10/output/LayerNorm/batchnorm/mul	1599833759976644	39
bert/encoder/layer_3/output/LayerNorm/moments/variance	1599833759882912	31
bert/encoder/layer_11/intermediate/dense/mul_2	1599833759986727	193
bert/encoder/layer_11/attention/self/value/bias/adam_m	-1	-1
bert/encoder/layer_2/intermediate/dense/add_1	1599833759866063	196
Sqrt_83	1599833760308621	10
clip_by_global_norm/mul_195	1599833760275102	4
add_277	1599833760317289	8
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
bert/encoder/layer_10/intermediate/dense/Pow	1599833759971307	366
Mul_718	1599833760294618	4
Mul_453	1599833760277071	4
gradients/bert/encoder/layer_10/attention/self/Reshape_3_grad/Reshape	-1	-1
Sqrt_75	1599833760315452	16
Assign_227	1599833760315477	20
bert/encoder/layer_5/attention/self/Mul	1599833759899454	99
bert/encoder/layer_2/output/dense/bias/adam_v/read	-1	-1
Mul_780	1599833759806215	11
Assign_210	1599833760337383	4
bert/embeddings/LayerNorm/gamma/adam_v/read	-1	-1
clip_by_global_norm/mul_39	1599833760273795	4
bert/encoder/layer_4/output/dense/bias/read	-1	-1
bert/encoder/layer_11/output/dropout/mul_1	1599833759989739	73
bert/encoder/layer_8/attention/output/dense/kernel/read	-1	-1
Mul_61	1599833760278390	4
bert/encoder/layer_11/output/dense/bias/read	-1	-1
bert/encoder/layer_2/intermediate/dense/bias/adam_v/read	-1	-1
Sqrt_173	1599833760311786	6
global_norm/L2Loss_27	1599833760242165	12
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Square_78	1599833760280467	4
bert/encoder/layer_1/output/dense/kernel	-1	-1
bert/encoder/layer_6/output/LayerNorm/gamma/adam_v/read	-1	-1
bert/encoder/layer_2/attention/self/key/kernel/adam_m/read	-1	-1
Mul_490	1599833759820294	10
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760019046	32
Assign_31	1599833760300123	4
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760072672	53
bert/encoder/layer_11/output/LayerNorm/batchnorm/Rsqrt	1599833759990016	4
PolynomialDecay/sub	-1	-1
Assign_345	1599833760334035	4
bert/encoder/layer_2/attention/output/dense/bias/adam_v/read	-1	-1
Square_48	1599833760276393	4
bert/encoder/layer_8/output/dropout/mul_1	1599833759949573	73
add_178	1599833760303738	4
Assign_228	1599833760339519	10
Mul_1040	1599833759806596	12
bert/encoder/layer_3/output/LayerNorm/beta	-1	-1
gradients/cls/predictions/transform/dense/mul_grad/Mul_1	-1	-1
gradients/bert/encoder/layer_5/output/LayerNorm/moments/variance_grad/Tile	1599833760138911	27
gradients/bert/encoder/layer_8/intermediate/dense/mul_3_grad/Mul_1	1599833759944891	195
Mul_1046	1599833760290858	39
Mul_275	1599833759817922	4
gradients/bert/encoder/layer_9/output/dense/BiasAdd_grad/BiasAddGrad	1599833760050938	55
bert/encoder/layer_1/attention/self/value/bias/adam_m	-1	-1
Mul_711	1599833760276087	4
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul_1	1599833759886630	99
bert/encoder/layer_0/attention/self/query/bias/read	-1	-1
gradients/bert/encoder/layer_1/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760240840	56
Assign_172	1599833760297744	14
gradients/bert/encoder/layer_7/attention/self/Reshape_grad/Reshape	-1	-1
Mul_204	1599833759811170	13
Square_99	1599833760276693	4
bert/encoder/layer_0/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v/read	-1	-1
add_349	1599833760282811	4
clip_by_global_norm/mul_61	1599833760273447	4
gradients/cls/predictions/transform/dense/MatMul_grad/MatMul	1599833760004943	163
bert/encoder/layer_6/output/dense/bias/read	-1	-1
add_641	1599833760302616	10
gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul_1	1599833760225777	615
Sqrt_154	1599833760311145	4
gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1	1599833760090449	241
global_norm/L2Loss_189	1599833760018796	5
bert/encoder/layer_0/output/dense/bias/adam_v/read	-1	-1
GatherV2	1599833759990321	19
Mul_1008	1599833759808567	10
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
Mul_957	1599833759817719	44
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760218196	47
global_norm/L2Loss_187	1599833760021019	13
global_norm/L2Loss_61	1599833760195661	5
bert/encoder/layer_11/output/LayerNorm/gamma/adam_v/read	-1	-1
Assign_543	1599833760341314	14
bert/encoder/layer_1/attention/self/query/bias/read	-1	-1
bert/encoder/layer_11/attention/self/key/bias/adam_m	-1	-1
Mul_1014	1599833760291068	18
gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul	1599833760222901	244
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum	1599833760244663	55
Mul_227	1599833760277351	12
Mul_384	1599833759818288	11
bert/encoder/layer_7/output/dropout/random_uniform	-1	-1
Square_203	1599833760279899	4
Mul_324	1599833760280339	12
Sqrt_140	1599833760315263	11
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760217790	53
bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference	1599833759848000	55
bert/encoder/layer_10/attention/self/query/bias/adam_m	-1	-1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760107175	69
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub	1599833759968758	54
bert/encoder/layer_6/attention/self/query/bias/adam_m	-1	-1
Mul_131	1599833759809269	18
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v	-1	-1
bert/encoder/layer_10/output/dense/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_11/attention/self/Reshape_2_grad/Reshape	-1	-1
sub_107	1599833760330011	8
Assign_591	1599833760341402	13
bert/encoder/layer_7/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_0/output/dense/bias/read	-1	-1
bert/encoder/layer_4/attention/self/value/bias/adam_v	-1	-1
Mul_50	1599833760278493	4
truediv_184	1599833760323682	17
cls/predictions/transform/dense/kernel/read	-1	-1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760040665	74
bert/encoder/layer_5/intermediate/dense/bias/adam_m/read	-1	-1
Sqrt_130	1599833760316456	8
Mul_325	1599833759818979	18
gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1	1599833760087745	240
gradients/bert/encoder/layer_4/intermediate/dense/mul_3_grad/Mul	1599833760166226	279
mul_381	1599833760330287	15
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul_1	1599833759846438	99
Mul_233	1599833760277496	4
bert/encoder/layer_0/output/dense/bias/adam_m/read	-1	-1
add_615	1599833760290358	22
add_262	1599833760294718	11
add_309	1599833760282783	4
add_424	1599833760294991	9
gradients/bert/encoder/layer_5/intermediate/dense/mul_3_grad/Mul	1599833760144161	279
bert/encoder/layer_11/output/LayerNorm/beta/adam_v/read	-1	-1
bert/encoder/layer_5/attention/self/query/bias/adam_m	-1	-1
bert/encoder/layer_6/intermediate/dense/BiasAdd	1599833759917548	224
gradients/bert/embeddings/LayerNorm/moments/variance_grad/Tile/multiples	-1	-1
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v	-1	-1
bert/encoder/layer_0/output/dense/kernel/read	-1	-1
global_norm/L2Loss_66	1599833760188331	5
Sqrt_202	1599833760312971	8
Sqrt_106	1599833760307402	8
Assign_327	1599833760337597	5
clip_by_global_norm/mul_187	1599833760274991	4
Mul_244	1599833760276677	4
Mul_971	1599833760290946	8
Assign_85	1599833760296702	13
gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul	1599833760132855	639
bert/pooler/dense/bias/adam_v	-1	-1
bert/encoder/layer_3/attention/output/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/mul	1599833759905343	194
bert/encoder/layer_7/intermediate/dense/mul	1599833759931904	195
truediv_69	1599833760324521	5
add_679	1599833760328256	16
gradients/AddN_72	1599833760218521	96
bert/encoder/layer_1/intermediate/dense/MatMul	1599833759848400	2192
bert/encoder/layer_4/intermediate/dense/kernel/adam_m	-1	-1
Mul_1057	1599833760291572	9
bert/encoder/layer_1/attention/self/dropout/random_uniform	-1	-1
bert/encoder/layer_3/output/LayerNorm/gamma/adam_m/read	-1	-1
mul_585	1599833759813724	18
add_699	1599833759812557	14
bert/encoder/layer_0/intermediate/dense/mul_1	1599833759838850	195
truediv_5	1599833760322821	5
Assign_260	1599833760308215	12
bert/encoder/layer_5/attention/self/value/kernel/adam_v/read	-1	-1
mul_896	1599833759815603	18
Assign_248	1599833760314649	3
truediv_127	1599833760320858	8
truediv_145	1599833760324697	9
gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul	1599833760069032	640
Assign_82	1599833760296112	14
bert/encoder/layer_5/attention/self/query/kernel/read	-1	-1
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v	-1	-1
add_345	1599833760282612	17
bert/encoder/layer_9/output/LayerNorm/moments/mean	1599833759963119	32
Assign_64	1599833760303448	21
Mul_44	1599833760278371	11
Mul_754	1599833760281439	13
bert/encoder/layer_3/attention/self/key/bias/read	-1	-1
Mul_77	1599833760278637	4
add_556	1599833760301191	9
Assign_130	1599833760303605	20
global_norm/L2Loss_67	1599833760182679	5
clip_by_global_norm/mul_69	1599833760275616	4
Assign_341	1599833760315974	41
sub_124	1599833760334058	17
Assign_335	1599833760316620	41
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760205365	53
cls/predictions/mul_1	1599833760004515	6
gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/mul_1	1599833760123133	280
bert/encoder/layer_6/output/LayerNorm/batchnorm/Rsqrt	1599833759923101	4
add_428	1599833760306168	12
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m	-1	-1
add_638	1599833760319163	16
bert/encoder/layer_4/attention/self/key/BiasAdd	1599833759885219	60
gradients/AddN_3	1599833760005337	71
gradients/bert/encoder/layer_1/intermediate/dense/mul_2_grad/Mul_1	1599833760232902	279
gradients/bert/encoder/layer_3/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760199107	55
gradients/bert/encoder/layer_6/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760136624	54
cls/predictions/transform/dense/add_1	1599833759990690	7
cls/predictions/transform/dense/mul_3	1599833759990731	11
sub_3	1599833760335995	12
Square_34	1599833760277299	4
add_105	1599833760317069	8
mul_821	1599833759815214	13
bert/encoder/layer_6/attention/self/key/bias/read	-1	-1
bert/encoder/layer_10/attention/self/transpose_2	1599833759966021	190
Assign_352	1599833760306033	20
Mul_562	1599833760293464	17
bert/encoder/layer_10/output/LayerNorm/batchnorm/sub	1599833759976814	54
Mul_522	1599833759808537	11
bert/encoder/layer_5/attention/self/key/kernel/adam_m/read	-1	-1
bert/encoder/layer_2/attention/self/key/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760161172	72
clip_by_global_norm/mul_124	1599833760273374	13
gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul_1	1599833760047486	661
bert/encoder/layer_7/output/dropout/mul	1599833759826880	52
bert/encoder/layer_4/attention/self/query/bias/adam_m/read	-1	-1
bert/encoder/layer_10/attention/self/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_0/output/dense/bias	-1	-1
gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/Pow	1599833759837980	196
gradients/bert/encoder/layer_6/intermediate/dense/Tanh_grad/TanhGrad	1599833760122661	274
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
Assign_605	1599833760312668	5
Assign_293	1599833760308381	41
bert/embeddings/word_embeddings/adam_m/read	-1	-1
Sqrt_103	1599833760315078	16
bert/encoder/layer_5/intermediate/dense/kernel	-1	-1
mul_677	1599833760329409	4
bert/encoder/layer_6/attention/self/value/bias/adam_v	-1	-1
add_390	1599833760295186	9
Mul_712	1599833759806302	11
sub_174	1599833760331876	4
gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul_1	1599833760137319	611
add_501	1599833760295632	4
add_152	1599833760319717	16
bert/encoder/layer_8/attention/self/dropout/GreaterEqual	1599833759823052	74
clip_by_global_norm/mul_135	1599833760275987	4
add_121	1599833760298502	9
mul_623	1599833760329500	4
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760018544	51
Mul_100	1599833760284409	40
Mul_745	1599833760293764	17
bert/encoder/layer_3/attention/self/transpose_1	1599833759872142	189
add_53	1599833760300353	4
add_92	1599833760328429	20
clip_by_global_norm/mul_11	1599833760274433	4
bert/encoder/layer_6/output/LayerNorm/moments/mean	1599833759922972	32
Mul_549	1599833760276960	12
Square_178	1599833760279344	4
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
truediv_118	1599833760325165	18
gradients/bert/encoder/layer_6/output/LayerNorm/moments/variance_grad/truediv	1599833760116848	52
clip_by_global_norm/mul_199	1599833760275108	4
Sqrt_134	1599833760316494	8
bert/encoder/layer_7/attention/self/query/kernel/adam_v	-1	-1
Mul_744	1599833759805782	18
clip_by_global_norm/mul_105	1599833760275805	4
Mul_884	1599833759807664	4
Mul_819	1599833759807525	18
gradients/bert/encoder/layer_9/attention/self/transpose_3_grad/transpose	1599833760065249	189
Assign_214	1599833760304750	20
clip_by_global_norm/mul_3	1599833760274538	9
mul_317	1599833760330496	12
bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference	1599833759874799	55
Mul_432	1599833759804768	51
gradients/bert/encoder/layer_4/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760180797	55
global_norm/L2Loss_83	1599833760160571	5
bert/encoder/layer_2/output/dense/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_6/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760132799	54
bert/encoder/layer_4/attention/output/LayerNorm/moments/mean	1599833759888142	32
Assign_326	1599833760314907	11
bert/encoder/layer_0/intermediate/dense/mul_2/x	-1	-1
clip_by_global_norm/mul_73	1599833760275794	4
bert/encoder/layer_5/output/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_7/attention/self/transpose_1	1599833759925681	189
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1	1599833759901706	73
bert/encoder/layer_8/output/LayerNorm/beta/adam_v	-1	-1
add_231	1599833760293013	10
Mul_595	1599833760293313	9
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m	-1	-1
bert/encoder/layer_2/attention/self/key/bias/read	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/beta	-1	-1
bert/encoder/layer_5/intermediate/dense/kernel/adam_m/read	-1	-1
Sqrt_147	1599833760316663	5
add_153	1599833760328533	23
bert/encoder/layer_2/output/LayerNorm/beta	-1	-1
bert/encoder/layer_3/attention/self/key/bias/adam_v	-1	-1
bert/encoder/layer_4/intermediate/dense/add	1599833759892154	278
gradients/cls/predictions/Sum_grad/Tile	1599833759828534	148
bert/encoder/layer_0/intermediate/dense/Pow	1599833759837416	366
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m	-1	-1
truediv_111	1599833760324566	8
bert/encoder/layer_9/output/LayerNorm/batchnorm/mul	1599833759963253	39
clip_by_global_norm/mul_103	1599833760273807	4
gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul	1599833760065440	159
bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1	1599833759909763	73
add_423	1599833760325574	17
Assign_324	1599833760337592	4
Mul_722	1599833759806626	18
add_493	1599833760320231	8
Assign_221	1599833760308074	20
bert/encoder/layer_1/attention/self/query/bias/adam_m	-1	-1
add_578	1599833760327700	17
add_653	1599833760292034	11
Mul_850	1599833759806871	11
gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1	1599833760220441	241
Sqrt_120	1599833760315503	9
add_684	1599833760302032	4
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul	1599833760043293	161
add_264	1599833760320427	8
bert/encoder/layer_3/output/LayerNorm/gamma/read	-1	-1
Assign_401	1599833760315900	20
Mul_846	1599833760278837	4
add_90	1599833760303320	21
Assign_101	1599833760307676	41
gradients/bert/encoder/layer_9/attention/self/Reshape_grad/Reshape	-1	-1
gradients/bert/encoder/layer_9/attention/output/dropout/mul_1_grad/Mul	1599833760063781	51
Mul_6	1599833759810620	364
gradients/bert/encoder/layer_2/intermediate/dense/mul_3_grad/Mul	1599833760210528	279
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
clip_by_global_norm/mul_41	1599833760273981	4
mul_736	1599833760329772	15
bert/encoder/layer_3/attention/output/dense/kernel/adam_m	-1	-1
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760173554	32
bert/encoder/layer_2/output/dense/bias/read	-1	-1
Assign_434	1599833760315329	10
bert/encoder/layer_11/attention/output/dense/kernel	-1	-1
add_510	1599833760327606	17
bert/encoder/layer_9/output/LayerNorm/beta/adam_m	-1	-1
Assign_219	1599833760340255	14
Mul_584	1599833760282345	13
mul_26	1599833760327544	4
Assign_466	1599833760300591	13
Assign_539	1599833760312708	11
Mul_267	1599833760281956	4
add_218	1599833760303839	61
truediv_1	1599833760322291	526
bert/encoder/layer_11/intermediate/dense/bias/adam_v/read	-1	-1
Mul_964	1599833760279350	4
bert/encoder/layer_3/output/LayerNorm/gamma/adam_v	-1	-1
Assign_224	1599833760314481	11
Mul_249	1599833760277370	4
add_183	1599833760282659	17
add_313	1599833760298452	22
bert/encoder/layer_1/output/LayerNorm/gamma/adam_m/read	-1	-1
add_617	1599833760318885	13
bert/encoder/layer_10/attention/output/dropout/mul	1599833759826933	51
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_10/output/dropout/mul	1599833759825873	52
Mul_760	1599833760280832	4
bert/encoder/layer_11/attention/self/key/kernel	-1	-1
Sqrt	1599833760309950	375
bert/encoder/layer_5/attention/self/key/BiasAdd	1599833759898595	61
Mul_79	1599833760284676	4
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_4/attention/output/LayerNorm/gamma	-1	-1
mul_601	1599833760328948	7
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2	1599833759968703	54
bert/encoder/layer_3/attention/self/MatMul_1	1599833759873485	247
Square_84	1599833760277250	4
sub_189	1599833760332342	8
Sqrt_157	1599833760311546	8
Sqrt_57	1599833760308483	16
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760217720	68
mul_1016	1599833760332184	16
Mul_831	1599833760290621	17
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum	1599833760023512	55
global_norm/L2Loss_24	1599833760248519	5
add_631	1599833760318904	17
Mul_1044	1599833760279501	39
Assign_51	1599833760340940	41
gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/truediv	1599833760004770	6
Mul_1109	1599833759812002	11
Mul_697	1599833760293817	10
Mul_436	1599833759818889	11
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760217518	73
mul_225	1599833760326667	4
truediv_94	1599833760321361	4
Assign_270	1599833760334793	9
global_norm/L2Loss_87	1599833760160153	16
clip_by_global_norm/mul_83	1599833760275545	4
bert/encoder/layer_4/intermediate/dense/bias/read	-1	-1
mul_1048	1599833760332086	42
Assign_166	1599833760303767	13
Mul_1039	1599833760280053	4
Mul_282	1599833759817765	4
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul	1599833759834695	39
gradients/AddN_47	1599833760139174	95
Mul_842	1599833760284883	12
cls/predictions/transform/dense/bias	-1	-1
bert/encoder/layer_5/output/LayerNorm/batchnorm/Rsqrt	1599833759909717	4
bert/encoder/layer_0/attention/self/value/bias/adam_v/read	-1	-1
bert/encoder/layer_2/attention/output/dense/kernel	-1	-1
mul_122	1599833759815526	12
gradients/bert/encoder/layer_11/attention/self/Reshape_grad/Reshape	-1	-1
bert/encoder/layer_5/output/LayerNorm/beta/read	-1	-1
global_norm/L2Loss_147	1599833760072179	5
bert/encoder/layer_9/output/dropout/random_uniform/mul	-1	-1
gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul_1	1599833760158071	611
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760262274	27
Assign_337	1599833760304569	12
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760116516	56
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760262302	33
bert/encoder/layer_7/intermediate/dense/add	1599833759932296	280
truediv_46	1599833760322054	7
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul	1599833760022050	141
gradients/cls/predictions/BiasAdd_grad/BiasAddGrad	1599833759997378	155
bert/encoder/layer_10/intermediate/dense/add_1	1599833759973148	196
cls/predictions/transform/dense/mul_1	1599833759990647	6
bert/encoder/layer_1/attention/self/key/kernel/adam_v	-1	-1
clip_by_global_norm/mul_23	1599833760275312	4
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760072344	53
Mul_561	1599833759815262	19
bert/encoder/layer_2/output/dense/bias	-1	-1
sub_85	1599833760330736	7
mul_956	1599833760328286	4
add_470	1599833760329088	23
global_norm/global_norm	1599833760273231	5
gradients/bert/encoder/layer_4/attention/self/Reshape_1_grad/Reshape	-1	-1
bert/encoder/layer_10/attention/self/value/bias/adam_v/read	-1	-1
gradients/cls/predictions/transform/dense/Pow_grad/mul_1	1599833760004892	11
bert/encoder/layer_10/attention/output/dense/bias/adam_m	-1	-1
add_595	1599833760291335	57
Mul_110	1599833759809252	4
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760138782	65
add_24	1599833760327408	17
add_418	1599833760295739	9
Assign_25	1599833760300235	4
bert/encoder/layer_4/intermediate/dense/bias	-1	-1
bert/encoder/layer_8/attention/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_9/attention/self/dropout/random_uniform	-1	-1
gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul	1599833760158742	639
Assign_153	1599833760337278	4
add_192	1599833760319788	12
clip_by_global_norm/mul_143	1599833760275865	4
clip_by_global_norm/mul_52	1599833760275406	4
gradients/AddN_78	1599833760239975	69
bert/encoder/layer_8/attention/output/dense/bias/adam_m	-1	-1
mul_220	1599833760330816	16
sub_8	1599833760335843	16
gradients/AddN_17	1599833760049494	119
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
Mul_296	1599833759819254	11
truediv_163	1599833760323266	5
gradients/cls/predictions/LogSoftmax_grad/Sum	1599833759829290	151
bert/encoder/layer_8/intermediate/dense/bias/adam_v/read	-1	-1
bert/encoder/Reshape	-1	-1
sub_62	1599833760330267	7
global_norm/L2Loss_113	1599833760122627	32
add_341	1599833760317167	8
Sqrt_178	1599833760312070	8
Mul_1066	1599833759805333	19
gradients/bert/encoder/layer_0/attention/self/dropout/mul_1_grad/Mul	1599833760264874	99
mul_870	1599833760327879	4
bert/encoder/layer_8/attention/self/dropout/random_uniform/RandomUniform	1599833759801360	83
bert/encoder/layer_8/output/dropout/Cast	1599833759824520	36
Mul_519	1599833760283153	39
mul_440	1599833760326000	8
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760262164	54
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
cls/predictions/transform/LayerNorm/gamma/read	-1	-1
truediv_78	1599833760321722	5
mul_999	1599833760328235	4
add_249	1599833760305401	9
bert/encoder/layer_9/intermediate/dense/add	1599833759959070	280
truediv_56	1599833760324214	17
bert/encoder/layer_10/attention/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_0/attention/self/key/kernel/adam_v/read	-1	-1
add_163	1599833760292479	56
sub_57	1599833760332979	7
add_411	1599833760305165	9
Mul_404	1599833759818551	11
clip_by_global_norm/mul_131	1599833760275982	4
Assign_344	1599833760316313	10
bert/encoder/layer_11/attention/self/add	1599833759979866	128
bert/encoder/layer_5/attention/output/dropout/random_uniform	-1	-1
bert/encoder/layer_11/intermediate/dense/kernel	-1	-1
bert/encoder/layer_6/attention/self/key/kernel	-1	-1
gradients/bert/encoder/layer_7/attention/self/Reshape_1_grad/Reshape	-1	-1
gradients/bert/encoder/layer_6/attention/self/Reshape_2_grad/Reshape	-1	-1
Mul_1064	1599833759806563	18
Assign_195	1599833760340124	41
bert/encoder/layer_2/output/LayerNorm/batchnorm/add_1	1599833759869788	74
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760116902	53
add_642	1599833760319322	8
sub_27	1599833760332700	8
bert/encoder/layer_1/attention/self/value/kernel/adam_v	-1	-1
bert/encoder/layer_7/attention/output/LayerNorm/beta	-1	-1
bert/encoder/layer_3/attention/self/key/kernel/read	-1	-1
bert/encoder/layer_0/output/LayerNorm/batchnorm/mul	1599833759842752	39
Square_72	1599833760281048	4
add_399	1599833760295233	21
global_norm/L2Loss_183	1599833760027472	16
gradients/bert/encoder/layer_5/attention/self/Reshape_grad/Reshape	-1	-1
gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1	1599833760156744	240
gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/mul	1599833759945481	194
sub_95	1599833760330405	8
bert/encoder/layer_10/attention/self/Softmax	1599833759966605	323
bert/encoder/layer_11/attention/self/value/bias	-1	-1
bert/encoder/layer_2/attention/output/dense/bias/adam_m	-1	-1
Assign_364	1599833760306085	20
Mul_759	1599833759807019	12
Assign_173	1599833760308505	20
cls/seq_relationship/mul	1599833759990705	4
sub_133	1599833760333397	4
add_132	1599833760325849	17
Mul_861	1599833760279060	40
Assign_15	1599833760340998	13
bert/encoder/layer_3/attention/self/query/kernel	-1	-1
add_21	1599833760284565	17
add_647	1599833760302669	9
add_273	1599833760320015	44
add_634	1599833760301816	10
bert/encoder/layer_4/attention/self/key/kernel/adam_v	-1	-1
Mul_765	1599833760280931	4
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_3/intermediate/dense/kernel/adam_v	-1	-1
gradients/cls/predictions/transform/dense/Pow_grad/mul	1599833759990590	7
Mul_200	1599833760277600	4
Mul_438	1599833759819194	11
bert/encoder/layer_7/attention/output/dropout/mul_1	1599833759928141	73
add_413	1599833760295283	21
bert/encoder/layer_8/attention/self/key/bias/adam_m	-1	-1
Assign_225	1599833760341877	13
bert/encoder/layer_5/attention/output/dense/bias/adam_m	-1	-1
gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul_1	1599833760067122	658
Assign_230	1599833760315804	13
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760205275	5
bert/encoder/layer_2/output/dense/bias/adam_m	-1	-1
Assign_396	1599833760339487	4
clip_by_global_norm/mul_112	1599833760276007	40
bert/encoder/layer_7/attention/self/key/bias/adam_m/read	-1	-1
bert/encoder/layer_10/output/LayerNorm/beta/adam_v	-1	-1
add_239	1599833760317249	16
bert/encoder/layer_1/attention/output/LayerNorm/beta/read	-1	-1
bert/encoder/layer_1/intermediate/dense/mul_2	1599833759852856	195
gradients/AddN_69	1599833760211878	365
global_norm/mul	1599833760273225	4
Assign_529	1599833760302928	12
Mul_400	1599833759818316	17
Mul_642	1599833759816041	11
Assign_74	1599833760307540	3
global_norm/L2Loss_132	1599833760094719	5
Assign_584	1599833760312899	10
bert/encoder/layer_10/attention/self/key/bias/adam_v/read	-1	-1
add_376	1599833760293384	12
Mul_774	1599833759805184	44
Mul_138	1599833760282113	4
bert/encoder/layer_0/output/LayerNorm/moments/variance	1599833759842707	32
add_9	1599833760319558	8
clip_by_global_norm/mul_133	1599833760275781	5
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760195771	55
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760240341	5
Mul_407	1599833760292787	10
Mul_1067	1599833760290918	12
add_667	1599833760303062	4
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760138748	32
bert/encoder/layer_0/output/dense/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul_1	1599833760008999	2473
bert/encoder/layer_7/attention/self/dropout/mul	1599833759827993	99
add_336	1599833760282805	4
bert/encoder/layer_3/attention/self/value/MatMul	1599833759871130	633
cls/predictions/transform/LayerNorm/gamma	-1	-1
bert/encoder/layer_3/attention/self/key/bias/adam_v/read	-1	-1
Assign_419	1599833760316552	20
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub	1599833759861671	55
sub_44	1599833760337195	21
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760262403	46
bert/encoder/layer_6/intermediate/dense/kernel	-1	-1
mul_150	1599833760328515	7
add_197	1599833760282691	17
bert/encoder/layer_11/attention/self/dropout/GreaterEqual	1599833759823574	74
bert/encoder/layer_8/attention/self/key/kernel/read	-1	-1
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
gradients/bert/encoder/layer_1/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760245861	55
Sqrt_36	1599833760309879	4
bert/encoder/layer_0/attention/self/transpose	1599833759831739	190
gradients/bert/encoder/layer_7/attention/self/dropout/mul_1_grad/Mul	1599833760110130	98
truediv_136	1599833760320903	21
Mul_1001	1599833760279383	12
add_491	1599833760293687	11
Assign_561	1599833760341431	14
bert/encoder/layer_9/attention/self/query/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_6/intermediate/dense/mul_1_grad/Mul_1	1599833760122936	195
add_95	1599833760319698	8
gradients/bert/encoder/layer_11/output/dense/BiasAdd_grad/BiasAddGrad	1599833760006716	53
Mul_238	1599833760280191	11
mul_236	1599833760326672	4
add_477	1599833760329603	20
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_5/attention/output/dense/bias/adam_v	-1	-1
Sqrt_73	1599833760308051	17
Mul_670	1599833760281844	12
bert/encoder/layer_0/intermediate/dense/mul_2	1599833759839455	194
Mul_877	1599833759808493	4
Mul_516	1599833759807679	45
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760151423	53
Sqrt_153	1599833760312356	16
mul_445	1599833759815416	45
clip_by_global_norm/mul_38	1599833760273556	12
gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul	1599833760026063	639
bert/encoder/layer_2/intermediate/dense/MatMul	1599833759861801	2196
bert/encoder/layer_11/intermediate/dense/kernel/adam_v/read	-1	-1
Assign_614	1599833760313659	11
bert/encoder/layer_0/attention/self/key/bias/adam_m/read	-1	-1
truediv_96	1599833760322096	61
Mul_307	1599833759818948	11
global_norm/L2Loss_93	1599833760151477	5
bert/encoder/layer_7/attention/self/value/BiasAdd	1599833759925428	60
mul_1005	1599833760331947	12
sub_175	1599833760331987	8
Assign_92	1599833760308167	11
bert/encoder/layer_4/output/dense/BiasAdd	1599833759895987	60
Mul_431	1599833760280660	40
Mul_1055	1599833760279855	4
Mul_388	1599833760280797	12
bert/encoder/layer_2/intermediate/dense/kernel/adam_m	-1	-1
Assign_236	1599833760314525	17
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760205153	65
Sqrt_187	1599833760312784	15
bert/encoder/layer_4/attention/self/query/MatMul	1599833759883255	632
bert/encoder/layer_7/attention/self/dropout/random_uniform	-1	-1
add_93	1599833760292357	8
Sqrt_18	1599833760311079	4
clip_by_global_norm/mul_25	1599833760273499	4
bert/encoder/layer_5/attention/self/key/kernel	-1	-1
bert/encoder/layer_3/attention/self/Reshape_3	-1	-1
Mul_622	1599833760295209	9
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760129786	5
Square_189	1599833760279691	4
bert/encoder/layer_8/attention/self/Softmax	1599833759939834	323
bert/encoder/layer_5/output/LayerNorm/batchnorm/mul	1599833759909722	39
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_5/attention/self/value/kernel/adam_v	-1	-1
bert/encoder/layer_5/output/dense/kernel	-1	-1
Mul_221	1599833759811948	10
bert/encoder/layer_3/attention/self/value/BiasAdd	1599833759871889	61
truediv_202	1599833760323670	4
gradients/bert/encoder/layer_2/attention/self/Reshape_2_grad/Reshape	-1	-1
bert/encoder/layer_1/attention/output/add	1599833759847893	72
Mul_1071	1599833760279707	4
Mul_259	1599833760280246	39
add_99	1599833760325532	16
Square_138	1599833760281337	4
bert/encoder/layer_8/attention/self/Reshape_3	-1	-1
bert/encoder/layer_9/attention/self/query/bias/read	-1	-1
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760160856	65
Square_83	1599833760277065	4
clip_by_global_norm/mul_159	1599833760274654	4
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760271163	53
Mul_361	1599833759820055	12
bert/encoder/layer_6/attention/self/query/kernel/adam_m	-1	-1
bert/encoder/layer_11/attention/self/key/kernel/adam_v	-1	-1
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v	-1	-1
bert/encoder/layer_3/attention/output/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_0/intermediate/dense/bias/adam_v/read	-1	-1
mul_563	1599833759814131	20
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760018742	53
add_19	1599833760300074	4
Assign_562	1599833760302166	19
sub_194	1599833760336562	56
gradients/bert/encoder/layer_6/attention/self/Reshape_1_grad/Reshape	-1	-1
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760240484	51
Mul_1061	1599833759805274	12
mul_930	1599833760331907	13
mul_177	1599833760330992	42
bert/encoder/layer_10/output/dropout/Cast	1599833759823990	36
mul_1074	1599833760328161	4
gradients/bert/encoder/layer_0/intermediate/dense/mul_3_grad/Mul_1	1599833759837784	195
add_495	1599833760305288	56
Assign_69	1599833760341588	13
sub	1599833759825606	72
Mul_255	1599833759817630	10
gradients/bert/encoder/layer_4/output/LayerNorm/moments/mean_grad/truediv	1599833760160923	46
Square_177	1599833760279143	39
mul_349	1599833760332821	43
gradients/bert/encoder/layer_5/intermediate/dense/mul_3_grad/Mul_1	1599833759904752	195
bert/embeddings/LayerNorm/beta	-1	-1
bert/encoder/layer_11/attention/self/query/bias	-1	-1
Mul_17	1599833760278612	7
add_594	1599833760318964	8
Mul_239	1599833759812055	17
add_401	1599833760320647	16
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760182927	32
add_432	1599833760316704	7
bert/encoder/layer_6/attention/self/key/MatMul	1599833759910654	633
add_478	1599833760293636	11
sub_20	1599833760331627	4
bert/encoder/layer_1/output/LayerNorm/gamma/read	-1	-1
Sqrt_155	1599833760311156	12
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760151890	26
Assign_234	1599833760337393	8
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/add_1	1599833759848326	73
PolynomialDecay/Mul	1599833759827099	4
add_408	1599833760316851	16
truediv_129	1599833760324742	6
bert/encoder/layer_11/output/LayerNorm/gamma	-1	-1
gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1	1599833760245232	241
Mul_548	1599833759813253	12
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760160317	51
Assign_284	1599833760308310	12
bert/encoder/layer_5/attention/output/LayerNorm/beta	-1	-1
truediv_157	1599833760323143	4
Mul_680	1599833759816235	11
add_492	1599833760304959	9
bert/encoder/layer_9/intermediate/dense/mul_3	1599833759960152	279
Square_201	1599833760279609	4
cls/predictions/transform/dense/mul	1599833759990578	10
gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/Pow	1599833759878186	195
clip_by_global_norm/mul_102	1599833760273717	12
gradients/AddN_85	1599833760262094	68
bert/encoder/layer_8/intermediate/dense/add_1	1599833759946361	195
Assign_615	1599833760336881	11
Mul_916	1599833759809660	18
Square_156	1599833760278843	4
bert/encoder/Reshape_1/shape	-1	-1
bert/encoder/layer_3/attention/output/dense/bias	-1	-1
add_168	1599833760296357	5
gradients/AddN_89	1599833760271901	97
clip_by_global_norm/mul_72	1599833760275658	13
bert/encoder/layer_1/attention/self/transpose_3	1599833759846926	189
add_406	1599833760281792	18
bert/encoder/layer_9/intermediate/dense/kernel/adam_v	-1	-1
Mul_97	1599833759809206	44
gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul	1599833760057523	2499
bert/encoder/layer_10/attention/self/value/kernel/adam_m/read	-1	-1
mul_182	1599833760326768	4
truediv_142	1599833760324806	9
bert/encoder/layer_2/output/LayerNorm/gamma/adam_m	-1	-1
add_283	1599833760304207	4
add_545	1599833760290022	9
bert/encoder/layer_8/attention/output/dense/bias/adam_v/read	-1	-1
truediv_88	1599833760321745	20
cls/predictions/transform/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_0/output/LayerNorm/beta/adam_m/read	-1	-1
add_672	1599833760328085	16
bert/encoder/layer_4/attention/self/transpose	1599833759885342	190
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760107741	52
add_166	1599833760328558	58
Cast	1599833759812522	14
bert/encoder/layer_0/attention/self/query/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1	1599833760024069	240
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760093921	71
Sqrt_98	1599833760308826	9
bert/encoder/layer_7/attention/self/dropout/mul_1	1599833759926881	141
Assign_310	1599833760304671	20
bert/encoder/layer_9/output/dense/bias/adam_v	-1	-1
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760218252	5
bert/encoder/layer_3/output/LayerNorm/gamma	-1	-1
Mul_529	1599833759810032	45
bert/encoder/layer_11/output/LayerNorm/beta	-1	-1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_5/output/dense/BiasAdd	1599833759909376	62
Mul_860	1599833759807976	45
add_373	1599833760293287	8
global_norm/L2Loss	1599833760273029	175
gradients/AddN_41	1599833760123414	365
bert/encoder/layer_0/attention/self/key/MatMul	1599833759830286	633
Sqrt_97	1599833760308335	45
clip_by_global_norm/mul_179	1599833760274921	4
Assign_437	1599833760315641	41
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760094327	31
bert/encoder/layer_3/output/dense/MatMul	1599833759880135	2473
Mul_337	1599833760282359	4
bert/encoder/layer_3/attention/self/query/bias/adam_v	-1	-1
bert/encoder/layer_3/output/LayerNorm/beta/adam_v/read	-1	-1
Square_81	1599833760280478	39
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/mean_grad/truediv	1599833760063361	46
Mul_894	1599833759809455	30
bert/encoder/layer_4/attention/self/key/kernel/adam_m	-1	-1
bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2	1599833759976759	53
bert/encoder/layer_11/attention/self/query/bias/adam_m	-1	-1
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760205119	33
gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul	1599833760267090	233
Sqrt_161	1599833760311298	46
global_norm/L2Loss_95	1599833760151046	32
Mul_1020	1599833760291471	10
bert/embeddings/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add_1	1599833759822939	70
bert/encoder/layer_4/output/LayerNorm/batchnorm/sub	1599833759896502	53
bert/encoder/layer_9/attention/output/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_2/attention/self/query/kernel/adam_v/read	-1	-1
add_201	1599833760282937	4
bert/encoder/layer_9/intermediate/dense/mul	1599833759958678	195
bert/encoder/layer_8/attention/self/value/MatMul	1599833759938045	633
Assign_176	1599833760309062	12
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_9/intermediate/dense/bias	-1	-1
add_404	1599833760295684	9
sub_75	1599833760332881	8
add_482	1599833760305216	9
add_284	1599833760319962	4
gradients/AddN_64	1599833760195701	68
mul_289	1599833760329012	7
bert/encoder/layer_2/attention/output/add	1599833759861291	73
add_2	1599833759828516	4
Mul_925	1599833759809754	18
Mul_154	1599833760281748	13
Sqrt_12	1599833760310930	4
bert/embeddings/LayerNorm/batchnorm/add_1	1599833759829496	73
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760085569	27
Mul_781	1599833760280843	4
gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul_1	1599833760111406	611
bert/encoder/layer_6/attention/self/Reshape_1	-1	-1
mul_687	1599833760325387	8
bert/encoder/layer_6/attention/self/query/bias/adam_v	-1	-1
gradients/bert/encoder/layer_8/attention/self/Reshape_grad/Reshape	-1	-1
PolynomialDecay/sub_1	1599833759824859	5
bert/encoder/layer_3/intermediate/dense/add	1599833759878775	280
cls/predictions/transform/LayerNorm/batchnorm/add	1599833759990921	4
add_297	1599833760317328	8
cls/predictions/Sum_1	1599833760004599	5
gradients/bert/encoder/layer_10/intermediate/dense/Tanh_grad/TanhGrad	1599833760034146	275
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
mul_386	1599833760328723	4
add_81	1599833760319639	8
bert/encoder/layer_7/attention/output/LayerNorm/moments/variance	1599833759928379	32
Mul_895	1599833760285069	12
bert/encoder/layer_1/attention/self/key/kernel/adam_m/read	-1	-1
truediv_122	1599833760325185	17
sub_131	1599833760333710	7
truediv_42	1599833760322032	21
add_524	1599833760327991	17
add_550	1599833760318642	39
bert/encoder/layer_3/attention/output/dropout/Cast	1599833759824557	36
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760050319	66
gradients/bert/encoder/layer_11/attention/output/dropout/mul_1_grad/Mul	1599833760019568	51
bert/encoder/layer_2/attention/self/query/bias/read	-1	-1
cls/predictions/output_bias/adam_v/read	-1	-1
bert/encoder/layer_5/attention/self/query/bias/read	-1	-1
bert/encoder/layer_8/attention/self/query/bias/adam_v/read	-1	-1
bert/encoder/layer_11/attention/self/key/bias	-1	-1
truediv_32	1599833760322216	61
Mul_1024	1599833759815725	10
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760183171	53
Assign_302	1599833760307883	12
gradients/bert/encoder/layer_8/attention/self/dropout/mul_grad/Mul	1599833760088278	141
bert/encoder/layer_10/output/LayerNorm/moments/mean	1599833759976509	31
bert/encoder/layer_4/attention/output/dense/bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_10/output/LayerNorm/moments/variance_grad/truediv	1599833760028332	52
gradients/bert/encoder/layer_10/output/LayerNorm/moments/mean_grad/truediv	1599833760028241	47
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760116574	53
bert/encoder/layer_5/attention/output/dense/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760174096	52
truediv_206	1599833760323861	4
Assign_208	1599833760297216	24
bert/encoder/layer_9/intermediate/dense/bias/read	-1	-1
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760196102	51
Assign_420	1599833760337744	9
gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul	1599833760175974	161
add_449	1599833760316722	7
mul_161	1599833760325771	4
bert/encoder/layer_10/output/dropout/GreaterEqual	1599833759821999	38
bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference	1599833759963152	55
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760050584	51
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760249613	51
bert/encoder/layer_7/output/dropout/random_uniform/mul	-1	-1
clip_by_global_norm/mul_97	1599833760273441	4
bert/encoder/layer_11/output/add	1599833759989814	72
add_250	1599833760320406	7
add_409	1599833760325556	17
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760028140	33
clip_by_global_norm/mul_145	1599833760275689	4
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_1/intermediate/dense/kernel/adam_v	-1	-1
Square_204	1599833760280036	4
gradients/bert/encoder/layer_9/output/LayerNorm/moments/mean_grad/Tile	1599833760050255	28
Sqrt_137	1599833760315354	16
clip_by_global_norm/mul_146	1599833760275740	40
add_444	1599833760295387	9
add_106	1599833760282453	4
Mul_1013	1599833759814751	17
bert/encoder/layer_11/attention/self/dropout/mul_1	1599833759980421	142
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760239538	68
bert/encoder/layer_4/attention/self/key/bias/adam_v/read	-1	-1
Mul_756	1599833760295508	17
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m/read	-1	-1
bert/encoder/layer_0/attention/self/Reshape_3	-1	-1
bert/encoder/layer_1/attention/self/dropout/mul	1599833759828094	99
mul_988	1599833760328009	4
add_137	1599833760298552	21
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760239607	73
gradients/bert/encoder/layer_4/attention/self/transpose_3_grad/transpose	1599833760175783	189
bert/encoder/Reshape/shape	-1	-1
Mul_646	1599833760276257	13
bert/encoder/layer_1/attention/output/dense/MatMul	1599833759847117	635
Mul_162	1599833759811631	11
Assign_538	1599833760302078	15
bert/encoder/layer_1/attention/output/dense/bias/read	-1	-1
add_164	1599833760303634	60
add_435	1599833760316892	42
gradients/bert/encoder/layer_0/attention/self/Reshape_3_grad/Reshape	-1	-1
Sqrt_74	1599833760314462	9
Mul_721	1599833760281080	13
bert/encoder/layer_9/intermediate/dense/mul_2	1599833759959956	194
Mul_630	1599833760281037	4
Square_197	1599833760279597	11
clip_by_global_norm/mul_196	1599833760275182	4
global_norm/L2Loss_120	1599833760115862	5
mul_644	1599833760325363	4
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760116007	71
bert/encoder/layer_0/attention/self/transpose/perm	-1	-1
bert/encoder/layer_6/intermediate/dense/bias/adam_v/read	-1	-1
bert/encoder/layer_10/attention/output/dense/bias/adam_v/read	-1	-1
global_norm/L2Loss_32	1599833760239420	9
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760204668	74
truediv_41	1599833760322165	5
bert/encoder/layer_7/attention/output/dense/bias/read	-1	-1
clip_by_global_norm/mul_54	1599833760273731	13
global_norm/L2Loss_84	1599833760160971	5
clip_by_global_norm/mul_186	1599833760274940	13
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760071853	72
bert/encoder/layer_11/attention/self/key/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
sub_139	1599833760333628	4
truediv_58	1599833760321696	18
cls/predictions/transform/LayerNorm/beta/adam_m/read	-1	-1
Mul_969	1599833760279626	4
bert/encoder/layer_8/output/LayerNorm/gamma/adam_m/read	-1	-1
Assign_129	1599833760341602	14
bert/encoder/layer_6/output/dense/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760226762	74
bert/encoder/layer_11/attention/self/query/bias/adam_v	-1	-1
bert/embeddings/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_1/attention/output/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_9/output/LayerNorm/gamma/adam_m	-1	-1
Mul_441	1599833759819529	44
add_233	1599833760319968	4
gradients/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad	1599833760004755	4
add_671	1599833760319107	12
gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul	1599833760221319	641
Mul_368	1599833759818779	4
truediv_8	1599833760322827	17
Mul_554	1599833759812298	12
mul_451	1599833760328803	4
bert/encoder/layer_3/intermediate/dense/bias/adam_v	-1	-1
bert/encoder/layer_7/output/LayerNorm/gamma/adam_v/read	-1	-1
mul_666	1599833760325368	6
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760138375	68
bert/embeddings/position_embeddings/adam_v/read	-1	-1
Mul_936	1599833759811082	4
Mul_313	1599833760277029	12
Mul_619	1599833759814012	4
bert/encoder/layer_1/output/dropout/random_uniform/RandomUniform	1599833759800399	46
Assign_518	1599833760312285	10
gradients/bert/encoder/layer_9/intermediate/dense/mul_3_grad/Mul	1599833760055694	280
mul_547	1599833760325894	7
Assign_145	1599833760296363	4
Mul_934	1599833760290485	13
truediv_47	1599833760322193	8
gradients/AddN_13	1599833760034899	365
sub_142	1599833760333331	7
bert/encoder/layer_5/output/LayerNorm/moments/variance	1599833759909677	32
bert/encoder/layer_10/attention/self/transpose_1	1599833759965831	189
mul_741	1599833760329032	8
Mul_995	1599833759820471	10
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2	1599833759848216	53
Mul_669	1599833759816876	17
add_194	1599833760293063	9
Assign_588	1599833760336988	4
global_norm/L2Loss_185	1599833760023575	14
gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul	1599833760092447	641
bert/encoder/layer_7/output/LayerNorm/beta/adam_m/read	-1	-1
bert/encoder/layer_7/attention/self/key/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_3/attention/self/transpose_grad/transpose	1599833760201236	189
add_374	1599833760304542	9
Assign_503	1599833760311945	23
add_339	1599833760282309	4
Sqrt_4	1599833760310694	4
bert/encoder/layer_9/attention/self/key/bias/adam_m	-1	-1
bert/encoder/layer_4/attention/self/query/kernel/adam_m/read	-1	-1
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760217592	52
Mul_683	1599833759816595	11
Sqrt_44	1599833760307746	4
Mul_81	1599833759809724	11
bert/encoder/layer_6/output/LayerNorm/beta/adam_v/read	-1	-1
add_240	1599833760325956	20
Square_63	1599833760280352	40
Assign_254	1599833760309112	14
bert/encoder/layer_3/attention/output/LayerNorm/moments/mean	1599833759874766	31
mul_37	1599833760331506	13
Assign_390	1599833760339844	10
bert/encoder/layer_2/attention/output/dropout/Cast	1599833759824254	36
gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul	1599833760247239	637
Mul_493	1599833760282777	4
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760138301	73
Assign_244	1599833760304165	41
gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul_1	1599833760199810	612
bert/encoder/layer_3/attention/output/dense/bias/adam_v	-1	-1
Assign_211	1599833760303973	12
bert/encoder/layer_0/output/dense/kernel/adam_m/read	-1	-1
truediv_28	1599833760321011	17
Mul_120	1599833759811297	13
bert/encoder/layer_2/intermediate/dense/bias/read	-1	-1
bert/encoder/layer_7/attention/self/value/kernel/adam_v	-1	-1
add_509	1599833760318629	12
Square_55	1599833760280297	13
Assign_9	1599833760336082	4
gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul	1599833760091134	637
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v	-1	-1
Sqrt_47	1599833760314119	46
add_332	1599833760282555	56
Sqrt_17	1599833760310747	41
truediv_107	1599833760321165	8
bert/encoder/layer_0/attention/output/dense/kernel/adam_m/read	-1	-1
sub_99	1599833760330638	7
bert/encoder/layer_0/attention/output/dense/bias/read	-1	-1
bert/encoder/layer_0/output/dropout/GreaterEqual	1599833759822365	39
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760018672	68
Mul_900	1599833759807286	11
add_59	1599833760284814	5
bert/encoder/layer_4/attention/self/key/kernel/adam_v/read	-1	-1
Assign_475	1599833760300624	4
cls/predictions/transform/LayerNorm/batchnorm/mul	1599833759990987	13
Mul_933	1599833759811076	4
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760151643	53
Mul_679	1599833760281407	4
mul_902	1599833760327862	4
Assign_159	1599833760340342	13
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760249349	65
bert/embeddings/ExpandDims/dim	-1	-1
Mul_1060	1599833760280009	4
Assign_379	1599833760295767	12
Mul_621	1599833759815713	4
Assign_452	1599833760311140	3
Mul_828	1599833759806531	18
bert/encoder/layer_6/output/dropout/GreaterEqual	1599833759822445	39
add_111	1599833760317947	43
Mul_708	1599833760295410	9
Mul_960	1599833760289873	39
Mul_997	1599833759805079	10
bert/encoder/layer_5/output/dense/bias/read	-1	-1
bert/encoder/layer_4/output/dense/bias	-1	-1
gradients/bert/encoder/layer_4/intermediate/dense/mul_3_grad/Mul_1	1599833759891367	195
add_143	1599833760283055	17
bert/encoder/layer_5/intermediate/dense/BiasAdd	1599833759904160	224
add_221	1599833760281643	5
Mul_593	1599833760280724	4
Mul_178	1599833759811658	10
Sqrt_11	1599833760310843	12
bert/encoder/layer_2/attention/self/MatMul	1599833759859118	160
gradients/bert/encoder/layer_9/attention/self/transpose_grad/transpose	1599833760068591	189
bert/encoder/layer_8/output/LayerNorm/moments/variance	1599833759949812	31
bert/encoder/layer_0/attention/self/query/bias/adam_m	-1	-1
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760063558	51
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2	1599833759888392	53
Mul_931	1599833759808811	11
add_660	1599833760291141	9
add_280	1599833760319918	43
Assign_527	1599833760313062	41
Assign_412	1599833760304983	20
Square_118	1599833760276060	4
Sqrt_59	1599833760314270	16
bert/encoder/layer_10/attention/self/key/bias/read	-1	-1
Square_70	1599833760280446	4
gradients/bert/encoder/layer_7/intermediate/dense/mul_2_grad/Mul_1	1599833760100259	280
gradients/bert/encoder/layer_4/attention/output/dropout/mul_1_grad/Mul	1599833760174320	52
Assign_2	1599833760310327	365
bert/encoder/layer_6/attention/self/Softmax	1599833759913070	321
add_104	1599833760296729	9
bert/encoder/layer_11/attention/self/query/kernel	-1	-1
Mul_422	1599833759819163	11
Mul_928	1599833760290181	17
add_623	1599833760302112	22
bert/encoder/layer_11/attention/self/Reshape_3	-1	-1
Mul_479	1599833759804993	11
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760028494	72
sub_72	1599833760337681	18
cls/seq_relationship/output_bias/read	-1	-1
bert/encoder/layer_1/attention/output/LayerNorm/moments/variance	1599833759848056	31
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Mul_557	1599833760282816	4
Mul_143	1599833760292133	17
bert/encoder/layer_10/output/dense/bias/adam_m	-1	-1
gradients/cls/seq_relationship/MatMul_grad/MatMul	1599833759990775	14
bert/encoder/layer_3/attention/self/value/kernel/adam_m	-1	-1
add_252	1599833760297244	21
bert/encoder/layer_5/attention/self/query/BiasAdd	1599833759898533	60
Mul_874	1599833760285009	39
Sqrt_40	1599833760309630	12
Mul_1110	1599833760291696	9
bert/encoder/layer_10/output/dense/kernel	-1	-1
clip_by_global_norm/mul_86	1599833760275562	12
bert/encoder/layer_11/intermediate/dense/kernel/adam_m	-1	-1
bert/encoder/layer_10/attention/self/Reshape	-1	-1
bert/encoder/layer_5/attention/self/key/kernel/adam_m	-1	-1
add_51	1599833760318516	4
bert/encoder/layer_9/attention/self/key/bias/adam_v/read	-1	-1
global_norm/L2Loss_51	1599833760204869	5
bert/encoder/layer_5/attention/self/dropout/random_uniform/mul	-1	-1
Mul_701	1599833759806060	39
global_norm/L2Loss_44	1599833760220067	5
gradients/AddN_77	1599833760239430	72
bert/encoder/layer_0/attention/self/ExpandDims/dim	-1	-1
bert/encoder/layer_5/intermediate/dense/add	1599833759905538	281
truediv_167	1599833760323418	5
global_norm/L2Loss_197	1599833759990933	14
bert/encoder/layer_1/output/LayerNorm/gamma/adam_v/read	-1	-1
add_394	1599833760296133	4
bert/encoder/layer_5/attention/self/MatMul	1599833759899292	161
Mul_339	1599833759819010	11
clip_by_global_norm/mul_78	1599833760273765	4
bert/encoder/layer_4/attention/self/value/kernel/read	-1	-1
bert/embeddings/assert_less_equal/x	-1	-1
bert/encoder/layer_9/output/dropout/random_uniform/RandomUniform	1599833759801024	46
bert/encoder/layer_0/attention/output/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_1/attention/self/dropout/random_uniform/RandomUniform	1599833759802125	83
bert/encoder/layer_0/output/dense/BiasAdd	1599833759842407	60
add_474	1599833760295482	21
bert/encoder/layer_6/intermediate/dense/kernel/adam_v	-1	-1
bert/encoder/layer_7/attention/self/query/kernel	-1	-1
bert/encoder/layer_1/output/LayerNorm/batchnorm/add_1	1599833759856383	73
mul_53	1599833760327384	4
bert/encoder/layer_3/attention/self/key/bias/adam_m/read	-1	-1
mul_241	1599833759815044	19
add_487	1599833760281650	56
bert/embeddings/assert_less_equal/y	-1	-1
Mul_345	1599833760280394	39
bert/encoder/layer_2/attention/self/key/bias/adam_m	-1	-1
Assign_382	1599833760296250	48
Mul_949	1599833760291289	45
Assign_57	1599833760336093	4
gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul	1599833760225131	637
bert/encoder/layer_8/output/LayerNorm/batchnorm/mul	1599833759949857	38
bert/encoder/layer_10/output/dense/bias	-1	-1
bert/encoder/layer_11/intermediate/dense/bias/adam_m/read	-1	-1
Mul_566	1599833760281069	4
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760005818	31
global_norm/L2Loss_85	1599833760160009	15
gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul	1599833760139458	2223
Mul_92	1599833759808318	11
Assign_563	1599833760312807	20
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760094068	51
bert/encoder/layer_8/attention/self/value/BiasAdd	1599833759938806	60
truediv_181	1599833760323804	5
add_38	1599833760327426	17
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2	1599833759915166	53
Sqrt_62	1599833760308531	10
bert/encoder/layer_6/attention/output/dense/kernel/adam_v/read	-1	-1
Mul_216	1599833760277323	11
bert/encoder/layer_3/output/dense/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_1/attention/self/dropout/mul_1_grad/Mul	1599833760242762	99
Mul_963	1599833759818638	12
gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul	1599833760006771	2227
gradients/bert/encoder/layer_9/intermediate/dense/mul_3_grad/Mul_1	1599833759958285	195
bert/encoder/layer_6/intermediate/dense/kernel/adam_m/read	-1	-1
Assign_178	1599833760303811	19
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/add	1599833759848089	5
bert/encoder/layer_7/attention/self/value/kernel/adam_m	-1	-1
bert/encoder/layer_8/attention/self/transpose_1	1599833759939058	189
add_141	1599833760298881	4
clip_by_global_norm/mul_167	1599833760274858	4
gradients/bert/encoder/layer_8/output/LayerNorm/moments/mean_grad/Tile	1599833760072399	28
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Reshape	-1	-1
Assign_508	1599833760300832	13
bert/encoder/layer_8/attention/self/query/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_5/attention/self/query/BiasAdd_grad/BiasAddGrad	1599833760157371	54
bert/encoder/layer_0/output/dropout/mul	1599833759826351	52
bert/encoder/layer_4/attention/self/MatMul	1599833759885914	161
gradients/bert/encoder/layer_3/output/LayerNorm/moments/mean_grad/truediv	1599833760183028	47
Assign_203	1599833760314670	11
Assign_42	1599833760335989	4
gradients/bert/encoder/layer_5/attention/self/dropout/mul_grad/Mul	1599833760154587	141
Assign_415	1599833760305964	4
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v/read	-1	-1
bert/encoder/layer_11/attention/output/add	1599833759981768	72
global_norm/L2Loss_3	1599833760271218	5
truediv_43	1599833760322173	8
bert/encoder/layer_7/intermediate/dense/bias/adam_m/read	-1	-1
Mul_412	1599833760293839	534
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1	1599833759968628	73
Assign_343	1599833760305982	12
bert/encoder/layer_1/attention/self/query/bias/adam_v	-1	-1
gradients/cls/predictions/LogSoftmax_grad/sub	1599833759996939	429
global_norm/L2Loss_5	1599833760270582	16
bert/encoder/layer_2/attention/self/query/bias/adam_v	-1	-1
Sqrt_199	1599833760313292	15
gradients/bert/encoder/layer_6/attention/self/dropout/mul_grad/Mul	1599833760132509	142
gradients/bert/encoder/layer_1/intermediate/dense/mul_3_grad/Mul	1599833760232622	278
Assign_372	1599833760339670	9
bert/encoder/layer_4/output/dense/kernel	-1	-1
Sqrt_133	1599833760315875	15
Mul_579	1599833760282009	4
gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1	1599833760112587	227
gradients/bert/encoder/layer_9/intermediate/dense/mul_2_grad/Mul_1	1599833760055975	279
gradients/bert/encoder/layer_3/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760202987	54
add_407	1599833760296170	22
Sqrt_51	1599833760314209	4
global_norm/L2Loss_145	1599833760078397	32
bert/encoder/layer_7/output/LayerNorm/batchnorm/add	1599833759936467	5
bert/encoder/layer_11/attention/self/key/kernel/adam_m/read	-1	-1
global_norm/L2Loss_92	1599833760153694	5
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul_1	1599833759940158	99
add_320	1599833760296957	9
Mul_631	1599833759813756	11
bert/encoder/layer_6/attention/self/query/kernel/adam_v	-1	-1
bert/encoder/layer_7/output/LayerNorm/beta	-1	-1
global_step/cond/Read/ReadVariableOp/Switch	-1	-1
add_553	1599833760300851	4
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
add_311	1599833760317546	8
truediv_4	1599833760323081	4
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760129793	27
bert/encoder/layer_7/attention/self/key/bias/adam_v	-1	-1
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub	1599833759848271	54
bert/encoder/layer_5/attention/self/Reshape	-1	-1
Assign_226	1599833760305112	20
gradients/cls/predictions/transform/dense/Pow_grad/sub	-1	-1
bert/encoder/layer_11/attention/self/transpose_2	1599833759979414	189
gradients/bert/encoder/layer_7/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760108204	55
add_396	1599833760294623	4
global_norm/L2Loss_196	1599833760006212	5
sub_180	1599833760332150	4
gradients/bert/encoder/layer_2/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760218748	55
bert/encoder/layer_6/output/LayerNorm/gamma/adam_m/read	-1	-1
add_590	1599833760301123	4
Assign_322	1599833760297099	20
Mul_792	1599833760281200	4
bert/encoder/layer_1/attention/self/value/bias/adam_v/read	-1	-1
bert/encoder/layer_3/output/dropout/random_uniform/RandomUniform	1599833759801071	46
Mul_869	1599833760290048	9
Mul_609	1599833760280735	4
bert/encoder/layer_0/attention/self/value/bias/read	-1	-1
bert/encoder/layer_10/attention/self/value/kernel/read	-1	-1
bert/encoder/layer_11/attention/self/dropout/random_uniform/RandomUniform	1599833759801955	83
Assign_409	1599833760304830	12
add_486	1599833760320531	4
add_469	1599833760320249	15
gradients/bert/encoder/layer_2/intermediate/dense/mul_2_grad/Mul_1	1599833760210808	280
mul_144	1599833759814565	18
Assign_271	1599833760298000	12
mul_553	1599833760330468	12
truediv_182	1599833760323499	17
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum	1599833760112018	54
bert/encoder/layer_3/intermediate/dense/bias/adam_m	-1	-1
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760226634	73
bert/encoder/layer_1/intermediate/dense/mul	1599833759851581	194
truediv_27	1599833760324099	7
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/encoder/layer_11/attention/self/Mul_grad/Mul	1599833760023691	142
Assign_567	1599833760336686	4
bert/encoder/layer_8/attention/output/LayerNorm/gamma	-1	-1
Sqrt_136	1599833760315218	8
Assign_331	1599833760304620	12
Square_169	1599833760278984	13
clip_by_global_norm/mul_202	1599833760275058	4
global_norm/L2Loss_42	1599833760222635	5
Square_42	1599833760277491	4
Mul_442	1599833760280519	39
sub_74	1599833760334488	19
global_norm/L2Loss_122	1599833760112074	5
clip_by_global_norm/mul_35	1599833760273890	4
global_norm/L2Loss_153	1599833760067843	15
gradients/bert/encoder/layer_4/output/dropout/mul_grad/Mul	1599833760161396	72
gradients/bert/encoder/layer_6/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760123785	109
bert/encoder/layer_11/attention/self/MatMul_1	1599833759980564	243
bert/encoder/layer_11/attention/self/value/bias/adam_v	-1	-1
bert/encoder/layer_3/attention/output/dense/BiasAdd	1599833759874556	60
gradients/bert/encoder/layer_5/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760152379	55
bert/encoder/layer_8/intermediate/dense/Tanh	1599833759946154	205
bert/encoder/layer_11/intermediate/dense/add_1	1599833759986530	195
Sqrt_189	1599833760312832	8
gradients/AddN_45	1599833760137962	119
bert/encoder/layer_2/attention/self/value/kernel	-1	-1
bert/encoder/layer_3/intermediate/dense/mul_1	1599833759879056	194
gradients/AddN_9	1599833760019472	95
add_23	1599833760318461	13
gradients/bert/encoder/layer_2/intermediate/dense/Tanh_grad/TanhGrad	1599833760211124	275
truediv_53	1599833760324633	5
Assign_139	1599833760298994	15
gradients/cls/predictions/Sum_1_grad/Const	-1	-1
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m/read	-1	-1
Mul_602	1599833759814225	46
bert/embeddings/dropout/random_uniform/mul	-1	-1
bert/encoder/layer_6/intermediate/dense/MatMul	1599833759915352	2195
bert/encoder/layer_6/output/dense/kernel/adam_m/read	-1	-1
bert/encoder/layer_1/attention/self/dropout/GreaterEqual	1599833759823724	72
add_256	1599833760303997	11
mul_719	1599833760329254	4
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Mul_1050	1599833760279696	4
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760138173	73
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760040467	69
Square_190	1599833760279850	4
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760062611	69
sub_152	1599833760336261	17
bert/encoder/layer_6/intermediate/dense/bias/read	-1	-1
bert/encoder/layer_5/attention/self/value/MatMul	1599833759897899	632
global_norm/L2Loss_135	1599833760093903	17
clip_by_global_norm/mul_46	1599833760273923	4
bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1	1599833759882997	73
bert/encoder/layer_3/attention/self/value/kernel/adam_v/read	-1	-1
bert/encoder/layer_2/attention/output/dense/kernel/read	-1	-1
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v	-1	-1
bert/encoder/layer_1/attention/self/key/BiasAdd	1599833759845015	60
global_norm/L2Loss_17	1599833760255291	32
bert/encoder/layer_9/output/LayerNorm/beta	-1	-1
truediv_67	1599833760324311	5
Sqrt_203	1599833760313351	10
add_241	1599833760292763	9
Assign_313	1599833760305502	10
Assign_550	1599833760302142	20
bert/encoder/layer_1/attention/output/dense/bias	-1	-1
bert/encoder/layer_7/output/LayerNorm/batchnorm/Rsqrt	1599833759936473	4
add_663	1599833760291545	12
global_norm/L2Loss_59	1599833760197932	12
add_216	1599833760317474	4
bert/encoder/layer_3/output/dense/bias	-1	-1
bert/encoder/layer_5/attention/self/value/kernel	-1	-1
bert/embeddings/LayerNorm/beta/adam_v/read	-1	-1
Mul_420	1599833759818859	11
add_222	1599833760295932	13
gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/mul_1	1599833760167295	279
mul_478	1599833760330579	16
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760261800	52
gradients/AddN_33	1599833760094996	95
gradients/bert/encoder/layer_7/attention/self/Mul_grad/Mul	1599833760112198	141
bert/encoder/layer_5/attention/output/LayerNorm/moments/mean	1599833759901532	32
bert/encoder/layer_8/attention/output/dropout/random_uniform/RandomUniform	1599833759800447	46
Assign_131	1599833760314086	21
truediv_193	1599833760323967	9
add_156	1599833760317148	4
sub_187	1599833760332048	8
Mul_1056	1599833759806793	4
Mul_173	1599833760277553	39
bert/encoder/layer_10/attention/self/query/BiasAdd	1599833759965452	61
bert/encoder/layer_6/output/dropout/mul_1	1599833759922823	73
gradients/cls/seq_relationship/LogSoftmax_grad/mul	1599833759990719	5
bert/encoder/layer_10/attention/self/query/kernel/adam_v/read	-1	-1
truediv_171	1599833760323424	4
Mul_1104	1599833760291965	9
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760138666	52
mul_859	1599833760327594	4
bert/encoder/layer_3/attention/self/value/kernel/read	-1	-1
bert/encoder/layer_3/attention/self/dropout/GreaterEqual	1599833759823127	74
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760217447	69
add_30	1599833760318381	12
truediv_81	1599833760321488	8
bert/encoder/layer_2/output/LayerNorm/beta/adam_v	-1	-1
bert/encoder/layer_4/output/dense/bias/adam_v/read	-1	-1
global_norm/L2Loss_100	1599833760138897	5
bert/encoder/layer_2/output/LayerNorm/gamma/read	-1	-1
gradients/bert/encoder/layer_7/intermediate/dense/mul_1_grad/Mul_1	1599833760100851	195
bert/encoder/layer_0/attention/output/dense/MatMul	1599833759833712	633
sub_170	1599833760336238	17
Sqrt_164	1599833760312499	8
truediv_18	1599833760322852	58
bert/encoder/layer_6/attention/self/MatMul	1599833759912681	157
Mul_160	1599833760282125	4
gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1	1599833760223146	228
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760240185	32
gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul	1599833760201675	639
bert/encoder/layer_5/output/dense/MatMul	1599833759906902	2472
Assign_107	1599833760309617	3
bert/encoder/layer_6/attention/self/key/bias/adam_v/read	-1	-1
bert/encoder/layer_7/attention/self/Reshape_2	-1	-1
gradients/bert/encoder/layer_8/output/LayerNorm/moments/variance_grad/truediv	1599833760072618	52
Assign_551	1599833760312756	19
global_norm/L2Loss_88	1599833760160026	5
Mul_163	1599833760276558	4
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
Mul_560	1599833760280757	12
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760227246	66
bert/encoder/layer_4/output/LayerNorm/batchnorm/add	1599833759896320	4
bert/encoder/layer_6/attention/output/dense/bias	-1	-1
gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/mul_1	1599833760078903	280
add_138	1599833760317760	15
add_567	1599833760318818	4
Assign_97	1599833760299152	11
truediv_113	1599833760324544	9
cls/predictions/transform/LayerNorm/beta/read	-1	-1
Assign_158	1599833760315196	11
bert/encoder/layer_9/intermediate/dense/kernel/adam_v/read	-1	-1
clip_by_global_norm/mul_114	1599833760275824	39
gradients/cls/predictions/truediv_grad/RealDiv	1599833759823948	4
Mul_841	1599833759805451	19
global_norm/L2Loss_203	1599833760004523	13
truediv_102	1599833760321659	17
Mul_373	1599833759819418	4
gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul	1599833760044334	638
gradients/bert/encoder/layer_10/attention/self/transpose_3_grad/transpose	1599833760043103	189
add_463	1599833760325428	21
bert/encoder/layer_2/output/add	1599833759869352	73
bert/encoder/layer_9/attention/self/value/BiasAdd	1599833759952185	60
Mul_807	1599833760278848	13
add_202	1599833760298310	4
bert/encoder/layer_8/output/dense/kernel/adam_m/read	-1	-1
add_318	1599833760316942	4
add_568	1599833760290130	21
Square_50	1599833760277376	4
add_521	1599833760290597	22
bert/embeddings/word_embeddings	-1	-1
add_387	1599833760305658	55
Assign_308	1599833760308901	12
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum	1599833760006063	32
Sqrt_170	1599833760311775	4
add_446	1599833760320704	8
mul_612	1599833760328930	7
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760072186	31
bert/encoder/layer_0/attention/self/query/kernel/adam_m	-1	-1
mul_520	1599833759813548	39
Assign_317	1599833760316249	14
Sqrt_41	1599833760309359	18
Mul_115	1599833759810507	4
gradients/cls/seq_relationship/LogSoftmax_grad/Sum	1599833759822038	6
bert/encoder/layer_6/attention/self/key/kernel/adam_v	-1	-1
mul_832	1599833759815814	18
bert/encoder/layer_6/attention/self/dropout/GreaterEqual	1599833759823278	71
Assign_113	1599833760307732	12
bert/encoder/layer_0/intermediate/dense/mul	1599833759838178	194
Mul_406	1599833759818829	11
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760196262	73
Mul_597	1599833759813440	11
Assign_229	1599833760305478	12
Sqrt_58	1599833760309040	9
Sqrt_185	1599833760312165	16
bert/encoder/layer_8/attention/self/query/kernel/adam_m	-1	-1
add_326	1599833760298812	56
mul_790	1599833760333353	42
Mul_49	1599833759810448	4
bert/encoder/layer_5/attention/output/LayerNorm/gamma	-1	-1
Assign_602	1599833760313611	11
bert/encoder/layer_4/attention/self/transpose_2	1599833759885724	189
global_norm/L2Loss_96	1599833760150961	9
Assign_580	1599833760301892	47
add_149	1599833760317905	10
global_norm/L2Loss_199	1599833760005316	19
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/mean_grad/Tile	1599833760019017	27
sub_67	1599833760332866	4
sub_184	1599833760336649	17
Square_193	1599833760279542	39
bert/encoder/layer_1/output/dense/bias	-1	-1
Sqrt_54	1599833760309017	4
Assign_307	1599833760298141	11
Assign_600	1599833760336993	4
add_177	1599833760292543	4
bert/encoder/layer_5/output/dense/kernel/adam_m/read	-1	-1
gradients/bert/embeddings/Reshape_1_grad/Reshape	-1	-1
bert/encoder/layer_1/attention/self/transpose_2	1599833759845522	190
Assign_151	1599833760298736	16
Sqrt_65	1599833760307446	45
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760262060	32
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760004687	9
bert/encoder/layer_6/attention/self/dropout/Cast	1599833759825173	68
bert/encoder/layer_8/attention/self/query/bias/read	-1	-1
bert/encoder/layer_10/attention/self/query/kernel/adam_v	-1	-1
bert/encoder/layer_9/output/LayerNorm/batchnorm/add	1599833759963241	5
add_605	1599833760327805	55
Assign_323	1599833760307932	20
bert/encoder/layer_4/attention/self/key/kernel/adam_m/read	-1	-1
bert/encoder/layer_1/intermediate/dense/Pow	1599833759850818	368
bert/encoder/layer_4/output/dropout/Cast	1599833759824482	36
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760240537	73
bert/encoder/layer_7/attention/output/dense/kernel/adam_v/read	-1	-1
Mul_417	1599833759820146	10
Assign_458	1599833760312043	11
bert/encoder/layer_7/output/dense/bias/adam_m	-1	-1
add_552	1599833760289601	22
add_195	1599833760304253	13
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul_1	1599833759966929	100
add_485	1599833760305721	4
bert/encoder/layer_4/intermediate/dense/MatMul	1599833759888577	2195
Square_155	1599833760278814	11
Assign_553	1599833760302559	16
gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/mul	1599833759932100	195
bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference	1599833759909620	55
cls/seq_relationship/Sum/reduction_indices	-1	-1
mul_950	1599833759817263	45
add_699/y	-1	-1
bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1	1599833759856196	73
Assign_258	1599833760334553	10
Assign_99	1599833760340166	41
Mul_867	1599833760279257	5
gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/mul_1	1599833760211597	280
bert/encoder/layer_1/attention/output/dropout/Cast	1599833759824444	36
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/add_1	1599833759901892	73
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v/read	-1	-1
bert/encoder/layer_4/intermediate/dense/mul_2	1599833759893035	194
global_norm/L2Loss_38	1599833760226393	5
Assign_356	1599833760306496	17
gradients/bert/encoder/layer_1/output/dropout/mul_1_grad/Mul	1599833760227737	51
Assign_12	1599833760335837	4
Mul_217	1599833759811734	17
Square_75	1599833760280893	12
gradients/bert/encoder/layer_6/output/dense/BiasAdd_grad/BiasAddGrad	1599833760117312	55
Assign_526	1599833760302485	40
Assign_94	1599833760299084	41
bert/encoder/layer_2/attention/self/query/bias/adam_m/read	-1	-1
Assign_5	1599833760313703	10
add_600	1599833760302900	9
clip_by_global_norm/mul_91	1599833760273790	4
gradients/bert/encoder/layer_0/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760256454	110
Assign_564	1599833760336775	4
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
bert/encoder/layer_6/attention/self/Reshape_3	-1	-1
bert/embeddings/dropout/GreaterEqual	1599833759823011	40
Mul_583	1599833759813353	19
PolynomialDecay/Cast_2/ReadVariableOp	-1	-1
bert/encoder/layer_1/intermediate/dense/mul_3	1599833759853052	280
Assign_515	1599833760311997	20
bert/encoder/layer_4/attention/self/dropout/random_uniform	-1	-1
Mul_149	1599833760292333	10
Assign_497	1599833760311402	13
gradients/bert/encoder/layer_0/output/LayerNorm/moments/variance_grad/Tile	1599833760249478	27
gradients/bert/encoder/layer_2/attention/self/Mul_grad/Mul	1599833760222758	141
bert/encoder/layer_7/attention/output/dropout/mul	1599833759826721	52
bert/encoder/layer_4/intermediate/dense/BiasAdd	1599833759890773	226
Mul_654	1599833760294444	9
gradients/bert/encoder/layer_7/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760110709	54
mul_758	1599833760333747	16
bert/encoder/layer_6/attention/self/value/BiasAdd	1599833759912046	60
Mul_1025	1599833760291116	9
sub_58	1599833760334720	17
bert/encoder/layer_4/attention/self/key/kernel	-1	-1
Assign_334	1599833760306349	51
Assign_330	1599833760337635	10
truediv_194	1599833760323593	57
bert/encoder/layer_4/attention/self/query/bias/adam_m	-1	-1
bert/encoder/layer_0/attention/self/query/kernel	-1	-1
Assign_366	1599833760333836	11
bert/encoder/layer_10/attention/output/LayerNorm/moments/mean	1599833759968454	31
edge_1734_cls/seq_relationship/Reshape@@MemcpyHtoD	1599833759821157	31
bert/encoder/layer_0/intermediate/dense/kernel/adam_m/read	-1	-1
Mul_336	1599833759818434	11
gradients/bert/encoder/layer_4/output/LayerNorm/moments/variance_grad/Tile	1599833760160984	26
mul_660	1599833759816007	18
Assign_388	1599833760305821	51
Mul_624	1599833759815988	4
clip_by_global_norm/mul_63	1599833760273759	4
add_274	1599833760328847	58
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum	1599833760040871	31
bert/encoder/layer_2/attention/self/Reshape_2	-1	-1
Assign_115	1599833760298051	13
truediv_176	1599833760323745	58
bert/encoder/layer_9/attention/self/query/bias/adam_v	-1	-1
sub_102	1599833760334684	17
Assign_537	1599833760336644	4
add_683	1599833760290937	4
Mul_954	1599833759817510	11
global_norm/L2Loss_13	1599833760262053	5
gradients/cls/predictions/LogSoftmax_grad/Exp	1599833759995736	305
gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul	1599833760157427	639
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m	-1	-1
gradients/bert/encoder/layer_6/attention/self/Reshape_grad/Reshape	-1	-1
gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul	1599833760240897	641
Sqrt_67	1599833760314654	5
Sqrt_29	1599833760307586	9
add_452	1599833760320391	4
bert/encoder/layer_2/intermediate/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_6/intermediate/dense/kernel/read	-1	-1
bert/encoder/layer_1/attention/self/query/bias	-1	-1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760040612	51
add_329	1599833760281979	5
gradients/bert/encoder/layer_11/intermediate/dense/BiasAdd_grad/BiasAddGrad	1599833760013201	109
Mul_837	1599833760284859	4
Assign_519	1599833760336411	4
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/add	1599833759874889	5
clip_by_global_norm/mul_155	1599833760274634	4
gradients/AddN_67	1599833760204909	68
gradients/AddN_46	1599833760138539	68
bert/encoder/layer_6/attention/output/dropout/random_uniform	-1	-1
Assign_555	1599833760341330	13
bert/encoder/layer_3/attention/output/add	1599833759874692	73
bert/encoder/layer_1/intermediate/dense/Tanh	1599833759852451	206
Assign_242	1599833760308122	11
Mul_439	1599833760282435	4
Assign_238	1599833760304420	40
mul_1042	1599833760328385	7
bert/encoder/layer_5/output/dropout/GreaterEqual	1599833759822245	39
add_35	1599833760284600	17
Assign_413	1599833760315376	23
Assign_461	1599833760312380	20
bert/encoder/layer_3/attention/self/Reshape_2	-1	-1
add_437	1599833760293790	10
Mul_332	1599833760281991	5
bert/embeddings/LayerNorm/beta/adam_v	-1	-1
gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul_1	1599833760126397	2466
mul_500	1599833760330776	16
add_579	1599833760289988	4
bert/encoder/layer_2/attention/self/add	1599833759859380	129
Square_52	1599833760280811	4
mul_107	1599833760327538	4
mul_328	1599833760332805	13
Mul_213	1599833760282788	4
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Mul_1	1599833760005757	53
Assign_292	1599833760297615	40
Mul_743	1599833760280855	13
bert/encoder/layer_9/output/LayerNorm/batchnorm/add_1	1599833759963479	73
Mul_82	1599833760278596	4
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760107582	65
bert/encoder/layer_2/output/dense/kernel/read	-1	-1
mul_203	1599833760326763	4
bert/embeddings/MatMul	1599833759812191	49
bert/encoder/layer_0/output/LayerNorm/gamma/adam_v	-1	-1
gradients/bert/encoder/layer_7/intermediate/dense/mul_3_grad/Mul	1599833760099978	280
sub_143	1599833760333536	4
gradients/bert/encoder/layer_2/attention/self/Reshape_3_grad/Reshape	-1	-1
Sqrt_7	1599833760310705	15
Square_168	1599833760279409	4
Square_93	1599833760276688	4
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
global_norm/L2Loss_159	1599833760062578	32
add_198	1599833760297725	18
sub_19	1599833760331610	4
add_369	1599833760325915	20
gradients/cls/predictions/mul_grad/Mul	1599833759828726	433
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_7/attention/self/Mul	1599833759926224	99
truediv_137	1599833760324652	9
bert/encoder/layer_10/intermediate/dense/bias/adam_v	-1	-1
Assign_243	1599833760341733	41
bert/encoder/layer_7/output/dense/bias/read	-1	-1
gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/Pow	1599833759958481	195
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_5/output/LayerNorm/batchnorm/sub	1599833759909892	55
gradients/AddN_20	1599833760057044	364
Sqrt_56	1599833760314696	8
bert/encoder/layer_11/attention/output/dense/kernel/adam_v/read	-1	-1
gradients/bert/encoder/layer_5/intermediate/dense/mul_2_grad/Mul_1	1599833760144442	281
bert/encoder/layer_6/intermediate/dense/bias/adam_m/read	-1	-1
bert/encoder/layer_7/output/add	1599833759936271	73
bert/embeddings/Slice/begin	-1	-1
mul_929	1599833759816202	19
add_661	1599833760302293	13
add_190	1599833760292562	17
bert/encoder/layer_8/intermediate/dense/kernel/adam_v	-1	-1
mul_156	1599833760329859	13
gradients/cls/predictions/transform/dense/BiasAdd_grad/BiasAddGrad	1599833760004927	11
mul_209	1599833760330162	13
bert/encoder/layer_7/attention/self/add	1599833759926324	129
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/variance_grad/truediv	1599833760218286	51
sub_118	1599833760339783	18
truediv_98	1599833760321596	61
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760204979	55
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760006360	52
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760248864	74
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
bert/encoder/layer_10/attention/output/dense/kernel/adam_v	-1	-1
sub_13	1599833760331535	4
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760195408	51
Assign_291	1599833760340285	41
Square_92	1599833760276404	4
Sqrt_28	1599833760307545	5
bert/encoder/layer_7/output/dense/MatMul	1599833759933659	2473
Assign_582	1599833760336691	4
bert/encoder/layer_11/attention/self/query/kernel/read	-1	-1
Assign_484	1599833760300680	41
bert/encoder/layer_9/attention/self/key/kernel/adam_m	-1	-1
clip_by_global_norm/mul_29	1599833760273504	4
bert/encoder/layer_9/attention/self/value/bias/adam_m/read	-1	-1
Sqrt_24	1599833760307535	4
add_73	1599833760303268	9
Assign_237	1599833760341790	41
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Mul_1	1599833760205036	53
Assign_261	1599833760340372	14
bert/embeddings/position_embeddings/adam_m/read	-1	-1
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760139047	51
Assign_268	1599833760297477	23
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760019080	65
sub_15	1599833760331541	5
bert/encoder/layer_10/attention/self/value/bias/adam_v	-1	-1
mul_461	1599833760326440	4
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760063422	26
Assign_33	1599833760340983	13
Mul_845	1599833759805394	11
bert/encoder/layer_2/attention/self/query/MatMul	1599833759856457	635
Assign_165	1599833760341660	13
global_norm/L2Loss_12	1599833760264269	5
bert/encoder/layer_2/attention/output/LayerNorm/beta/read	-1	-1
gradients/AddN_55	1599833760167576	364
Square_14	1599833760278590	4
Assign_272	1599833760308743	12
sub_147	1599833760333541	4
Mul_733	1599833759805751	17
gradients/AddN_30	1599833760085831	95
bert/encoder/layer_4/attention/output/dense/kernel/adam_m	-1	-1
Mul_459	1599833759820444	11
gradients/bert/encoder/layer_3/attention/output/dense/BiasAdd_grad/BiasAddGrad	1599833760196562	55
Sqrt_100	1599833760307862	9
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760227512	51
clip_by_global_norm/mul_99	1599833760273801	4
Assign_381	1599833760340082	40
Mul_923	1599833760289983	4
bert/encoder/layer_3/intermediate/dense/kernel/adam_m	-1	-1
Assign_149	1599833760308975	40
global_norm/L2Loss_150	1599833760071612	5
bert/encoder/layer_0/output/dense/bias/adam_v	-1	-1
gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul_1	1599833760197264	660
add_69	1599833760284826	4
bert/encoder/layer_2/attention/output/LayerNorm/moments/mean	1599833759861365	31
add_344	1599833760317188	9
truediv_104	1599833760324587	21
bert/encoder/layer_8/attention/self/key/MatMul	1599833759937414	630
add_459	1599833760320724	8
clip_by_global_norm/mul_68	1599833760275551	4
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760218129	66
global_norm/L2Loss_139	1599833760087380	12
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul	1599833760176814	142
bert/encoder/layer_0/intermediate/dense/mul_1/x	-1	-1
gradients/AddN_16	1599833760041543	95
Assign_547	1599833760301759	12
cls/predictions/transform/LayerNorm/gamma/adam_m	-1	-1
add_308	1599833760326060	20
bert/encoder/layer_4/intermediate/dense/Pow	1599833759891000	366
add_371	1599833760304491	9
bert/encoder/layer_1/attention/self/query/kernel/read	-1	-1
bert/encoder/layer_0/output/LayerNorm/gamma/read	-1	-1
Mul_334	1599833759818201	10
gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul_1	1599833760069677	613
Assign_373	1599833760305764	12
add_658	1599833760319062	43
Mul_202	1599833760283431	4
gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760270890	73
bert/encoder/layer_2/intermediate/dense/bias/adam_v	-1	-1
bert/pooler/dense/Tanh	1599833759990599	5
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/sub	1599833760023591	99
Assign_53	1599833760310790	41
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v	-1	-1
Mul_1073	1599833760291217	9
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760005558	51
Sqrt_150	1599833760311134	4
bert/encoder/layer_10/attention/self/query/bias/read	-1	-1
bert/encoder/layer_6/attention/self/transpose_3	1599833759913882	190
Square_144	1599833760280849	4
gradients/cls/predictions/Sum_1_grad/Tile	1599833759828397	5
mul_510	1599833760325873	4
Sqrt_184	1599833760313115	5
bert/encoder/layer_7/attention/self/value/kernel	-1	-1
add_193	1599833760328623	16
cls/predictions/add/y	-1	-1
Square_110	1599833760280752	4
add_47	1599833760300340	4
add_182	1599833760320172	7
bert/encoder/layer_8/attention/self/value/bias/adam_v/read	-1	-1
bert/encoder/layer_4/attention/self/key/kernel/read	-1	-1
sub_122	1599833760339803	21
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/sub	1599833760067859	99
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760196209	51
gradients/bert/encoder/layer_8/intermediate/dense/mul_2_grad/Mul_1	1599833760078116	279
mul_1021	1599833760328240	4
bert/encoder/layer_1/output/dense/kernel/adam_v	-1	-1
add_83	1599833760303471	21
Mul_35	1599833760284551	12
add_161	1599833760298972	9
bert/embeddings/LayerNorm/gamma/adam_v	-1	-1
add_613	1599833760302527	4
Assign_10	1599833760300529	4
bert/encoder/layer_0/attention/self/Reshape_3/shape	-1	-1
Mul_635	1599833760281353	13
Assign_350	1599833760315711	11
truediv_198	1599833760323651	18
bert/encoder/layer_2/attention/self/key/bias/adam_m/read	-1	-1
bert/encoder/layer_2/intermediate/dense/mul_3	1599833759866457	279
add_500	1599833760320536	4
bert/encoder/layer_7/attention/self/key/BiasAdd	1599833759925366	61
bert/encoder/layer_5/output/LayerNorm/beta	-1	-1
bert/encoder/layer_9/output/LayerNorm/gamma/adam_m/read	-1	-1
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
Mul_975	1599833759819755	4
bert/encoder/layer_2/attention/self/key/kernel/read	-1	-1
bert/encoder/layer_10/intermediate/dense/Tanh	1599833759972938	208
Mul_476	1599833760282745	12
Assign_442	1599833760306459	4
Sqrt_117	1599833760316334	15
bert/encoder/layer_10/output/LayerNorm/batchnorm/add	1599833759976632	5
Mul_605	1599833760295592	39
bert/encoder/layer_5/attention/self/key/bias/adam_m	-1	-1
bert/encoder/layer_5/attention/self/query/kernel/adam_v	-1	-1
Assign_29	1599833760310915	13
add_698	1599833760319440	9
truediv_115	1599833760325159	5
Mul_409	1599833759819130	18
bert/encoder/layer_10/attention/output/dense/kernel	-1	-1
bert/encoder/layer_3/intermediate/dense/mul_3	1599833759879854	280
add_112	1599833760326700	61
bert/encoder/layer_6/attention/output/add	1599833759914842	72
global_norm/L2Loss_181	1599833760027328	15
bert/encoder/layer_6/attention/output/LayerNorm/gamma	-1	-1
global_norm/L2Loss_114	1599833760122060	5
mul_70	1599833760331492	12
truediv_35	1599833760321989	8
Mul_517	1599833760277387	40
gradients/bert/encoder/layer_3/attention/self/Mul_grad/Mul	1599833760200603	142
Sqrt_91	1599833760309191	16
bert/encoder/layer_4/intermediate/dense/kernel/adam_v/read	-1	-1
bert/encoder/layer_3/attention/output/dense/bias/adam_m	-1	-1
bert/encoder/layer_4/attention/self/value/kernel/adam_m/read	-1	-1
bert/encoder/layer_11/output/LayerNorm/batchnorm/add	1599833759990010	5
cls/predictions/transform/dense/Tanh	1599833759990661	10
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v/read	-1	-1
Assign_439	1599833760305726	4
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760173658	54
gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul_1	1599833760203685	612
bert/encoder/layer_4/output/LayerNorm/batchnorm/mul	1599833759896332	39
bert/encoder/layer_4/attention/output/dense/kernel/adam_v	-1	-1
bert/encoder/layer_4/attention/output/add	1599833759888068	72
bert/embeddings/LayerNorm/moments/SquaredDifference	1599833759828404	60
bert/encoder/layer_5/intermediate/dense/Pow	1599833759904386	365
mul_102	1599833760331445	40
Mul_73	1599833759811011	4
bert/encoder/layer_8/attention/output/dense/bias/adam_v	-1	-1
gradients/bert/encoder/layer_10/attention/output/dropout/mul_grad/Mul	1599833760041692	73
gradients/bert/encoder/layer_8/attention/self/Reshape_2_grad/Reshape	-1	-1
truediv_152	1599833760323272	17
mul_456	1599833760326243	8
sub_129	1599833760333271	8
add_210	1599833760316970	8
bert/encoder/layer_10/attention/output/dense/bias/adam_m/read	-1	-1
Mul_534	1599833760277130	4
bert/encoder/layer_2/intermediate/dense/Tanh	1599833759865857	205
Assign_353	1599833760316354	19
bert/encoder/layer_1/attention/self/Reshape_3	-1	-1
bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference	1599833759901566	54
bert/encoder/layer_1/attention/self/value/bias/adam_m/read	-1	-1
bert/encoder/layer_9/attention/self/Reshape_1	-1	-1
Mul_226	1599833759810581	12
truediv_155	1599833760323111	4
Mul_1000	1599833759820023	18
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1	1599833760195945	65
Mul_675	1599833759815893	4
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape	-1	-1
Sqrt_109	1599833760314932	8
Assign_365	1599833760316395	19
gradients/AddN_88	1599833760271258	68
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760240429	53
Mul_766	1599833759806187	11
add_230	1599833760319859	4
bert/encoder/layer_6/attention/self/key/kernel/adam_m/read	-1	-1
bert/encoder/layer_11/intermediate/dense/kernel/adam_v	-1	-1
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1	1599833760085757	72
Assign_603	1599833760336638	4
sub_22	1599833760337127	23
sub_173	1599833760331966	7
Assign_583	1599833760302269	12
Mul_471	1599833760282459	4
edge_1725_bert/encoder/Reshape@@MemcpyHtoD	1599833759797816	57
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul	1599833760128984	69
Assign_73	1599833760296676	4
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760271773	51
clip_by_global_norm/mul_127	1599833760273272	4
bert/encoder/layer_10/attention/self/Reshape_2	-1	-1
truediv_68	1599833760324416	4
gradients/bert/encoder/layer_9/attention/self/key/BiasAdd_grad/BiasAddGrad	1599833760070295	54
Square_90	1599833760277103	4
bert/encoder/layer_3/output/LayerNorm/batchnorm/add_1	1599833759883181	73
global_norm/L2Loss_204	1599833759990847	6
Mul_151	1599833759809302	18
add_188	1599833760298299	4
Square_130	1599833760281423	4
bert/encoder/layer_9/attention/self/key/bias/adam_v	-1	-1
clip_by_global_norm/mul_33	1599833760274044	4
sub_185	1599833760332335	4
Assign_506	1599833760312241	11
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance_grad/Tile	1599833760262465	27
gradients/bert/encoder/layer_0/attention/self/value/BiasAdd_grad/BiasAddGrad	1599833760265454	54
bert/encoder/layer_4/output/LayerNorm/gamma/adam_m	-1	-1
clip_by_global_norm/mul_12	1599833760274485	12
gradients/bert/encoder/layer_2/attention/self/transpose_3_grad/transpose	1599833760220088	189
Mul_853	1599833760289747	9
bert/encoder/layer_7/attention/self/query/bias/adam_v	-1	-1
add_588	1599833760318944	8
global_norm/L2Loss_55	1599833760204449	16
Assign_534	1599833760336469	4
Mul_260	1599833759818046	39
add_126	1599833760283437	4
bert/encoder/layer_10/intermediate/dense/MatMul	1599833759968888	2194
Mul_889	1599833759808508	11
add_94	1599833760303526	9
Assign_405	1599833760339979	13
Square_126	1599833760276082	4
bert/encoder/layer_7/intermediate/dense/bias/adam_v/read	-1	-1
truediv_70	1599833760321445	21
gradients/bert/encoder/layer_7/intermediate/dense/Tanh_grad/TanhGrad	1599833760100575	275
Square_36	1599833760277594	4
add_377	1599833760304647	9
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Sum	1599833760116206	74
Assign_502	1599833760301218	20
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760018470	72
bert/encoder/layer_10/intermediate/dense/mul_1	1599833759972743	194
add_654	1599833760303241	9
global_norm/L2Loss_23	1599833760248646	17
bert/encoder/layer_10/attention/self/add	1599833759966476	127
Assign_513	1599833760341299	14
bert/encoder/layer_5/attention/self/key/bias/adam_m/read	-1	-1
gradients/bert/encoder/layer_8/attention/self/transpose_2_grad/transpose	1599833760088087	190
Mul_978	1599833759818519	18
bert/encoder/layer_7/attention/self/key/kernel/read	-1	-1
global_norm/L2Loss_177	1599833760034111	34
Mul_184	1599833760276604	39
gradients/bert/embeddings/dropout/mul_1_grad/Mul	1599833759827138	47
clip_by_global_norm/mul_128	1599833760273388	40
bert/encoder/layer_9/attention/output/dense/bias	-1	-1
bert/encoder/layer_3/output/LayerNorm/gamma/adam_v/read	-1	-1
mul_21	1599833760331547	8
add_514	1599833760289672	22
global_norm/L2Loss_134	1599833760093753	5
add_7	1599833760292011	9
Mul_475	1599833759804873	18
Assign_594	1599833760336696	4
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/add_1	1599833759968813	73
bert/encoder/layer_0/output/dense/kernel/adam_v	-1	-1
clip_by_global_norm/mul_17	1599833760274612	4
mul_31	1599833760327298	4
bert/encoder/layer_7/attention/output/dense/bias/adam_v/read	-1	-1
cls/seq_relationship/output_bias/adam_v/read	-1	-1
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
gradients/bert/encoder/layer_6/intermediate/dense/mul_3_grad/Mul_1	1599833759918141	195
bert/encoder/layer_10/attention/self/MatMul_1	1599833759967172	242
global_norm/L2Loss_138	1599833760089933	5
global_norm/Sum	1599833760273219	4
bert/encoder/layer_0/attention/self/key/kernel/adam_v	-1	-1
bert/encoder/layer_7/output/dense/kernel/adam_v/read	-1	-1
Square_59	1599833760280325	13
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Mul	1599833760005923	55
truediv_149	1599833760323099	4
bert/encoder/layer_10/attention/self/query/bias/adam_m/read	-1	-1
Square_9	1599833760278552	12
bert/encoder/layer_5/output/dropout/mul_1	1599833759909440	73
sub_14	1599833760331563	4
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Neg	1599833760173292	51
mul_886	1599833760327885	4
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v	-1	-1
Mul_23	1599833760278740	4
mul_295	1599833760330482	13
bert/encoder/layer_7/output/LayerNorm/gamma	-1	-1
bert/encoder/layer_10/output/dropout/random_uniform/mul	-1	-1
truediv_14	1599833760323009	4
Assign_471	1599833760336280	4
mul_924	1599833760327868	4
bert/encoder/layer_4/attention/self/transpose_3	1599833759887108	190
Mul_210	1599833759811708	10
Assign_175	1599833760298324	12
add_85	1599833760328494	18
bert/encoder/layer_4/attention/self/value/MatMul	1599833759884520	635
add_346	1599833760297672	17
bert/encoder/layer_0/output/LayerNorm/moments/mean	1599833759842618	32
gradients/bert/embeddings/Slice_grad/concat	-1	-1
add_403	1599833760281560	4
mul_532	1599833760330424	43
bert/encoder/layer_2/attention/self/transpose	1599833759858545	189
bert/encoder/layer_6/intermediate/dense/mul_3	1599833759920004	280
add_87	1599833760296670	4
gradients/bert/encoder/layer_11/attention/self/dropout/mul_grad/Mul	1599833760021907	141
add_294	1599833760328825	20
Mul_1017	1599833759814874	10
Assign_309	1599833760341832	13
Mul_836	1599833759807948	11
bert/encoder/layer_8/attention/self/value/kernel/read	-1	-1
bert/encoder/layer_1/attention/output/dense/kernel/adam_v/read	-1	-1
add_431	1599833760295796	9
sub_53	1599833760333173	7
bert/encoder/layer_7/attention/output/dense/kernel/adam_m	-1	-1
bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2	1599833759963368	53
add_539	1599833760300629	4
gradients/AddN_19	1599833760050710	95
PolynomialDecay/Minimum	1599833759820849	14
add_682	1599833760319531	4
bert/encoder/layer_3/attention/self/query/kernel/adam_m	-1	-1
Assign_509	1599833760311430	13
bert/encoder/layer_6/attention/output/LayerNorm/beta	-1	-1
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul_1	1599833760248737	73
clip_by_global_norm/mul_165	1599833760274997	4
Mul_1051	1599833759807437	11
bert/encoder/layer_8/intermediate/dense/mul_3	1599833759946754	279
Mul_350	1599833759818231	10
Mul_947	1599833760279770	39
bert/encoder/layer_7/attention/self/value/kernel/read	-1	-1
mul_273	1599833759815076	46
Assign_120	1599833760335197	4
add_232	1599833760304219	4
Assign_298	1599833760297009	12
Assign_1	1599833760299707	365
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/scalar	-1	-1
Mul_1028	1599833760279844	4
gradients/bert/encoder/layer_0/attention/self/transpose_3_grad/transpose	1599833760264290	189
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/Mul	1599833760183226	51
bert/encoder/layer_10/attention/self/key/bias/adam_m/read	-1	-1
bert/encoder/layer_2/attention/self/value/bias/adam_v	-1	-1
add_415	1599833760320665	15
add_471	1599833760295174	4
gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1	1599833760109887	242
Square_139	1599833760281454	11
bert/encoder/layer_0/attention/self/key/kernel	-1	-1
clip_by_global_norm/mul_180	1599833760275063	4
Assign_48	1599833760336088	4
truediv_183	1599833760323574	5
add_296	1599833760297401	9
add_589	1599833760289999	9
clip_by_global_norm/mul_36	1599833760273975	4
Mul_726	1599833759806959	11
truediv_55	1599833760321915	5
bert/encoder/layer_11/output/LayerNorm/beta/adam_m	-1	-1
bert/encoder/layer_3/attention/self/transpose_2	1599833759872333	189
bert/encoder/layer_4/output/LayerNorm/gamma/read	-1	-1
bert/encoder/layer_9/attention/self/query/bias	-1	-1
gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1	1599833760068349	240
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul	1599833760261929	68
add_445	1599833760306222	9
bert/encoder/layer_11/output/LayerNorm/batchnorm/sub	1599833759990192	54
bert/encoder/layer_0/intermediate/dense/mul_3	1599833759839651	280
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub	1599833759941975	54
Mul_974	1599833760279817	4
cls/seq_relationship/output_weights/adam_m/read	-1	-1
Mul_541	1599833760282304	4
sub_96	1599833760335129	61
bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1	1599833759896373	73
clip_by_global_norm/mul_189	1599833760275171	4
Mul_298	1599833759817625	4
mul_994	1599833760332163	16
bert/encoder/layer_5/attention/output/dense/BiasAdd	1599833759901321	60
Sqrt_144	1599833760315310	9
Assign_599	1599833760313317	21
bert/encoder/layer_11/output/dense/bias/adam_m/read	-1	-1
global_norm/L2Loss_170	1599833760045644	5
sub_34	1599833760334291	60
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/sub	1599833760138992	53
bert/encoder/layer_10/intermediate/dense/BiasAdd	1599833759971083	222
Assign_14	1599833760310700	3
bert/encoder/layer_8/output/LayerNorm/beta	-1	-1
Square_116	1599833760281042	4
add_226	1599833760317009	42
mul_392	1599833760333155	16
Assign_217	1599833760305423	16
mul_354	1599833760325451	7
