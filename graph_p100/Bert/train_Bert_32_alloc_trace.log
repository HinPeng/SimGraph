1608802723825811 bert/embeddings/one_hot:0 32768
1608802723825957 cls/predictions/one_hot:0 93769728
1608802723826282 cls/predictions/Sum_2:0 256
1608802723826324 bert/embeddings/dropout/random_uniform/RandomUniform:0 12582912
1608802723826534 gradients/cls/predictions/truediv_grad/RealDiv:0 256
1608802723826636 gradients/cls/predictions/Sum_1_grad/Tile:0 2560
1608802723826692 gradients/cls/predictions/truediv_grad/RealDiv:0 -256
1608802723826843 gradients/cls/predictions/Sum_grad/Tile:0 78136320
1608802723826909 gradients/cls/predictions/Sum_1_grad/Tile:0 -2560
1608802723826951 cls/seq_relationship/one_hot:0 256
1608802723826982 gradients/cls/predictions/LogSoftmax_grad/Sum:0 2560
1608802723826982 gradients/cls/predictions/LogSoftmax_grad/Sum:t0 256
1608802723827040 gradients/cls/seq_relationship/mul_grad/Mul_1:0 256
1608802723827101 gradients/cls/predictions/LogSoftmax_grad/Sum:t0 -256
1608802723827135 gradients/cls/seq_relationship/LogSoftmax_grad/Sum:0 256
1608802723829115 bert/encoder/layer_11/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829203 bert/encoder/layer_4/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829278 bert/encoder/layer_7/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829343 bert/encoder/layer_9/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829409 bert/encoder/layer_0/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829469 bert/encoder/layer_1/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829525 bert/encoder/layer_2/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829588 bert/encoder/layer_10/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829643 bert/encoder/layer_5/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829695 bert/encoder/layer_8/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829756 bert/encoder/layer_6/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723829813 bert/encoder/layer_3/attention/self/dropout/random_uniform/RandomUniform:0 25165824
1608802723831021 bert/encoder/layer_0/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831093 bert/encoder/layer_6/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831155 bert/encoder/layer_8/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831217 bert/encoder/layer_4/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831276 bert/encoder/layer_9/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831353 bert/encoder/layer_2/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831419 bert/encoder/layer_4/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831477 bert/encoder/layer_10/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831529 bert/encoder/layer_10/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831586 bert/encoder/layer_1/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831639 bert/encoder/layer_5/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831703 bert/encoder/layer_9/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831752 bert/encoder/layer_11/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831809 bert/encoder/layer_1/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831860 bert/encoder/layer_3/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831919 bert/encoder/layer_6/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723831975 bert/encoder/layer_11/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723832032 bert/encoder/layer_2/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723832085 bert/encoder/layer_3/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723832131 bert/encoder/layer_7/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723832189 bert/encoder/layer_0/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723832243 bert/encoder/layer_5/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723832297 bert/encoder/layer_7/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723832352 bert/encoder/layer_8/attention/output/dropout/random_uniform/RandomUniform:0 12582912
1608802723836668 Mul_497:0 2359296
1608802723836743 Cast:0 256
1608802723836891 PolynomialDecay/Cast_2:0 256
1608802723837079 mul_47:0 2359296
1608802723837150 Mul_559:0 2359296
1608802723837210 mul_20:0 1572864
1608802723837263 Mul_572:0 3145728
1608802723837321 mul_230:0 2359296
1608802723837376 mul_69:0 2359296
1608802723837429 mul_133:0 2359296
1608802723837486 Mul_642:0 3072
1608802723837535 mul_703:0 9437184
1608802723837584 mul_789:0 9437184
1608802723837642 mul_821:0 2359296
1608802723837687 mul_918:0 2359296
1608802723837739 mul_950:0 9437184
1608802723837795 Mul_694:0 12288
1608802723837840 Mul_705:0 3072
1608802723837885 Mul_722:0 2359296
1608802723837936 Mul_1101:0 8448
1608802723837981 Mul_759:0 3072
1608802723838028 Mul_726:0 3072
1608802723838070 Mul_715:0 3072
1608802723838115 Mul_1051:0 3072
1608802723838158 Mul_742:0 2359296
1608802723838201 Mul_761:0 3072
1608802723838250 Mul_845:0 3072
1608802723838297 mul_735:0 2359296
1608802723838342 Mul_678:0 3072
1608802723838387 Mul_1032:0 9437184
1608802723838430 Mul_1038:0 12288
1608802723838480 Mul_1034:0 9437184
1608802723838534 mul_1047:0 9437184
1608802723838580 Mul_764:0 3072
1608802723838628 Mul_806:0 2359296
1608802723838678 Mul_717:0 3072
1608802723838720 Mul_728:0 3072
1608802723838766 Mul_1045:0 9437184
1608802723838816 Mul_774:0 9437184
1608802723838869 Mul_1103:0 6144
1608802723838921 mul_896:0 2359296
1608802723838967 Mul_1061:0 3072
1608802723839008 Mul_847:0 3072
1608802723839055 Mul_680:0 3072
1608802723839099 Mul_683:0 3072
1608802723839138 Mul_776:0 9437184
1608802723839189 Mul_658:0 2359296
1608802723839239 Mul_662:0 3072
1608802723839280 Mul_707:0 3072
1608802723839339 mul_929:0 2359296
1608802723839386 Mul_675:0 3072
1608802723839437 Mul_744:0 2359296
1608802723839490 Mul_766:0 3072
1608802723839533 Mul_1054:0 3072
1608802723839582 Mul_808:0 2359296
1608802723839631 mul_864:0 9437184
1608802723839679 Mul_731:0 2359296
1608802723839732 Mul_769:0 3072
1608802723839779 Mul_780:0 12288
1608802723839822 Mul_793:0 3072
1608802723839868 mul_1068:0 2359296
1608802723839913 Mul_825:0 3072
1608802723839960 Mul_1064:0 2359296
1608802723840008 Mul_699:0 9437184
1608802723840052 Mul_710:0 3072
1608802723840098 Mul_1049:0 3072
1608802723840144 Mul_782:0 12288
1608802723840186 Mul_796:0 3072
1608802723840233 Mul_812:0 3072
1608802723840272 Mul_828:0 2359296
1608802723840323 mul_1105:0 6144
1608802723840386 Mul_701:0 9437184
1608802723840436 Mul_720:0 2359296
1608802723840487 Mul_733:0 2359296
1608802723840538 Mul_748:0 3072
1608802723840579 Mul_771:0 3072
1608802723840625 Mul_785:0 9437184
1608802723840691 Mul_1056:0 3072
1608802723840736 Mul_814:0 3072
1608802723840774 Mul_830:0 2359296
1608802723840825 Mul_850:0 3072
1608802723840882 Mul_696:0 12288
1608802723840920 Mul_712:0 3072
1608802723840961 Mul_737:0 3072
1608802723841007 Mul_750:0 3072
1608802723841046 Mul_787:0 9437184
1608802723841092 Mul_803:0 3072
1608802723841132 Mul_817:0 2359296
1608802723841177 Mul_834:0 3072
1608802723841219 Mul_852:0 3072
1608802723841259 mul_90:0 9437184
1608802723841305 Mul_1043:0 9437184
1608802723841352 Mul_753:0 2359296
1608802723841395 mul_327:0 2359296
1608802723841447 Mul_791:0 3072
1608802723841497 Mul_798:0 3072
1608802723841538 Mul_823:0 3072
1608802723841582 Mul_841:0 2359296
1608802723841622 Mul_855:0 3072
1608802723841668 mul_58:0 2359296
1608802723841717 mul_155:0 2359296
1608802723841761 mul_187:0 9437184
1608802723841809 Mul_739:0 3072
1608802723841852 Mul_755:0 2359296
1608802723841893 mul_262:0 9437184
1608802723841944 mul_294:0 2359296
1608802723841992 Mul_801:0 3072
1608802723842033 Mul_1059:0 3072
1608802723842073 Mul_836:0 3072
1608802723842113 Mul_1066:0 2359296
1608802723842159 Mul_548:0 2359296
1608802723842207 Mul_554:0 3072
1608802723842248 Mul_561:0 2359296
1608802723842296 Mul_508:0 3072
1608802723842342 mul_144:0 2359296
1608802723842385 mul_176:0 9437184
1608802723842437 mul_208:0 2359296
1608802723842485 Mul_581:0 2359296
1608802723842527 Mul_819:0 2359296
1608802723842573 Mul_839:0 2359296
1608802723842619 Mul_857:0 3072
1608802723842659 mul_391:0 2359296
1608802723842708 mul_14:0 6144
1608802723842752 bert/embeddings/MatMul:0 12582912
1608802723842843 Mul_556:0 3072
1608802723842893 Mul_1011:0 2359296
1608802723842938 Mul_565:0 3072
1608802723842983 Mul_576:0 3072
1608802723843026 Mul_583:0 2359296
1608802723843067 mul_273:0 9437184
1608802723843114 mul_359:0 9437184
1608802723843165 Mul_545:0 3072
1608802723843205 mul_520:0 9437184
1608802723843252 mul_36:0 2359296
1608802723843295 Mul_634:0 2359296
1608802723843515 Mul_567:0 3072
1608802723843564 mul_219:0 2359296
1608802723843609 mul_241:0 2359296
1608802723843654 mul_305:0 2359296
1608802723843700 Mul_599:0 3072
1608802723843739 Mul_1019:0 3072
1608802723843800 Mul_613:0 9437184
1608802723843847 Mul_626:0 3072
1608802723843895 mul_649:0 2359296
1608802723843948 mul_122:0 2359296
1608802723843998 Mul_1013:0 2359296
1608802723844067 mul_832:0 2359296
1608802723844110 Mul_578:0 3072
1608802723844157 Mul_602:0 9437184
1608802723844206 mul_434:0 9437184
1608802723844251 Mul_610:0 12288
1608802723844296 mul_488:0 2359296
1608802723844345 Mul_550:0 2359296
1608802723844402 Mul_631:0 3072
1608802723844451 Mul_1027:0 3072
1608802723844491 mul_101:0 9437184
1608802723844540 Mul_570:0 2359296
1608802723844589 Mul_664:0 3072
1608802723844629 mul_961:0 9437184
1608802723844675 mul_348:0 9437184
1608802723844725 Mul_1096:0 122112
1608802723844769 mul_402:0 2359296
1608802723844822 Mul_1022:0 3072
1608802723844866 mul_552:0 2359296
1608802723844909 mul_617:0 9437184
1608802723844955 Mul_636:0 2359296
1608802723845000 mul_692:0 9437184
1608802723845049 Mul_667:0 2359296
1608802723845097 mul_993:0 2359296
1608802723845139 mul_316:0 2359296
1608802723845189 Mul_608:0 12288
1608802723845233 mul_466:0 2359296
1608802723845278 Mul_615:0 9437184
1608802723845330 Mul_619:0 3072
1608802723845378 mul_585:0 2359296
1608802723845419 mul_660:0 2359296
1608802723845468 Mul_640:0 3072
1608802723845511 mul_724:0 2359296
1608802723845551 Mul_647:0 2359296
1608802723845601 Mul_1029:0 3072
1608802723845952 mul_843:0 2359296
1608802723846002 mul_875:0 9437184
1608802723846052 mul_907:0 2359296
1608802723846097 Mul_587:0 3072
1608802723846140 Mul_1017:0 3072
1608802723846181 Mul_604:0 9437184
1608802723846230 mul_445:0 9437184
1608802723846279 mul_499:0 2359296
1608802723846321 mul_531:0 9437184
1608802723846370 mul_563:0 2359296
1608802723846417 mul_746:0 2359296
1608802723846462 mul_778:0 9437184
1608802723846513 Mul_673:0 3072
1608802723846557 mul_1004:0 2359296
1608802723846598 Mul_592:0 3072
1608802723846642 Mul_597:0 3072
1608802723846679 mul_380:0 2359296
1608802723846728 mul_413:0 2359296
1608802723846777 Mul_1040:0 12288
1608802723846820 mul_477:0 2359296
1608802723846870 Mul_645:0 2359296
1608802723846917 Mul_506:0 3072
1608802723846961 mul_810:0 2359296
1608802723847008 Mul_4:0 93763584
1608802723847064 Mul_669:0 2359296
1608802723847123 mul_982:0 2359296
1608802723847164 Mul_594:0 3072
1608802723847206 Mul_191:0 3072
1608802723847245 Mul_1083:0 3072
1608802723847280 Mul_621:0 3072
1608802723847315 Mul_1024:0 3072
1608802723847358 mul_638:0 2359296
1608802723847404 mul_671:0 2359296
1608802723847483 Mul_860:0 9437184
1608802723847548 Mul_1098:0 122112
1608802723847600 mul_8:0 93763584
1608802723847649 bert/embeddings/GatherV2:0 12582912
1608802723847715 Mul_651:0 3072
1608802723847772 Mul_1077:0 2359296
1608802723847818 Mul_51:0 3072
1608802723847867 Mul_73:0 3072
1608802723847910 mul_1015:0 2359296
1608802723847951 Mul_685:0 3072
1608802723847996 Mul_688:0 9437184
1608802723848041 Mul_589:0 3072
1608802723848084 Mul_1109:0 256
1608802723848125 Mul_223:0 3072
1608802723848166 Mul_624:0 3072
1608802723848211 mul_606:0 9437184
1608802723848258 Mul_862:0 9437184
1608802723848304 Mul_879:0 3072
1608802723848351 Mul_898:0 3072
1608802723848406 mul_757:0 2359296
1608802723848448 Mul_54:0 2359296
1608802723848495 Mul_920:0 3072
1608802723848540 Mul_88:0 9437184
1608802723848583 Mul_927:0 2359296
1608802723848631 mul_1036:0 9437184
1608802723848674 Mul_690:0 9437184
1608802723848722 mul_1079:0 2359296
1608802723848768 Mul_511:0 3072
1608802723848807 Mul_194:0 3072
1608802723848853 Mul_212:0 3072
1608802723848898 Mul_226:0 2359296
1608802723848940 mul_574:0 2359296
1608802723848989 Mul_629:0 3072
1608802723849033 Mul_866:0 12288
1608802723849074 Mul_882:0 3072
1608802723849117 Mul_900:0 3072
1608802723849156 Mul_653:0 3072
1608802723849200 Mul_56:0 2359296
1608802723849248 Mul_76:0 3072
1608802723849289 Mul_92:0 12288
1608802723849336 Mul_108:0 3072
1608802723849378 Mul_124:0 3072
1608802723849417 Mul_140:0 2359296
1608802723849465 Mul_159:0 3072
1608802723849510 Mul_178:0 12288
1608802723849556 Mul_946:0 9437184
1608802723849602 Mul_228:0 2359296
1608802723849645 Mul_868:0 12288
1608802723849691 Mul_1072:0 3072
1608802723849749 Mul_1002:0 2359296
1608802723849793 Mul_529:0 9437184
1608802723849844 Mul_6:0 93763584
1608802723849897 Mul_22:0 3072
1608802723849939 Mul_656:0 2359296
1608802723849985 Mul_78:0 3072
1608802723850028 Mul_94:0 12288
1608802723850073 Mul_110:0 3072
1608802723850116 Mul_126:0 3072
1608802723850155 Mul_142:0 2359296
1608802723850204 Mul_162:0 3072
1608802723850249 Mul_180:0 12288
1608802723850288 Mul_196:0 3072
1608802723850332 Mul_215:0 2359296
1608802723850378 Mul_232:0 3072
1608802723850422 Mul_871:0 9437184
1608802723850471 Mul_884:0 3072
1608802723850512 Mul_903:0 2359296
1608802723850558 Mul_513:0 3072
1608802723850603 Mul_533:0 3072
1608802723850642 Mul_10:0 6144
1608802723850688 Mul_24:0 3072
1608802723850732 Mul_1107:0 256
1608802723850770 Mul_60:0 3072
1608802723850812 Mul_922:0 3072
1608802723850852 Mul_97:0 9437184
1608802723850901 Mul_113:0 3072
1608802723850955 Mul_129:0 2359296
1608802723851002 Mul_146:0 3072
1608802723851052 Mul_938:0 3072
1608802723851095 Mul_1081:0 3072
1608802723851135 Mul_199:0 3072
1608802723851180 Mul_948:0 9437184
1608802723851229 Mul_234:0 3072
1608802723851270 Mul_245:0 3072
1608802723851313 Mul_952:0 12288
1608802723851354 Mul_258:0 9437184
1608802723851402 Mul_248:0 3072
1608802723851448 Mul_260:0 9437184
1608802723851493 Mul_275:0 3072
1608802723851539 Mul_250:0 3072
1608802723851585 Mul_264:0 12288
1608802723851627 Mul_873:0 9437184
1608802723851676 Mul_887:0 3072
1608802723851721 Mul_516:0 9437184
1608802723851765 Mul_1006:0 3072
1608802723851809 Mul_12:0 6144
1608802723851849 Mul_27:0 3072
1608802723851902 Mul_914:0 2359296
1608802723851949 Mul_62:0 3072
1608802723851989 Mul_81:0 3072
1608802723852034 Mul_925:0 2359296
1608802723852079 Mul_115:0 3072
1608802723852121 Mul_931:0 3072
1608802723852165 Mul_148:0 3072
1608802723852204 Mul_164:0 3072
1608802723852250 Mul_941:0 3072
1608802723852296 Mul_201:0 3072
1608802723852336 Mul_217:0 2359296
1608802723852395 Mul_237:0 2359296
1608802723852448 Mul_341:0 3072
1608802723852489 Mul_253:0 3072
1608802723852535 Mul_266:0 12288
1608802723852581 Mul_277:0 3072
1608802723852630 Mul_292:0 2359296
1608802723852676 Mul_309:0 3072
1608802723852720 Mul_329:0 3072
1608802723852758 Mul_344:0 9437184
1608802723852803 Mul_975:0 3072
1608802723852845 Mul_954:0 12288
1608802723852889 Mul_269:0 9437184
1608802723852932 Mul_280:0 3072
1608802723852971 Mul_877:0 3072
1608802723853013 Mul_889:0 3072
1608802723853054 Mul_905:0 2359296
1608802723853095 Mul_518:0 9437184
1608802723853143 Mul_535:0 3072
1608802723853182 Mul_909:0 3072
1608802723853223 Mul_29:0 3072
1608802723853265 Mul_43:0 2359296
1608802723853305 Mul_65:0 2359296
1608802723853351 Mul_83:0 3072
1608802723853395 Mul_99:0 9437184
1608802723853435 Mul_118:0 2359296
1608802723853481 Mul_131:0 2359296
1608802723853531 Mul_151:0 2359296
1608802723853573 Mul_167:0 3072
1608802723853618 Mul_183:0 9437184
1608802723853664 Mul_204:0 2359296
1608802723853755 Mul_221:0 3072
1608802723853813 Mul_239:0 2359296
1608802723853856 Mul_296:0 3072
1608802723853903 Mul_965:0 3072
1608802723853961 Mul_331:0 3072
1608802723854000 Mul_346:0 9437184
1608802723854046 Mul_361:0 3072
1608802723854088 Mul_378:0 2359296
1608802723854138 Mul_395:0 3072
1608802723854182 Mul_415:0 3072
1608802723854221 Mul_255:0 3072
1608802723854265 Mul_271:0 9437184
1608802723854312 Mul_959:0 9437184
1608802723854358 Mul_298:0 3072
1608802723854402 Mul_312:0 2359296
1608802723854449 Mul_1070:0 3072
1608802723854487 Mul_892:0 2359296
1608802723854531 Mul_1075:0 2359296
1608802723854573 Mul_522:0 12288
1608802723854620 Mul_538:0 3072
1608802723854665 Mul_16:0 1572864
1608802723854706 Mul_32:0 2359296
1608802723854754 Mul_45:0 2359296
1608802723854799 Mul_67:0 2359296
1608802723854840 Mul_86:0 9437184
1608802723854887 Mul_103:0 3072
1608802723854933 Mul_120:0 2359296
1608802723854974 Mul_135:0 3072
1608802723855018 Mul_153:0 2359296
1608802723855060 Mul_169:0 3072
1608802723855103 Mul_943:0 3072
1608802723855148 Mul_206:0 2359296
1608802723855190 Mul_417:0 3072
1608802723855235 Mul_243:0 3072
1608802723855278 Mul_447:0 3072
1608802723855319 Mul_464:0 2359296
1608802723855363 Mul_957:0 9437184
1608802723855409 Mul_282:0 3072
1608802723855447 Mul_301:0 2359296
1608802723855495 Mul_314:0 2359296
1608802723855537 Mul_1088:0 3072
1608802723855584 Mul_350:0 12288
1608802723855624 Mul_363:0 3072
1608802723855660 Mul_382:0 3072
1608802723855703 Mul_398:0 2359296
1608802723855747 Mul_984:0 3072
1608802723855787 Mul_430:0 9437184
1608802723855833 Mul_449:0 3072
1608802723855879 Mul_468:0 3072
1608802723855917 Mul_894:0 2359296
1608802723855959 Mul_501:0 3072
1608802723855997 Mul_524:0 12288
1608802723856041 Mul_1008:0 3072
1608802723856087 Mul_911:0 3072
1608802723856124 Mul_34:0 2359296
1608802723856168 Mul_916:0 2359296
1608802723856213 Mul_71:0 3072
1608802723856252 Mul_384:0 3072
1608802723856297 Mul_105:0 3072
1608802723856339 Mul_420:0 3072
1608802723856392 Mul_933:0 3072
1608802723856436 Mul_157:0 3072
1608802723856476 Mul_172:0 9437184
1608802723856525 Mul_185:0 9437184
1608802723856575 Mul_210:0 3072
1608802723856614 Mul_285:0 3072
1608802723856659 Mul_303:0 2359296
1608802723856706 Mul_318:0 3072
1608802723856748 Mul_968:0 3072
1608802723856794 Mul_352:0 12288
1608802723856830 Mul_366:0 3072
1608802723856875 Mul_978:0 2359296
1608802723856921 Mul_400:0 2359296
1608802723856962 Mul_422:0 3072
1608802723857006 Mul_432:0 9437184
1608802723857051 Mul_991:0 2359296
1608802723857095 Mul_470:0 3072
1608802723857139 Mul_997:0 3072
1608802723857180 Mul_503:0 3072
1608802723857215 Mul_287:0 3072
1608802723857256 Mul_1086:0 3072
1608802723857293 Mul_320:0 3072
1608802723857336 Mul_527:0 9437184
1608802723857383 Mul_540:0 3072
1608802723857421 Mul_18:0 1572864
1608802723857467 Mul_38:0 3072
1608802723857510 Mul_49:0 3072
1608802723857552 Mul_986:0 3072
1608802723857595 Mul_436:0 12288
1608802723857632 Mul_452:0 3072
1608802723857675 Mul_473:0 2359296
1608802723857717 Mul_137:0 3072
1608802723857757 Mul_936:0 3072
1608802723857802 Mul_189:0 3072
1608802723857841 Mul_290:0 2359296
1608802723857883 Mul_963:0 3072
1608802723857929 Mul_323:0 2359296
1608802723857973 Mul_334:0 3072
1608802723858013 Mul_355:0 9437184
1608802723858056 Mul_368:0 3072
1608802723858094 Mul_387:0 2359296
1608802723858140 Mul_404:0 3072
1608802723858191 Mul_425:0 3072
1608802723858231 Mul_438:0 12288
1608802723858275 Mul_454:0 3072
1608802723858317 Mul_1093:0 3072
1608802723858356 Mul_481:0 3072
1608802723858396 Mul_1000:0 2359296
1608802723858436 Mul_307:0 3072
1608802723858479 Mul_325:0 2359296
1608802723858547 Mul_336:0 3072
1608802723858588 Mul_543:0 3072
1608802723858627 Mul_371:0 3072
1608802723858662 Mul_40:0 3072
1608802723858700 Mul_406:0 3072
1608802723858736 Mul_427:0 3072
1608802723858774 Mul_441:0 9437184
1608802723858813 Mul_457:0 3072
1608802723858851 Mul_995:0 3072
1608802723858889 Mul_484:0 2359296
1608802723858939 Mul_174:0 9437184
1608802723858987 Mul_970:0 3072
1608802723859027 Mul_973:0 3072
1608802723859071 Mul_373:0 3072
1608802723859112 Mul_389:0 2359296
1608802723859154 Mul_409:0 2359296
1608802723859204 Mul_989:0 2359296
1608802723859250 Mul_459:0 3072
1608802723859289 Mul_475:0 2359296
1608802723859333 Mul_486:0 2359296
1608802723859373 Mul_339:0 3072
1608802723859417 Mul_357:0 9437184
1608802723859465 Mul_376:0 2359296
1608802723859504 Mul_980:0 2359296
1608802723859554 Mul_411:0 2359296
1608802723859601 Mul_443:0 9437184
1608802723859643 Mul_462:0 2359296
1608802723859689 Mul_479:0 3072
1608802723859730 Mul_490:0 3072
1608802723859766 Mul_393:0 3072
1608802723859808 Mul_1091:0 3072
1608802723859846 Mul_492:0 3072
1608802723859887 Mul_495:0 2359296
1608802723859930 Cast_1:0 256
1608802723860078 bert/embeddings/dropout/GreaterEqual:0 3145728
1608802723860126 bert/embeddings/dropout/random_uniform/RandomUniform:0 -12582912
1608802723860162 bert/encoder/layer_11/attention/self/dropout/GreaterEqual:0 6291456
1608802723860201 bert/encoder/layer_11/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860215 bert/encoder/layer_4/attention/self/dropout/GreaterEqual:0 6291456
1608802723860256 bert/encoder/layer_4/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860263 bert/encoder/layer_7/attention/self/dropout/GreaterEqual:0 6291456
1608802723860308 bert/encoder/layer_7/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860316 bert/encoder/layer_9/attention/self/dropout/GreaterEqual:0 6291456
1608802723860367 bert/encoder/layer_9/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860374 bert/encoder/layer_0/attention/self/dropout/GreaterEqual:0 6291456
1608802723860417 bert/encoder/layer_0/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860423 bert/encoder/layer_1/attention/self/dropout/GreaterEqual:0 6291456
1608802723860470 bert/encoder/layer_1/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860479 bert/encoder/layer_2/attention/self/dropout/GreaterEqual:0 6291456
1608802723860523 bert/encoder/layer_2/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860531 bert/encoder/layer_10/attention/self/dropout/GreaterEqual:0 6291456
1608802723860570 bert/encoder/layer_10/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860584 bert/encoder/layer_5/attention/self/dropout/GreaterEqual:0 6291456
1608802723860623 bert/encoder/layer_5/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860631 bert/encoder/layer_8/attention/self/dropout/GreaterEqual:0 6291456
1608802723860676 bert/encoder/layer_8/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860683 bert/encoder/layer_6/attention/self/dropout/GreaterEqual:0 6291456
1608802723860726 bert/encoder/layer_6/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860733 bert/encoder/layer_3/attention/self/dropout/GreaterEqual:0 6291456
1608802723860771 bert/encoder/layer_3/attention/self/dropout/random_uniform/RandomUniform:0 -25165824
1608802723860777 bert/encoder/layer_0/output/dropout/GreaterEqual:0 3145728
1608802723860823 bert/encoder/layer_0/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723860829 bert/encoder/layer_6/attention/output/dropout/GreaterEqual:0 3145728
1608802723860872 bert/encoder/layer_6/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723860879 bert/encoder/layer_8/output/dropout/GreaterEqual:0 3145728
1608802723860921 bert/encoder/layer_8/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723860927 bert/encoder/layer_4/attention/output/dropout/GreaterEqual:0 3145728
1608802723860973 bert/encoder/layer_4/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723860981 bert/encoder/layer_9/output/dropout/GreaterEqual:0 3145728
1608802723861027 bert/encoder/layer_9/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861036 bert/encoder/layer_2/attention/output/dropout/GreaterEqual:0 3145728
1608802723861075 bert/encoder/layer_2/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861087 bert/encoder/layer_4/output/dropout/GreaterEqual:0 3145728
1608802723861127 bert/encoder/layer_4/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861133 bert/encoder/layer_10/attention/output/dropout/GreaterEqual:0 3145728
1608802723861178 bert/encoder/layer_10/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861185 bert/encoder/layer_10/output/dropout/GreaterEqual:0 3145728
1608802723861226 bert/encoder/layer_10/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861238 bert/encoder/layer_1/attention/output/dropout/GreaterEqual:0 3145728
1608802723861278 bert/encoder/layer_1/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861284 bert/encoder/layer_5/attention/output/dropout/GreaterEqual:0 3145728
1608802723861329 bert/encoder/layer_5/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861336 bert/encoder/layer_9/attention/output/dropout/GreaterEqual:0 3145728
1608802723861373 bert/encoder/layer_9/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861384 bert/encoder/layer_11/attention/output/dropout/GreaterEqual:0 3145728
1608802723861423 bert/encoder/layer_11/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861429 bert/encoder/layer_1/output/dropout/GreaterEqual:0 3145728
1608802723861474 bert/encoder/layer_1/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861481 bert/encoder/layer_3/attention/output/dropout/GreaterEqual:0 3145728
1608802723861523 bert/encoder/layer_3/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861530 bert/encoder/layer_6/output/dropout/GreaterEqual:0 3145728
1608802723861571 bert/encoder/layer_6/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861579 bert/encoder/layer_11/output/dropout/GreaterEqual:0 3145728
1608802723861623 bert/encoder/layer_11/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861630 bert/encoder/layer_2/output/dropout/GreaterEqual:0 3145728
1608802723861675 bert/encoder/layer_2/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861682 bert/encoder/layer_3/output/dropout/GreaterEqual:0 3145728
1608802723861722 bert/encoder/layer_3/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861728 bert/encoder/layer_7/attention/output/dropout/GreaterEqual:0 3145728
1608802723861772 bert/encoder/layer_7/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861781 bert/encoder/layer_0/attention/output/dropout/GreaterEqual:0 3145728
1608802723861823 bert/encoder/layer_0/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861830 bert/encoder/layer_5/output/dropout/GreaterEqual:0 3145728
1608802723861870 bert/encoder/layer_5/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861876 bert/encoder/layer_7/output/dropout/GreaterEqual:0 3145728
1608802723861922 bert/encoder/layer_7/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723861929 bert/encoder/layer_8/attention/output/dropout/GreaterEqual:0 3145728
1608802723861971 bert/encoder/layer_8/attention/output/dropout/random_uniform/RandomUniform:0 -12582912
1608802723862198 bert/encoder/Cast:0 16384
1608802723862253 Cast:0 -256
1608802723862338 bert/encoder/mul:0 2097152
1608802723862349 bert/embeddings/dropout/Cast:0 12582912
1608802723862446 bert/encoder/Cast:0 -16384
1608802723862476 bert/embeddings/dropout/GreaterEqual:0 -3145728
1608802723862496 bert/encoder/layer_11/attention/self/dropout/Cast:0 25165824
1608802723862725 Cast_3:0 256
1608802723862840 bert/encoder/layer_11/attention/self/dropout/GreaterEqual:0 -6291456
1608802723862903 bert/encoder/layer_4/attention/self/dropout/Cast:0 25165824
1608802723862911 sub:0 256
1608802723863048 bert/encoder/layer_4/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863062 bert/encoder/layer_7/attention/self/dropout/Cast:0 25165824
1608802723863168 bert/encoder/layer_7/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863180 bert/encoder/layer_9/attention/self/dropout/Cast:0 25165824
1608802723863214 Cast_1:0 -256
1608802723863246 bert/encoder/layer_9/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863256 bert/encoder/layer_0/attention/self/dropout/Cast:0 25165824
1608802723863312 bert/encoder/layer_0/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863321 bert/encoder/layer_1/attention/self/dropout/Cast:0 25165824
1608802723863375 bert/encoder/layer_1/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863384 bert/encoder/layer_2/attention/self/dropout/Cast:0 25165824
1608802723863430 bert/encoder/layer_2/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863446 bert/encoder/layer_10/attention/self/dropout/Cast:0 25165824
1608802723863495 bert/encoder/layer_10/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863503 bert/encoder/layer_5/attention/self/dropout/Cast:0 25165824
1608802723863555 bert/encoder/layer_5/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863564 bert/encoder/layer_8/attention/self/dropout/Cast:0 25165824
1608802723863609 bert/encoder/layer_8/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863625 bert/encoder/layer_6/attention/self/dropout/Cast:0 25165824
1608802723863673 bert/encoder/layer_6/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863684 bert/encoder/layer_3/attention/self/dropout/Cast:0 25165824
1608802723863735 bert/encoder/layer_3/attention/self/dropout/GreaterEqual:0 -6291456
1608802723863744 bert/encoder/layer_0/output/dropout/Cast:0 12582912
1608802723863787 bert/encoder/layer_0/output/dropout/GreaterEqual:0 -3145728
1608802723863804 bert/encoder/layer_6/attention/output/dropout/Cast:0 15728640
1608802723863871 bert/encoder/layer_6/attention/output/dropout/GreaterEqual:0 -3145728
1608802723863883 bert/encoder/layer_8/output/dropout/Cast:0 12582912
1608802723863936 bert/encoder/layer_8/output/dropout/GreaterEqual:0 -3145728
1608802723863944 bert/encoder/layer_4/attention/output/dropout/Cast:0 12582912
1608802723863994 bert/encoder/layer_4/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864010 bert/encoder/layer_9/output/dropout/Cast:0 12582912
1608802723864058 bert/encoder/layer_9/output/dropout/GreaterEqual:0 -3145728
1608802723864067 bert/encoder/layer_2/attention/output/dropout/Cast:0 12582912
1608802723864118 bert/encoder/layer_2/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864126 bert/encoder/layer_4/output/dropout/Cast:0 12582912
1608802723864171 bert/encoder/layer_4/output/dropout/GreaterEqual:0 -3145728
1608802723864188 bert/encoder/layer_10/attention/output/dropout/Cast:0 12582912
1608802723864241 bert/encoder/layer_10/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864249 bert/encoder/layer_10/output/dropout/Cast:0 12582912
1608802723864301 bert/encoder/layer_10/output/dropout/GreaterEqual:0 -3145728
1608802723864309 bert/encoder/layer_1/attention/output/dropout/Cast:0 12582912
1608802723864352 bert/encoder/layer_1/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864403 bert/encoder/layer_5/attention/output/dropout/Cast:0 12582912
1608802723864455 bert/encoder/layer_5/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864464 bert/encoder/layer_9/attention/output/dropout/Cast:0 12582912
1608802723864519 bert/encoder/layer_9/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864527 bert/encoder/layer_11/attention/output/dropout/Cast:0 12582912
1608802723864571 bert/encoder/layer_11/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864585 bert/encoder/layer_1/output/dropout/Cast:0 12582912
1608802723864630 bert/encoder/layer_1/output/dropout/GreaterEqual:0 -3145728
1608802723864638 bert/encoder/layer_3/attention/output/dropout/Cast:0 12582912
1608802723864690 bert/encoder/layer_3/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864701 bert/encoder/layer_6/output/dropout/Cast:0 12582912
1608802723864748 bert/encoder/layer_6/output/dropout/GreaterEqual:0 -3145728
1608802723864764 bert/encoder/layer_11/output/dropout/Cast:0 12582912
1608802723864811 bert/encoder/layer_11/output/dropout/GreaterEqual:0 -3145728
1608802723864819 bert/encoder/layer_2/output/dropout/Cast:0 12582912
1608802723864871 bert/encoder/layer_2/output/dropout/GreaterEqual:0 -3145728
1608802723864879 bert/encoder/layer_3/output/dropout/Cast:0 12582912
1608802723864925 bert/encoder/layer_3/output/dropout/GreaterEqual:0 -3145728
1608802723864939 bert/encoder/layer_7/attention/output/dropout/Cast:0 12582912
1608802723864985 bert/encoder/layer_7/attention/output/dropout/GreaterEqual:0 -3145728
1608802723864994 bert/encoder/layer_0/attention/output/dropout/Cast:0 23068672
1608802723865045 bert/encoder/layer_0/attention/output/dropout/GreaterEqual:0 -3145728
1608802723865054 bert/encoder/layer_5/output/dropout/Cast:0 12582912
1608802723865096 bert/encoder/layer_5/output/dropout/GreaterEqual:0 -3145728
1608802723865117 bert/encoder/layer_7/output/dropout/Cast:0 12582912
1608802723865163 bert/encoder/layer_7/output/dropout/GreaterEqual:0 -3145728
1608802723865171 bert/encoder/layer_8/attention/output/dropout/Cast:0 12582912
1608802723865223 bert/encoder/layer_8/attention/output/dropout/GreaterEqual:0 -3145728
1608802723865290 bert/embeddings/MatMul:0 -12582912
1608802723865437 bert/encoder/layer_11/attention/self/dropout/mul:0 25165824
1608802723865505 bert/encoder/layer_4/attention/self/dropout/mul:0 25165824
1608802723865564 bert/encoder/layer_7/attention/self/dropout/mul:0 25165824
1608802723865624 bert/encoder/layer_9/attention/self/dropout/mul:0 25165824
1608802723865678 bert/encoder/layer_0/attention/self/dropout/mul:0 25165824
1608802723865734 bert/encoder/layer_1/attention/self/dropout/mul:0 25165824
1608802723865792 bert/encoder/layer_2/attention/self/dropout/mul:0 25165824
1608802723865845 bert/encoder/layer_10/attention/self/dropout/mul:0 25165824
1608802723865901 bert/encoder/layer_5/attention/self/dropout/mul:0 25165824
1608802723865962 bert/encoder/layer_8/attention/self/dropout/mul:0 25165824
1608802723866019 bert/encoder/layer_6/attention/self/dropout/mul:0 25165824
1608802723866071 bert/encoder/layer_3/attention/self/dropout/mul:0 25165824
1608802723866128 bert/encoder/layer_0/output/dropout/mul:0 12582912
1608802723866184 bert/encoder/layer_6/attention/output/dropout/mul:0 12582912
1608802723866233 bert/encoder/layer_8/output/dropout/mul:0 12582912
1608802723866288 bert/encoder/layer_4/attention/output/dropout/mul:0 12582912
1608802723866345 bert/encoder/layer_9/output/dropout/mul:0 12582912
1608802723866401 bert/encoder/layer_2/attention/output/dropout/mul:0 12582912
1608802723866456 bert/encoder/layer_4/output/dropout/mul:0 12582912
1608802723866511 bert/encoder/layer_10/attention/output/dropout/mul:0 12582912
1608802723866568 bert/encoder/layer_10/output/dropout/mul:0 12582912
1608802723866618 bert/encoder/layer_1/attention/output/dropout/mul:0 12582912
1608802723866673 bert/encoder/layer_5/attention/output/dropout/mul:0 12582912
1608802723866730 bert/encoder/layer_9/attention/output/dropout/mul:0 12582912
1608802723866781 bert/encoder/layer_11/attention/output/dropout/mul:0 12582912
1608802723866836 bert/encoder/layer_1/output/dropout/mul:0 12582912
1608802723866892 bert/encoder/layer_3/attention/output/dropout/mul:0 12582912
1608802723866941 bert/encoder/layer_6/output/dropout/mul:0 12582912
1608802723866998 bert/encoder/layer_11/output/dropout/mul:0 12582912
1608802723867053 bert/encoder/layer_2/output/dropout/mul:0 12582912
1608802723867103 bert/encoder/layer_3/output/dropout/mul:0 12582912
1608802723867159 bert/encoder/layer_7/attention/output/dropout/mul:0 12582912
1608802723867212 bert/encoder/layer_0/attention/output/dropout/mul:0 12582912
1608802723867265 bert/encoder/layer_5/output/dropout/mul:0 12582912
1608802723867317 bert/encoder/layer_7/output/dropout/mul:0 12582912
1608802723867373 bert/encoder/layer_8/attention/output/dropout/mul:0 12582912
1608802723867497 global_step/add:0 256
1608802723867648 bert/embeddings/LayerNorm/moments/mean:0 16384
1608802723867866 bert/embeddings/LayerNorm/moments/SquaredDifference:0 12582912
1608802723867995 sub:0 -256
1608802723868000 bert/embeddings/LayerNorm/moments/variance:0 16384
1608802723868059 bert/embeddings/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723868140 Cast_3:0 -256
1608802723868140 global_step/add:0 -256
1608802723868342 bert/embeddings/LayerNorm/batchnorm/mul:0 12582912
1608802723868422 bert/embeddings/LayerNorm/batchnorm/mul_1:0 12582912
1608802723868668 bert/embeddings/LayerNorm/batchnorm/mul_2:0 12582912
1608802723868871 bert/embeddings/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723868944 bert/encoder/layer_0/attention/self/query/MatMul:0 12582912
1608802723869026 bert/encoder/layer_0/attention/self/key/MatMul:0 12582912
1608802723869084 bert/encoder/layer_0/attention/self/value/MatMul:0 12582912
1608802723869317 bert/encoder/layer_0/attention/self/transpose:0 12582912
1608802723869377 bert/encoder/layer_0/attention/self/query/MatMul:0 -12582912
1608802723869387 bert/encoder/layer_0/attention/self/transpose_1:0 12582912
1608802723869451 bert/encoder/layer_0/attention/self/key/MatMul:0 -12582912
1608802723869461 bert/encoder/layer_0/attention/self/transpose_2:0 12582912
1608802723869514 bert/encoder/layer_0/attention/self/value/MatMul:0 -12582912
1608802723869529 bert/encoder/layer_0/attention/self/MatMul:0 25165824
1608802723869529 bert/encoder/layer_0/attention/self/MatMul:t0 3072
1608802723869529 bert/encoder/layer_0/attention/self/MatMul:t1 3072
1608802723869529 bert/encoder/layer_0/attention/self/MatMul:t2 3072
1608802723869836 bert/encoder/layer_0/attention/self/MatMul:t0 -3072
1608802723869836 bert/encoder/layer_0/attention/self/MatMul:t1 -3072
1608802723869836 bert/encoder/layer_0/attention/self/MatMul:t2 -3072
1608802723869959 bert/encoder/layer_0/attention/self/Softmax:0 25165824
1608802723869959 bert/encoder/layer_0/attention/self/Softmax:t0 25165824
1608802723870073 bert/encoder/layer_0/attention/self/Softmax:0 -25165824
1608802723870073 bert/encoder/layer_0/attention/self/Softmax:t0 -25165824
1608802723870084 gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul_1:0 25165824
1608802723870185 bert/encoder/layer_0/attention/self/MatMul_1:0 12582912
1608802723870185 bert/encoder/layer_0/attention/self/MatMul_1:t0 3072
1608802723870185 bert/encoder/layer_0/attention/self/MatMul_1:t1 3072
1608802723870185 bert/encoder/layer_0/attention/self/MatMul_1:t2 3072
1608802723870432 bert/encoder/layer_0/attention/self/MatMul_1:t0 -3072
1608802723870432 bert/encoder/layer_0/attention/self/MatMul_1:t1 -3072
1608802723870432 bert/encoder/layer_0/attention/self/MatMul_1:t2 -3072
1608802723870445 bert/encoder/layer_0/attention/self/transpose_3:0 12582912
1608802723870501 bert/encoder/layer_0/attention/self/MatMul_1:0 -12582912
1608802723870527 bert/encoder/layer_0/attention/output/dense/MatMul:0 12582912
1608802723870677 bert/encoder/layer_0/attention/output/dropout/mul:0 -12582912
1608802723870724 bert/encoder/layer_0/attention/output/LayerNorm/moments/mean:0 16384
1608802723870781 bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723870838 bert/encoder/layer_0/attention/output/LayerNorm/moments/variance:0 16384
1608802723870884 bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723870986 bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723871041 bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723871093 bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723871242 bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723871247 bert/encoder/layer_0/intermediate/dense/MatMul:0 50331648
1608802723871373 bert/encoder/layer_0/intermediate/dense/Pow:0 50331648
1608802723871432 gradients/bert/encoder/layer_0/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723871497 gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723871816 bert/encoder/layer_0/intermediate/dense/add_1:0 50331648
1608802723871971 bert/encoder/layer_0/output/dense/MatMul:0 12582912
1608802723872127 bert/encoder/layer_0/output/dropout/mul:0 -12582912
1608802723872183 bert/encoder/layer_0/output/LayerNorm/moments/mean:0 16384
1608802723872241 bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723872296 bert/encoder/layer_0/output/LayerNorm/moments/variance:0 16384
1608802723872350 bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723872478 bert/encoder/layer_0/output/LayerNorm/batchnorm/mul:0 12582912
1608802723872538 bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723872599 bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723872760 bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723872765 bert/encoder/layer_1/attention/self/query/MatMul:0 12582912
1608802723872828 bert/encoder/layer_1/attention/self/key/MatMul:0 12582912
1608802723872896 bert/encoder/layer_1/attention/self/value/MatMul:0 12582912
1608802723873151 bert/encoder/layer_1/attention/self/transpose:0 12582912
1608802723873206 bert/encoder/layer_1/attention/self/query/MatMul:0 -12582912
1608802723873215 bert/encoder/layer_1/attention/self/transpose_1:0 12582912
1608802723873262 bert/encoder/layer_1/attention/self/key/MatMul:0 -12582912
1608802723873277 bert/encoder/layer_1/attention/self/transpose_2:0 12582912
1608802723873323 bert/encoder/layer_1/attention/self/value/MatMul:0 -12582912
1608802723873332 bert/encoder/layer_1/attention/self/MatMul:0 25165824
1608802723873332 bert/encoder/layer_1/attention/self/MatMul:t0 3072
1608802723873332 bert/encoder/layer_1/attention/self/MatMul:t1 3072
1608802723873332 bert/encoder/layer_1/attention/self/MatMul:t2 3072
1608802723873609 bert/encoder/layer_1/attention/self/MatMul:t0 -3072
1608802723873609 bert/encoder/layer_1/attention/self/MatMul:t1 -3072
1608802723873609 bert/encoder/layer_1/attention/self/MatMul:t2 -3072
1608802723873737 bert/encoder/layer_1/attention/self/Softmax:0 25165824
1608802723873737 bert/encoder/layer_1/attention/self/Softmax:t0 25165824
1608802723873834 bert/encoder/layer_1/attention/self/Softmax:0 -25165824
1608802723873834 bert/encoder/layer_1/attention/self/Softmax:t0 -25165824
1608802723873845 gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul_1:0 25165824
1608802723873973 bert/encoder/layer_1/attention/self/MatMul_1:0 12582912
1608802723873973 bert/encoder/layer_1/attention/self/MatMul_1:t0 3072
1608802723873973 bert/encoder/layer_1/attention/self/MatMul_1:t1 3072
1608802723873973 bert/encoder/layer_1/attention/self/MatMul_1:t2 3072
1608802723874208 bert/encoder/layer_1/attention/self/MatMul_1:t0 -3072
1608802723874208 bert/encoder/layer_1/attention/self/MatMul_1:t1 -3072
1608802723874208 bert/encoder/layer_1/attention/self/MatMul_1:t2 -3072
1608802723874221 bert/encoder/layer_1/attention/self/transpose_3:0 12582912
1608802723874279 bert/encoder/layer_1/attention/self/MatMul_1:0 -12582912
1608802723874309 bert/encoder/layer_1/attention/output/dense/MatMul:0 12582912
1608802723874459 bert/encoder/layer_1/attention/output/dropout/mul:0 -12582912
1608802723874510 bert/encoder/layer_1/attention/output/LayerNorm/moments/mean:0 16384
1608802723874564 bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723874622 bert/encoder/layer_1/attention/output/LayerNorm/moments/variance:0 16384
1608802723874672 bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723874762 bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723874822 bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723874876 bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723875030 bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723875035 bert/encoder/layer_1/intermediate/dense/MatMul:0 50331648
1608802723875165 bert/encoder/layer_1/intermediate/dense/Pow:0 50331648
1608802723875235 gradients/bert/encoder/layer_1/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723875299 gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723875602 bert/encoder/layer_1/intermediate/dense/add_1:0 50331648
1608802723875764 bert/encoder/layer_1/output/dense/MatMul:0 12582912
1608802723875928 bert/encoder/layer_1/output/dropout/mul:0 -12582912
1608802723875978 bert/encoder/layer_1/output/LayerNorm/moments/mean:0 16384
1608802723876037 bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723876107 bert/encoder/layer_1/output/LayerNorm/moments/variance:0 18688
1608802723876156 bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723876262 bert/encoder/layer_1/output/LayerNorm/batchnorm/mul:0 12582912
1608802723876324 bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723876394 bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723876562 bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723876566 bert/encoder/layer_2/attention/self/query/MatMul:0 12582912
1608802723876634 bert/encoder/layer_2/attention/self/key/MatMul:0 12582912
1608802723876697 bert/encoder/layer_2/attention/self/value/MatMul:0 12582912
1608802723876956 bert/encoder/layer_2/attention/self/transpose:0 12582912
1608802723877008 bert/encoder/layer_2/attention/self/query/MatMul:0 -12582912
1608802723877019 bert/encoder/layer_2/attention/self/transpose_1:0 12582912
1608802723877074 bert/encoder/layer_2/attention/self/key/MatMul:0 -12582912
1608802723877084 bert/encoder/layer_2/attention/self/transpose_2:0 12582912
1608802723877139 bert/encoder/layer_2/attention/self/value/MatMul:0 -12582912
1608802723877148 bert/encoder/layer_2/attention/self/MatMul:0 25165824
1608802723877148 bert/encoder/layer_2/attention/self/MatMul:t0 3072
1608802723877148 bert/encoder/layer_2/attention/self/MatMul:t1 3072
1608802723877148 bert/encoder/layer_2/attention/self/MatMul:t2 3072
1608802723877415 bert/encoder/layer_2/attention/self/MatMul:t0 -3072
1608802723877415 bert/encoder/layer_2/attention/self/MatMul:t1 -3072
1608802723877415 bert/encoder/layer_2/attention/self/MatMul:t2 -3072
1608802723877640 bert/encoder/layer_2/attention/self/Softmax:0 25165824
1608802723877640 bert/encoder/layer_2/attention/self/Softmax:t0 25165824
1608802723877739 bert/encoder/layer_2/attention/self/Softmax:0 -25165824
1608802723877739 bert/encoder/layer_2/attention/self/Softmax:t0 -25165824
1608802723877749 gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul_1:0 25165824
1608802723877856 bert/encoder/layer_2/attention/self/MatMul_1:0 12582912
1608802723877856 bert/encoder/layer_2/attention/self/MatMul_1:t0 3072
1608802723877856 bert/encoder/layer_2/attention/self/MatMul_1:t1 3072
1608802723877856 bert/encoder/layer_2/attention/self/MatMul_1:t2 3072
1608802723878094 bert/encoder/layer_2/attention/self/MatMul_1:t0 -3072
1608802723878094 bert/encoder/layer_2/attention/self/MatMul_1:t1 -3072
1608802723878094 bert/encoder/layer_2/attention/self/MatMul_1:t2 -3072
1608802723878107 bert/encoder/layer_2/attention/self/transpose_3:0 12582912
1608802723878162 bert/encoder/layer_2/attention/self/MatMul_1:0 -12582912
1608802723878191 bert/encoder/layer_2/attention/output/dense/MatMul:0 12582912
1608802723878346 bert/encoder/layer_2/attention/output/dropout/mul:0 -12582912
1608802723878396 bert/encoder/layer_2/attention/output/LayerNorm/moments/mean:0 16384
1608802723878449 bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723878502 bert/encoder/layer_2/attention/output/LayerNorm/moments/variance:0 16384
1608802723878547 bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723878640 bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723878693 bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723878745 bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723878896 bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723878905 bert/encoder/layer_2/intermediate/dense/MatMul:0 50331648
1608802723879022 bert/encoder/layer_2/intermediate/dense/Pow:0 50331648
1608802723879084 gradients/bert/encoder/layer_2/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723879148 gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723879450 bert/encoder/layer_2/intermediate/dense/add_1:0 50331648
1608802723879619 bert/encoder/layer_2/output/dense/MatMul:0 12582912
1608802723879777 bert/encoder/layer_2/output/dropout/mul:0 -12582912
1608802723879833 bert/encoder/layer_2/output/LayerNorm/moments/mean:0 16384
1608802723879885 bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723879951 bert/encoder/layer_2/output/LayerNorm/moments/variance:0 16384
1608802723879998 bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723880107 bert/encoder/layer_2/output/LayerNorm/batchnorm/mul:0 12582912
1608802723880164 bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723880218 bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723880392 bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723880399 bert/encoder/layer_3/attention/self/query/MatMul:0 12582912
1608802723880471 bert/encoder/layer_3/attention/self/key/MatMul:0 12582912
1608802723880534 bert/encoder/layer_3/attention/self/value/MatMul:0 12582912
1608802723880770 bert/encoder/layer_3/attention/self/transpose:0 12582912
1608802723880818 bert/encoder/layer_3/attention/self/query/MatMul:0 -12582912
1608802723880830 bert/encoder/layer_3/attention/self/transpose_1:0 12582912
1608802723880882 bert/encoder/layer_3/attention/self/key/MatMul:0 -12582912
1608802723880891 bert/encoder/layer_3/attention/self/transpose_2:0 12582912
1608802723880945 bert/encoder/layer_3/attention/self/value/MatMul:0 -12582912
1608802723880953 bert/encoder/layer_3/attention/self/MatMul:0 25165824
1608802723880953 bert/encoder/layer_3/attention/self/MatMul:t0 3072
1608802723880953 bert/encoder/layer_3/attention/self/MatMul:t1 3072
1608802723880953 bert/encoder/layer_3/attention/self/MatMul:t2 3072
1608802723881224 bert/encoder/layer_3/attention/self/MatMul:t0 -3072
1608802723881224 bert/encoder/layer_3/attention/self/MatMul:t1 -3072
1608802723881224 bert/encoder/layer_3/attention/self/MatMul:t2 -3072
1608802723881343 bert/encoder/layer_3/attention/self/Softmax:0 25165824
1608802723881343 bert/encoder/layer_3/attention/self/Softmax:t0 25165824
1608802723881437 bert/encoder/layer_3/attention/self/Softmax:0 -25165824
1608802723881437 bert/encoder/layer_3/attention/self/Softmax:t0 -25165824
1608802723881447 gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul_1:0 25165824
1608802723881553 bert/encoder/layer_3/attention/self/MatMul_1:0 12582912
1608802723881553 bert/encoder/layer_3/attention/self/MatMul_1:t0 3072
1608802723881553 bert/encoder/layer_3/attention/self/MatMul_1:t1 3072
1608802723881553 bert/encoder/layer_3/attention/self/MatMul_1:t2 3072
1608802723881797 bert/encoder/layer_3/attention/self/MatMul_1:t0 -3072
1608802723881797 bert/encoder/layer_3/attention/self/MatMul_1:t1 -3072
1608802723881797 bert/encoder/layer_3/attention/self/MatMul_1:t2 -3072
1608802723881810 bert/encoder/layer_3/attention/self/transpose_3:0 12582912
1608802723881866 bert/encoder/layer_3/attention/self/MatMul_1:0 -12582912
1608802723881897 bert/encoder/layer_3/attention/output/dense/MatMul:0 12582912
1608802723882048 bert/encoder/layer_3/attention/output/dropout/mul:0 -12582912
1608802723882096 bert/encoder/layer_3/attention/output/LayerNorm/moments/mean:0 16384
1608802723882151 bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723882206 bert/encoder/layer_3/attention/output/LayerNorm/moments/variance:0 16384
1608802723882251 bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723882347 bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723882402 bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723882454 bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723882603 bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723882608 bert/encoder/layer_3/intermediate/dense/MatMul:0 50331648
1608802723882731 bert/encoder/layer_3/intermediate/dense/Pow:0 50331648
1608802723882796 gradients/bert/encoder/layer_3/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723882860 gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723883162 bert/encoder/layer_3/intermediate/dense/add_1:0 50331648
1608802723883324 bert/encoder/layer_3/output/dense/MatMul:0 12582912
1608802723883484 bert/encoder/layer_3/output/dropout/mul:0 -12582912
1608802723883536 bert/encoder/layer_3/output/LayerNorm/moments/mean:0 16384
1608802723883598 bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723883661 bert/encoder/layer_3/output/LayerNorm/moments/variance:0 16384
1608802723883708 bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723883814 bert/encoder/layer_3/output/LayerNorm/batchnorm/mul:0 12582912
1608802723883872 bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723883935 bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723884092 bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723884097 bert/encoder/layer_4/attention/self/query/MatMul:0 12582912
1608802723884169 bert/encoder/layer_4/attention/self/key/MatMul:0 12582912
1608802723884253 bert/encoder/layer_4/attention/self/value/MatMul:0 12582912
1608802723884519 bert/encoder/layer_4/attention/self/transpose:0 12582912
1608802723884568 bert/encoder/layer_4/attention/self/query/MatMul:0 -12582912
1608802723884578 bert/encoder/layer_4/attention/self/transpose_1:0 12582912
1608802723884636 bert/encoder/layer_4/attention/self/key/MatMul:0 -12582912
1608802723884648 bert/encoder/layer_4/attention/self/transpose_2:0 12582912
1608802723884711 bert/encoder/layer_4/attention/self/value/MatMul:0 -12582912
1608802723884721 bert/encoder/layer_4/attention/self/MatMul:0 25165824
1608802723884721 bert/encoder/layer_4/attention/self/MatMul:t0 3072
1608802723884721 bert/encoder/layer_4/attention/self/MatMul:t1 3072
1608802723884721 bert/encoder/layer_4/attention/self/MatMul:t2 3072
1608802723884980 bert/encoder/layer_4/attention/self/MatMul:t0 -3072
1608802723884980 bert/encoder/layer_4/attention/self/MatMul:t1 -3072
1608802723884980 bert/encoder/layer_4/attention/self/MatMul:t2 -3072
1608802723885096 bert/encoder/layer_4/attention/self/Softmax:0 25165824
1608802723885096 bert/encoder/layer_4/attention/self/Softmax:t0 25165824
1608802723885186 bert/encoder/layer_4/attention/self/Softmax:0 -25165824
1608802723885186 bert/encoder/layer_4/attention/self/Softmax:t0 -25165824
1608802723885196 gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul_1:0 25165824
1608802723885290 bert/encoder/layer_4/attention/self/MatMul_1:0 12582912
1608802723885290 bert/encoder/layer_4/attention/self/MatMul_1:t0 3072
1608802723885290 bert/encoder/layer_4/attention/self/MatMul_1:t1 3072
1608802723885290 bert/encoder/layer_4/attention/self/MatMul_1:t2 3072
1608802723885510 bert/encoder/layer_4/attention/self/MatMul_1:t0 -3072
1608802723885510 bert/encoder/layer_4/attention/self/MatMul_1:t1 -3072
1608802723885510 bert/encoder/layer_4/attention/self/MatMul_1:t2 -3072
1608802723885522 bert/encoder/layer_4/attention/self/transpose_3:0 12582912
1608802723885574 bert/encoder/layer_4/attention/self/MatMul_1:0 -12582912
1608802723885599 bert/encoder/layer_4/attention/output/dense/MatMul:0 12582912
1608802723889000 bert/encoder/layer_4/attention/output/dropout/mul:0 -12582912
1608802723889070 bert/encoder/layer_4/attention/output/LayerNorm/moments/mean:0 16384
1608802723889129 bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723889194 bert/encoder/layer_4/attention/output/LayerNorm/moments/variance:0 16384
1608802723889240 bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723889333 bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723889397 bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723889458 bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723889612 bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723889617 bert/encoder/layer_4/intermediate/dense/MatMul:0 50331648
1608802723889758 bert/encoder/layer_4/intermediate/dense/Pow:0 50331648
1608802723889820 gradients/bert/encoder/layer_4/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723889882 gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723890166 bert/encoder/layer_4/intermediate/dense/add_1:0 50331648
1608802723890316 bert/encoder/layer_4/output/dense/MatMul:0 12582912
1608802723890469 bert/encoder/layer_4/output/dropout/mul:0 -12582912
1608802723890526 bert/encoder/layer_4/output/LayerNorm/moments/mean:0 16384
1608802723890578 bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723890638 bert/encoder/layer_4/output/LayerNorm/moments/variance:0 16384
1608802723890683 bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723890788 bert/encoder/layer_4/output/LayerNorm/batchnorm/mul:0 12582912
1608802723890843 bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723890894 bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723891044 bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723891049 bert/encoder/layer_5/attention/self/query/MatMul:0 12582912
1608802723891114 bert/encoder/layer_5/attention/self/key/MatMul:0 12582912
1608802723891176 bert/encoder/layer_5/attention/self/value/MatMul:0 12582912
1608802723891406 bert/encoder/layer_5/attention/self/transpose:0 12582912
1608802723891453 bert/encoder/layer_5/attention/self/query/MatMul:0 -12582912
1608802723891465 bert/encoder/layer_5/attention/self/transpose_1:0 12582912
1608802723891516 bert/encoder/layer_5/attention/self/key/MatMul:0 -12582912
1608802723891526 bert/encoder/layer_5/attention/self/transpose_2:0 12582912
1608802723891575 bert/encoder/layer_5/attention/self/value/MatMul:0 -12582912
1608802723891584 bert/encoder/layer_5/attention/self/MatMul:0 25165824
1608802723891584 bert/encoder/layer_5/attention/self/MatMul:t0 3072
1608802723891584 bert/encoder/layer_5/attention/self/MatMul:t1 3072
1608802723891584 bert/encoder/layer_5/attention/self/MatMul:t2 3072
1608802723891850 bert/encoder/layer_5/attention/self/MatMul:t0 -3072
1608802723891850 bert/encoder/layer_5/attention/self/MatMul:t1 -3072
1608802723891850 bert/encoder/layer_5/attention/self/MatMul:t2 -3072
1608802723891966 bert/encoder/layer_5/attention/self/Softmax:0 25165824
1608802723891966 bert/encoder/layer_5/attention/self/Softmax:t0 25165824
1608802723892057 bert/encoder/layer_5/attention/self/Softmax:0 -25165824
1608802723892057 bert/encoder/layer_5/attention/self/Softmax:t0 -25165824
1608802723892067 gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul_1:0 25165824
1608802723892161 bert/encoder/layer_5/attention/self/MatMul_1:0 12582912
1608802723892161 bert/encoder/layer_5/attention/self/MatMul_1:t0 3072
1608802723892161 bert/encoder/layer_5/attention/self/MatMul_1:t1 3072
1608802723892161 bert/encoder/layer_5/attention/self/MatMul_1:t2 3072
1608802723892407 bert/encoder/layer_5/attention/self/MatMul_1:t0 -3072
1608802723892407 bert/encoder/layer_5/attention/self/MatMul_1:t1 -3072
1608802723892407 bert/encoder/layer_5/attention/self/MatMul_1:t2 -3072
1608802723892419 bert/encoder/layer_5/attention/self/transpose_3:0 12582912
1608802723892476 bert/encoder/layer_5/attention/self/MatMul_1:0 -12582912
1608802723892500 bert/encoder/layer_5/attention/output/dense/MatMul:0 12582912
1608802723892641 bert/encoder/layer_5/attention/output/dropout/mul:0 -12582912
1608802723892688 bert/encoder/layer_5/attention/output/LayerNorm/moments/mean:0 16384
1608802723892738 bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723892788 bert/encoder/layer_5/attention/output/LayerNorm/moments/variance:0 16384
1608802723892832 bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723892922 bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723892974 bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723893023 bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723893151 bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723893156 bert/encoder/layer_5/intermediate/dense/MatMul:0 50331648
1608802723893280 bert/encoder/layer_5/intermediate/dense/Pow:0 50331648
1608802723893333 gradients/bert/encoder/layer_5/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723893394 gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723893723 bert/encoder/layer_5/intermediate/dense/add_1:0 50331648
1608802723893868 bert/encoder/layer_5/output/dense/MatMul:0 12582912
1608802723894002 bert/encoder/layer_5/output/dropout/mul:0 -12582912
1608802723894065 bert/encoder/layer_5/output/LayerNorm/moments/mean:0 16384
1608802723894115 bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723894169 bert/encoder/layer_5/output/LayerNorm/moments/variance:0 16384
1608802723894216 bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723894302 bert/encoder/layer_5/output/LayerNorm/batchnorm/mul:0 12582912
1608802723894352 bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723894403 bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723894574 bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723894579 bert/encoder/layer_6/attention/self/query/MatMul:0 12582912
1608802723894639 bert/encoder/layer_6/attention/self/key/MatMul:0 12582912
1608802723894702 bert/encoder/layer_6/attention/self/value/MatMul:0 12582912
1608802723894937 bert/encoder/layer_6/attention/self/transpose:0 12582912
1608802723894990 bert/encoder/layer_6/attention/self/query/MatMul:0 -12582912
1608802723894999 bert/encoder/layer_6/attention/self/transpose_1:0 12582912
1608802723895045 bert/encoder/layer_6/attention/self/key/MatMul:0 -12582912
1608802723895060 bert/encoder/layer_6/attention/self/transpose_2:0 12582912
1608802723895106 bert/encoder/layer_6/attention/self/value/MatMul:0 -12582912
1608802723895114 bert/encoder/layer_6/attention/self/MatMul:0 25165824
1608802723895114 bert/encoder/layer_6/attention/self/MatMul:t0 3072
1608802723895114 bert/encoder/layer_6/attention/self/MatMul:t1 3072
1608802723895114 bert/encoder/layer_6/attention/self/MatMul:t2 3072
1608802723895367 bert/encoder/layer_6/attention/self/MatMul:t0 -3072
1608802723895367 bert/encoder/layer_6/attention/self/MatMul:t1 -3072
1608802723895367 bert/encoder/layer_6/attention/self/MatMul:t2 -3072
1608802723895483 bert/encoder/layer_6/attention/self/Softmax:0 25165824
1608802723895483 bert/encoder/layer_6/attention/self/Softmax:t0 25165824
1608802723895573 bert/encoder/layer_6/attention/self/Softmax:0 -25165824
1608802723895573 bert/encoder/layer_6/attention/self/Softmax:t0 -25165824
1608802723895584 gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul_1:0 25165824
1608802723895683 bert/encoder/layer_6/attention/self/MatMul_1:0 12582912
1608802723895683 bert/encoder/layer_6/attention/self/MatMul_1:t0 3072
1608802723895683 bert/encoder/layer_6/attention/self/MatMul_1:t1 3072
1608802723895683 bert/encoder/layer_6/attention/self/MatMul_1:t2 3072
1608802723895911 bert/encoder/layer_6/attention/self/MatMul_1:t0 -3072
1608802723895911 bert/encoder/layer_6/attention/self/MatMul_1:t1 -3072
1608802723895911 bert/encoder/layer_6/attention/self/MatMul_1:t2 -3072
1608802723895923 bert/encoder/layer_6/attention/self/transpose_3:0 12582912
1608802723895976 bert/encoder/layer_6/attention/self/MatMul_1:0 -12582912
1608802723896001 bert/encoder/layer_6/attention/output/dense/MatMul:0 12582912
1608802723896143 bert/encoder/layer_6/attention/output/dropout/mul:0 -12582912
1608802723896194 bert/encoder/layer_6/attention/output/LayerNorm/moments/mean:0 16384
1608802723896247 bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723896299 bert/encoder/layer_6/attention/output/LayerNorm/moments/variance:0 16384
1608802723896343 bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723896462 bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723896515 bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723896566 bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723896722 bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723896727 bert/encoder/layer_6/intermediate/dense/MatMul:0 50331648
1608802723896848 bert/encoder/layer_6/intermediate/dense/Pow:0 50331648
1608802723896907 gradients/bert/encoder/layer_6/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723896967 gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723897250 bert/encoder/layer_6/intermediate/dense/add_1:0 50331648
1608802723897395 bert/encoder/layer_6/output/dense/MatMul:0 12582912
1608802723897543 bert/encoder/layer_6/output/dropout/mul:0 -12582912
1608802723897588 bert/encoder/layer_6/output/LayerNorm/moments/mean:0 16384
1608802723897645 bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723897704 bert/encoder/layer_6/output/LayerNorm/moments/variance:0 16384
1608802723897747 bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723897847 bert/encoder/layer_6/output/LayerNorm/batchnorm/mul:0 12582912
1608802723897897 bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723897952 bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723898094 bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723898099 bert/encoder/layer_7/attention/self/query/MatMul:0 12582912
1608802723898163 bert/encoder/layer_7/attention/self/key/MatMul:0 12582912
1608802723898222 bert/encoder/layer_7/attention/self/value/MatMul:0 12582912
1608802723898441 bert/encoder/layer_7/attention/self/transpose:0 12582912
1608802723898486 bert/encoder/layer_7/attention/self/query/MatMul:0 -12582912
1608802723898497 bert/encoder/layer_7/attention/self/transpose_1:0 12582912
1608802723898545 bert/encoder/layer_7/attention/self/key/MatMul:0 -12582912
1608802723898554 bert/encoder/layer_7/attention/self/transpose_2:0 12582912
1608802723898597 bert/encoder/layer_7/attention/self/value/MatMul:0 -12582912
1608802723898612 bert/encoder/layer_7/attention/self/MatMul:0 25165824
1608802723898612 bert/encoder/layer_7/attention/self/MatMul:t0 3072
1608802723898612 bert/encoder/layer_7/attention/self/MatMul:t1 3072
1608802723898612 bert/encoder/layer_7/attention/self/MatMul:t2 3072
1608802723898862 bert/encoder/layer_7/attention/self/MatMul:t0 -3072
1608802723898862 bert/encoder/layer_7/attention/self/MatMul:t1 -3072
1608802723898862 bert/encoder/layer_7/attention/self/MatMul:t2 -3072
1608802723898971 bert/encoder/layer_7/attention/self/Softmax:0 25165824
1608802723898971 bert/encoder/layer_7/attention/self/Softmax:t0 25165824
1608802723899055 bert/encoder/layer_7/attention/self/Softmax:0 -25165824
1608802723899055 bert/encoder/layer_7/attention/self/Softmax:t0 -25165824
1608802723899066 gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul_1:0 25165824
1608802723899156 bert/encoder/layer_7/attention/self/MatMul_1:0 12582912
1608802723899156 bert/encoder/layer_7/attention/self/MatMul_1:t0 3072
1608802723899156 bert/encoder/layer_7/attention/self/MatMul_1:t1 3072
1608802723899156 bert/encoder/layer_7/attention/self/MatMul_1:t2 3072
1608802723899368 bert/encoder/layer_7/attention/self/MatMul_1:t0 -3072
1608802723899368 bert/encoder/layer_7/attention/self/MatMul_1:t1 -3072
1608802723899368 bert/encoder/layer_7/attention/self/MatMul_1:t2 -3072
1608802723899380 bert/encoder/layer_7/attention/self/transpose_3:0 12582912
1608802723899431 bert/encoder/layer_7/attention/self/MatMul_1:0 -12582912
1608802723899455 bert/encoder/layer_7/attention/output/dense/MatMul:0 12582912
1608802723899590 bert/encoder/layer_7/attention/output/dropout/mul:0 -12582912
1608802723899636 bert/encoder/layer_7/attention/output/LayerNorm/moments/mean:0 16384
1608802723899683 bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723899738 bert/encoder/layer_7/attention/output/LayerNorm/moments/variance:0 16384
1608802723899779 bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723899863 bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723899911 bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723899970 bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723900732 bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723900737 bert/encoder/layer_7/intermediate/dense/MatMul:0 50331648
1608802723900875 bert/encoder/layer_7/intermediate/dense/Pow:0 50331648
1608802723900932 gradients/bert/encoder/layer_7/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723900991 gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723901266 bert/encoder/layer_7/intermediate/dense/add_1:0 50331648
1608802723903676 bert/encoder/layer_7/output/dense/MatMul:0 12582912
1608802723904441 bert/encoder/layer_7/output/dropout/mul:0 -12582912
1608802723904645 bert/encoder/layer_7/output/LayerNorm/moments/mean:0 16384
1608802723904839 bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723905124 bert/encoder/layer_7/output/LayerNorm/moments/variance:0 16384
1608802723905319 bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723905719 bert/encoder/layer_7/output/LayerNorm/batchnorm/mul:0 12582912
1608802723905929 bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723906210 bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723908808 bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723908820 bert/encoder/layer_8/attention/self/query/MatMul:0 12582912
1608802723908896 bert/encoder/layer_8/attention/self/key/MatMul:0 12582912
1608802723908954 bert/encoder/layer_8/attention/self/value/MatMul:0 12582912
1608802723909182 bert/encoder/layer_8/attention/self/transpose:0 12582912
1608802723909232 bert/encoder/layer_8/attention/self/query/MatMul:0 -12582912
1608802723909241 bert/encoder/layer_8/attention/self/transpose_1:0 12582912
1608802723909283 bert/encoder/layer_8/attention/self/key/MatMul:0 -12582912
1608802723909292 bert/encoder/layer_8/attention/self/transpose_2:0 12582912
1608802723909339 bert/encoder/layer_8/attention/self/value/MatMul:0 -12582912
1608802723909347 bert/encoder/layer_8/attention/self/MatMul:0 25165824
1608802723909347 bert/encoder/layer_8/attention/self/MatMul:t0 3072
1608802723909347 bert/encoder/layer_8/attention/self/MatMul:t1 3072
1608802723909347 bert/encoder/layer_8/attention/self/MatMul:t2 3072
1608802723910615 bert/encoder/layer_8/attention/self/MatMul:t0 -3072
1608802723910615 bert/encoder/layer_8/attention/self/MatMul:t1 -3072
1608802723910615 bert/encoder/layer_8/attention/self/MatMul:t2 -3072
1608802723911303 bert/encoder/layer_8/attention/self/Softmax:0 25165824
1608802723911303 bert/encoder/layer_8/attention/self/Softmax:t0 25165824
1608802723911614 bert/encoder/layer_8/attention/self/Softmax:0 -25165824
1608802723911614 bert/encoder/layer_8/attention/self/Softmax:t0 -25165824
1608802723911624 gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul_1:0 25165824
1608802723912002 bert/encoder/layer_8/attention/self/MatMul_1:0 12582912
1608802723912002 bert/encoder/layer_8/attention/self/MatMul_1:t0 3072
1608802723912002 bert/encoder/layer_8/attention/self/MatMul_1:t1 3072
1608802723912002 bert/encoder/layer_8/attention/self/MatMul_1:t2 3072
1608802723912227 bert/encoder/layer_8/attention/self/MatMul_1:t0 -3072
1608802723912227 bert/encoder/layer_8/attention/self/MatMul_1:t1 -3072
1608802723912227 bert/encoder/layer_8/attention/self/MatMul_1:t2 -3072
1608802723912239 bert/encoder/layer_8/attention/self/transpose_3:0 12582912
1608802723912290 bert/encoder/layer_8/attention/self/MatMul_1:0 -12582912
1608802723912318 bert/encoder/layer_8/attention/output/dense/MatMul:0 12582912
1608802723912559 bert/encoder/layer_8/attention/output/dropout/mul:0 -12582912
1608802723912718 bert/encoder/layer_8/attention/output/LayerNorm/moments/mean:0 16384
1608802723912823 bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723912969 bert/encoder/layer_8/attention/output/LayerNorm/moments/variance:0 16384
1608802723913013 bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723913097 bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723913202 bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723913394 bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723914164 bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723914168 bert/encoder/layer_8/intermediate/dense/MatMul:0 50331648
1608802723914301 bert/encoder/layer_8/intermediate/dense/Pow:0 50331648
1608802723914363 gradients/bert/encoder/layer_8/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723914417 gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723914696 bert/encoder/layer_8/intermediate/dense/add_1:0 50331648
1608802723917101 bert/encoder/layer_8/output/dense/MatMul:0 12582912
1608802723917863 bert/encoder/layer_8/output/dropout/mul:0 -12582912
1608802723918065 bert/encoder/layer_8/output/LayerNorm/moments/mean:0 16384
1608802723918269 bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723918548 bert/encoder/layer_8/output/LayerNorm/moments/variance:0 16384
1608802723918738 bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723919143 bert/encoder/layer_8/output/LayerNorm/batchnorm/mul:0 12582912
1608802723919349 bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723919637 bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723922242 bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723922247 bert/encoder/layer_9/attention/self/query/MatMul:0 12582912
1608802723922327 bert/encoder/layer_9/attention/self/key/MatMul:0 12582912
1608802723922378 bert/encoder/layer_9/attention/self/value/MatMul:0 12582912
1608802723922586 bert/encoder/layer_9/attention/self/transpose:0 12582912
1608802723922633 bert/encoder/layer_9/attention/self/query/MatMul:0 -12582912
1608802723922641 bert/encoder/layer_9/attention/self/transpose_1:0 12582912
1608802723922680 bert/encoder/layer_9/attention/self/key/MatMul:0 -12582912
1608802723922693 bert/encoder/layer_9/attention/self/transpose_2:0 12582912
1608802723922733 bert/encoder/layer_9/attention/self/value/MatMul:0 -12582912
1608802723922740 bert/encoder/layer_9/attention/self/MatMul:0 25165824
1608802723922740 bert/encoder/layer_9/attention/self/MatMul:t0 3072
1608802723922740 bert/encoder/layer_9/attention/self/MatMul:t1 3072
1608802723922740 bert/encoder/layer_9/attention/self/MatMul:t2 3072
1608802723924046 bert/encoder/layer_9/attention/self/MatMul:t0 -3072
1608802723924046 bert/encoder/layer_9/attention/self/MatMul:t1 -3072
1608802723924046 bert/encoder/layer_9/attention/self/MatMul:t2 -3072
1608802723924729 bert/encoder/layer_9/attention/self/Softmax:0 25165824
1608802723924729 bert/encoder/layer_9/attention/self/Softmax:t0 25165824
1608802723925044 bert/encoder/layer_9/attention/self/Softmax:0 -25165824
1608802723925044 bert/encoder/layer_9/attention/self/Softmax:t0 -25165824
1608802723925057 gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul_1:0 25165824
1608802723925430 bert/encoder/layer_9/attention/self/MatMul_1:0 12582912
1608802723925430 bert/encoder/layer_9/attention/self/MatMul_1:t0 3072
1608802723925430 bert/encoder/layer_9/attention/self/MatMul_1:t1 3072
1608802723925430 bert/encoder/layer_9/attention/self/MatMul_1:t2 3072
1608802723925640 bert/encoder/layer_9/attention/self/MatMul_1:t0 -3072
1608802723925640 bert/encoder/layer_9/attention/self/MatMul_1:t1 -3072
1608802723925640 bert/encoder/layer_9/attention/self/MatMul_1:t2 -3072
1608802723925650 bert/encoder/layer_9/attention/self/transpose_3:0 12582912
1608802723925699 bert/encoder/layer_9/attention/self/MatMul_1:0 -12582912
1608802723925722 bert/encoder/layer_9/attention/output/dense/MatMul:0 12582912
1608802723925992 bert/encoder/layer_9/attention/output/dropout/mul:0 -12582912
1608802723926150 bert/encoder/layer_9/attention/output/LayerNorm/moments/mean:0 16384
1608802723926256 bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723926401 bert/encoder/layer_9/attention/output/LayerNorm/moments/variance:0 16384
1608802723926442 bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723926522 bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723926657 bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723926839 bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723927612 bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723927617 bert/encoder/layer_9/intermediate/dense/MatMul:0 50331648
1608802723927738 bert/encoder/layer_9/intermediate/dense/Pow:0 50331648
1608802723927795 gradients/bert/encoder/layer_9/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723927852 gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723928107 bert/encoder/layer_9/intermediate/dense/add_1:0 50331648
1608802723930549 bert/encoder/layer_9/output/dense/MatMul:0 12582912
1608802723931312 bert/encoder/layer_9/output/dropout/mul:0 -12582912
1608802723931508 bert/encoder/layer_9/output/LayerNorm/moments/mean:0 16384
1608802723931708 bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723932001 bert/encoder/layer_9/output/LayerNorm/moments/variance:0 16384
1608802723932184 bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723932594 bert/encoder/layer_9/output/LayerNorm/batchnorm/mul:0 12582912
1608802723932812 bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723933076 bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723935688 bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723935693 bert/encoder/layer_10/attention/self/query/MatMul:0 12582912
1608802723935773 bert/encoder/layer_10/attention/self/key/MatMul:0 12582912
1608802723935830 bert/encoder/layer_10/attention/self/value/MatMul:0 12582912
1608802723936048 bert/encoder/layer_10/attention/self/transpose:0 12582912
1608802723936087 bert/encoder/layer_10/attention/self/query/MatMul:0 -12582912
1608802723936096 bert/encoder/layer_10/attention/self/transpose_1:0 12582912
1608802723936139 bert/encoder/layer_10/attention/self/key/MatMul:0 -12582912
1608802723936146 bert/encoder/layer_10/attention/self/transpose_2:0 12582912
1608802723936187 bert/encoder/layer_10/attention/self/value/MatMul:0 -12582912
1608802723936196 bert/encoder/layer_10/attention/self/MatMul:0 25165824
1608802723936196 bert/encoder/layer_10/attention/self/MatMul:t0 3072
1608802723936196 bert/encoder/layer_10/attention/self/MatMul:t1 3072
1608802723936196 bert/encoder/layer_10/attention/self/MatMul:t2 3072
1608802723937490 bert/encoder/layer_10/attention/self/MatMul:t0 -3072
1608802723937490 bert/encoder/layer_10/attention/self/MatMul:t1 -3072
1608802723937490 bert/encoder/layer_10/attention/self/MatMul:t2 -3072
1608802723938173 bert/encoder/layer_10/attention/self/Softmax:0 25165824
1608802723938173 bert/encoder/layer_10/attention/self/Softmax:t0 25165824
1608802723938487 bert/encoder/layer_10/attention/self/Softmax:0 -25165824
1608802723938487 bert/encoder/layer_10/attention/self/Softmax:t0 -25165824
1608802723938495 gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul_1:0 25165824
1608802723938873 bert/encoder/layer_10/attention/self/MatMul_1:0 12582912
1608802723938873 bert/encoder/layer_10/attention/self/MatMul_1:t0 3072
1608802723938873 bert/encoder/layer_10/attention/self/MatMul_1:t1 3072
1608802723938873 bert/encoder/layer_10/attention/self/MatMul_1:t2 3072
1608802723939059 bert/encoder/layer_10/attention/self/MatMul_1:t0 -3072
1608802723939059 bert/encoder/layer_10/attention/self/MatMul_1:t1 -3072
1608802723939059 bert/encoder/layer_10/attention/self/MatMul_1:t2 -3072
1608802723939071 bert/encoder/layer_10/attention/self/transpose_3:0 12582912
1608802723939138 bert/encoder/layer_10/attention/self/MatMul_1:0 -12582912
1608802723939159 bert/encoder/layer_10/attention/output/dense/MatMul:0 12582912
1608802723939431 bert/encoder/layer_10/attention/output/dropout/mul:0 -12582912
1608802723939590 bert/encoder/layer_10/attention/output/LayerNorm/moments/mean:0 16384
1608802723939693 bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723939837 bert/encoder/layer_10/attention/output/LayerNorm/moments/variance:0 16384
1608802723939874 bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723939948 bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723940072 bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723940263 bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723941033 bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723941038 bert/encoder/layer_10/intermediate/dense/MatMul:0 50331648
1608802723941170 bert/encoder/layer_10/intermediate/dense/Pow:0 50331648
1608802723941218 gradients/bert/encoder/layer_10/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723941271 gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723941511 bert/encoder/layer_10/intermediate/dense/add_1:0 50331648
1608802723943978 bert/encoder/layer_10/output/dense/MatMul:0 12582912
1608802723944740 bert/encoder/layer_10/output/dropout/mul:0 -12582912
1608802723944939 bert/encoder/layer_10/output/LayerNorm/moments/mean:0 16384
1608802723945142 bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723945421 bert/encoder/layer_10/output/LayerNorm/moments/variance:0 16384
1608802723945612 bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723946020 bert/encoder/layer_10/output/LayerNorm/batchnorm/mul:0 12582912
1608802723946222 bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723946507 bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723949120 bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723949125 bert/encoder/layer_11/attention/self/query/MatMul:0 12582912
1608802723949200 bert/encoder/layer_11/attention/self/key/MatMul:0 12582912
1608802723949249 bert/encoder/layer_11/attention/self/value/MatMul:0 12582912
1608802723949439 bert/encoder/layer_11/attention/self/transpose:0 12582912
1608802723949482 bert/encoder/layer_11/attention/self/query/MatMul:0 -12582912
1608802723949490 bert/encoder/layer_11/attention/self/transpose_1:0 12582912
1608802723949524 bert/encoder/layer_11/attention/self/key/MatMul:0 -12582912
1608802723949536 bert/encoder/layer_11/attention/self/transpose_2:0 12582912
1608802723949572 bert/encoder/layer_11/attention/self/value/MatMul:0 -12582912
1608802723949579 bert/encoder/layer_11/attention/self/MatMul:0 25165824
1608802723949579 bert/encoder/layer_11/attention/self/MatMul:t0 3072
1608802723949579 bert/encoder/layer_11/attention/self/MatMul:t1 3072
1608802723949579 bert/encoder/layer_11/attention/self/MatMul:t2 3072
1608802723950915 bert/encoder/layer_11/attention/self/MatMul:t0 -3072
1608802723950915 bert/encoder/layer_11/attention/self/MatMul:t1 -3072
1608802723950915 bert/encoder/layer_11/attention/self/MatMul:t2 -3072
1608802723951598 bert/encoder/mul:0 -2097152
1608802723951602 bert/encoder/layer_11/attention/self/Softmax:0 25165824
1608802723951602 bert/encoder/layer_11/attention/self/Softmax:t0 25165824
1608802723951914 bert/encoder/layer_11/attention/self/Softmax:0 -25165824
1608802723951914 bert/encoder/layer_11/attention/self/Softmax:t0 -25165824
1608802723951923 gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul_1:0 25165824
1608802723952300 bert/encoder/layer_11/attention/self/MatMul_1:0 12582912
1608802723952300 bert/encoder/layer_11/attention/self/MatMul_1:t0 3072
1608802723952300 bert/encoder/layer_11/attention/self/MatMul_1:t1 3072
1608802723952300 bert/encoder/layer_11/attention/self/MatMul_1:t2 3072
1608802723952487 bert/encoder/layer_11/attention/self/MatMul_1:t0 -3072
1608802723952487 bert/encoder/layer_11/attention/self/MatMul_1:t1 -3072
1608802723952487 bert/encoder/layer_11/attention/self/MatMul_1:t2 -3072
1608802723952496 bert/encoder/layer_11/attention/self/transpose_3:0 12582912
1608802723952565 bert/encoder/layer_11/attention/self/MatMul_1:0 -12582912
1608802723952586 bert/encoder/layer_11/attention/output/dense/MatMul:0 12582912
1608802723952857 bert/encoder/layer_11/attention/output/dropout/mul:0 -12582912
1608802723953016 bert/encoder/layer_11/attention/output/LayerNorm/moments/mean:0 16384
1608802723953119 bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723953264 bert/encoder/layer_11/attention/output/LayerNorm/moments/variance:0 16384
1608802723953300 bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723953377 bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul:0 12582912
1608802723953512 bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723953702 bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723954475 bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723954481 bert/encoder/layer_11/intermediate/dense/MatMul:0 50331648
1608802723954600 bert/encoder/layer_11/intermediate/dense/Pow:0 50331648
1608802723954652 gradients/bert/encoder/layer_11/intermediate/dense/mul_3_grad/Mul_1:0 50331648
1608802723954700 gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/Pow:0 50331648
1608802723954923 bert/encoder/layer_11/intermediate/dense/add_1:0 50331648
1608802723957409 bert/encoder/layer_11/output/dense/MatMul:0 12582912
1608802723958172 bert/encoder/layer_11/output/dropout/mul:0 -12582912
1608802723958366 bert/encoder/layer_11/output/LayerNorm/moments/mean:0 16384
1608802723958566 bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference:0 12582912
1608802723958860 bert/encoder/layer_11/output/LayerNorm/moments/variance:0 16384
1608802723959045 bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference:0 -12582912
1608802723959456 bert/encoder/layer_11/output/LayerNorm/batchnorm/mul:0 12582912
1608802723959657 bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2:0 12582912
1608802723959935 bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1:0 12582912
1608802723962543 bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2:0 -12582912
1608802723962564 GatherV2:0 1966080
1608802723962629 bert/pooler/strided_slice:0 98304
1608802723962687 bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802723962694 cls/predictions/transform/dense/MatMul:0 1966080
1608802723962850 bert/pooler/dense/MatMul:0 98304
1608802723962896 cls/predictions/transform/dense/Pow:0 1966080
1608802723962942 gradients/cls/predictions/transform/dense/Pow_grad/Pow:0 1966080
1608802723963684 cls/seq_relationship/MatMul:0 256
1608802723965086 cls/seq_relationship/LogSoftmax:0 256
1608802723965086 cls/seq_relationship/LogSoftmax:t0 256
1608802723965530 cls/seq_relationship/LogSoftmax:0 -256
1608802723965530 cls/seq_relationship/LogSoftmax:t0 -256
1608802723965534 cls/predictions/transform/dense/add_1:0 2818048
1608802723965733 gradients/cls/seq_relationship/LogSoftmax_grad/Exp:0 256
1608802723965809 cls/seq_relationship/one_hot:0 -256
1608802723965885 gradients/cls/seq_relationship/LogSoftmax_grad/Sum:0 -256
1608802723965889 cls/seq_relationship/Sum:0 256
1608802723965986 cls/seq_relationship/MatMul:0 -256
1608802723965992 cls/predictions/transform/dense/mul_3:0 1966080
1608802723966177 gradients/cls/seq_relationship/LogSoftmax_grad/Exp:0 -256
1608802723966284 cls/predictions/transform/LayerNorm/moments/mean:0 2560
1608802723966442 gradients/cls/seq_relationship/BiasAdd_grad/BiasAddGrad:0 256
1608802723966543 gradients/cls/seq_relationship/MatMul_grad/MatMul:0 98304
1608802723966680 gradients/cls/seq_relationship/MatMul_grad/MatMul_1:0 6144
1608802723966736 gradients/cls/seq_relationship/mul_grad/Mul_1:0 -256
1608802723966740 cls/seq_relationship/Mean:0 256
1608802723966778 cls/seq_relationship/Sum:0 -256
1608802723966786 cls/predictions/transform/LayerNorm/moments/SquaredDifference:0 1966080
1608802723966829 global_norm/L2Loss_205:0 256
1608802723967113 gradients/cls/seq_relationship/MatMul_grad/MatMul:0 -98304
1608802723967123 global_norm/L2Loss_204:0 256
1608802723967751 cls/predictions/transform/LayerNorm/moments/variance:0 2560
1608802723967816 cls/predictions/transform/LayerNorm/moments/SquaredDifference:0 -1966080
1608802723967819 gradients/bert/pooler/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802723967871 gradients/bert/pooler/dense/MatMul_grad/MatMul:0 98304
1608802723967920 gradients/bert/pooler/dense/MatMul_grad/MatMul_1:0 2359296
1608802723967984 bert/pooler/strided_slice:0 -98304
1608802723967984 bert/pooler/dense/MatMul:0 -98304
1608802723968036 global_norm/L2Loss_198:0 256
1608802723968084 global_norm/L2Loss_197:0 256
1608802723968084 global_norm/L2Loss_197:t0 9216
1608802723968164 global_norm/L2Loss_197:t0 -9216
1608802723968206 gradients/bert/pooler/strided_slice_grad/StridedSliceGrad:0 12582912
1608802723968286 gradients/bert/pooler/dense/MatMul_grad/MatMul:0 -98304
1608802723968295 cls/predictions/transform/LayerNorm/batchnorm/mul:0 1966080
1608802723968339 cls/predictions/transform/LayerNorm/batchnorm/mul_2:0 1966080
1608802723968399 cls/predictions/transform/LayerNorm/batchnorm/mul_1:0 1966080
1608802723970844 cls/predictions/transform/LayerNorm/batchnorm/mul_2:0 -1966080
1608802723970849 cls/predictions/MatMul:0 78136320
1608802723971406 cls/predictions/LogSoftmax:0 78136320
1608802723971406 cls/predictions/LogSoftmax:t0 78136320
1608802723971406 cls/predictions/LogSoftmax:t1 256
1608802723971406 cls/predictions/LogSoftmax:t2 256
1608802723971993 cls/predictions/LogSoftmax:0 -78136320
1608802723971993 cls/predictions/LogSoftmax:t0 -78136320
1608802723971993 cls/predictions/LogSoftmax:t1 -256
1608802723971993 cls/predictions/LogSoftmax:t2 -256
1608802723972002 gradients/cls/predictions/LogSoftmax_grad/Exp:0 78136320
1608802723972485 cls/predictions/one_hot:0 -93769728
1608802723972685 gradients/cls/predictions/LogSoftmax_grad/Sum:0 -2560
1608802723972689 cls/predictions/Sum:0 2560
1608802723972689 cls/predictions/Sum:t0 256
1608802723972892 cls/predictions/MatMul:0 -78136320
1608802723972892 cls/predictions/Sum:t0 -256
1608802723973081 gradients/cls/predictions/LogSoftmax_grad/Exp:0 -78136320
1608802723973364 gradients/cls/predictions/BiasAdd_grad/BiasAddGrad:0 122112
1608802723975901 gradients/cls/predictions/MatMul_grad/MatMul:0 1966080
1608802723975973 gradients/cls/predictions/MatMul_grad/MatMul_1:0 93763584
1608802723976019 gradients/cls/predictions/Sum_grad/Tile:0 -78136320
1608802723976019 cls/predictions/transform/LayerNorm/batchnorm/mul_1:0 -1966080
1608802723976082 global_norm/L2Loss_203:0 256
1608802723976082 global_norm/L2Loss_203:t0 256
1608802723976150 global_norm/L2Loss_203:t0 -256
1608802723976154 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_1_grad/Mul:0 3810048
1608802723976192 gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Neg:0 1966080
1608802723976234 gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802723976318 cls/predictions/Sum_1:0 256
1608802723976350 cls/predictions/Sum:0 -2560
1608802723976443 global_norm/L2Loss_201:0 256
1608802723977132 cls/predictions/Sum_2:0 -256
1608802723977137 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Sum:0 2560
1608802723977763 cls/predictions/transform/LayerNorm/batchnorm/mul:0 -1966080
1608802723978398 gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Neg:0 -1966080
1608802723978463 cls/seq_relationship/Mean:0 -256
1608802723978477 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Mul:0 1966080
1608802723978660 gradients/cls/predictions/transform/LayerNorm/moments/mean_grad/Tile:0 1966080
1608802723978790 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Sum:0 -2560
1608802723978798 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum:0 3072
1608802723978972 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Mul:0 -1966080
1608802723978978 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802723979161 gradients/cls/predictions/MatMul_grad/MatMul:0 -1966080
1608802723979219 global_norm/L2Loss_202:0 256
1608802723979289 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum:0 -3072
1608802723979294 gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/Tile:0 1966080
1608802723979331 cls/predictions/transform/LayerNorm/moments/variance:0 -2560
1608802723979561 cls/predictions/transform/LayerNorm/moments/mean:0 -2560
1608802723979718 cls/predictions/transform/dense/mul_3:0 -1966080
1608802723979884 gradients/cls/predictions/transform/LayerNorm/moments/mean_grad/Tile:0 -1966080
1608802723979884 gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/Tile:0 -1966080
1608802723979889 gradients/cls/predictions/transform/dense/mul_3_grad/Mul_1:0 1966080
1608802723980119 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_1_grad/Mul:0 -3810048
1608802723980158 gradients/cls/predictions/transform/dense/mul_3_grad/Mul_1:0 -1966080
1608802723980190 cls/predictions/transform/dense/MatMul:0 -1966080
1608802723980549 cls/predictions/transform/dense/Pow:0 -1966080
1608802723980549 cls/predictions/transform/dense/add_1:0 -2818048
1608802723980559 gradients/cls/predictions/transform/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802723981180 gradients/cls/predictions/transform/dense/MatMul_grad/MatMul:0 1966080
1608802723981250 gradients/cls/predictions/transform/dense/MatMul_grad/MatMul_1:0 2818048
1608802723981302 GatherV2:0 -1966080
1608802723981302 gradients/cls/predictions/transform/dense/Pow_grad/Pow:0 -1966080
1608802723981314 global_norm/L2Loss_200:0 256
1608802723981349 gradients/Reshape_2_grad/Reshape/tensor:0 12582912
1608802723981429 gradients/cls/predictions/transform/dense/MatMul_grad/MatMul:0 -1966080
1608802723981437 global_norm/L2Loss_199:0 256
1608802723981437 global_norm/L2Loss_199:t0 9216
1608802723981518 global_norm/L2Loss_199:t0 -9216
1608802723981577 gradients/bert/pooler/strided_slice_grad/StridedSliceGrad:0 -12582912
1608802723981591 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 14548992
1608802723981634 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802723981674 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Sum:0 3584
1608802723981836 global_norm/L2Loss_195:0 256
1608802723984031 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802723984263 bert/encoder/layer_11/output/LayerNorm/batchnorm/mul:0 -12582912
1608802723984627 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802723984645 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802723985024 gradients/bert/encoder/layer_11/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802723985217 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802723985226 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802723985420 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802723985425 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802723985697 gradients/Reshape_2_grad/Reshape/tensor:0 -12582912
1608802723985908 global_norm/L2Loss_196:0 256
1608802723986304 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802723986308 gradients/bert/encoder/layer_11/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802723986495 bert/encoder/layer_11/output/LayerNorm/moments/variance:0 -16384
1608802723989257 bert/encoder/layer_11/output/LayerNorm/moments/mean:0 -16384
1608802723989395 bert/encoder/layer_11/output/dense/MatMul:0 -12582912
1608802723989466 gradients/bert/encoder/layer_11/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802723989466 gradients/bert/encoder/layer_11/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802723989472 gradients/bert/encoder/layer_11/output/dropout/mul_1_grad/Mul:0 12582912
1608802723989560 gradients/bert/encoder/layer_11/output/dropout/mul_1_grad/Mul:0 -12582912
1608802723989564 gradients/bert/encoder/layer_11/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802723989615 gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul:0 78136320
1608802723989660 gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802723989704 bert/encoder/layer_11/output/dropout/Cast:0 -12582912
1608802723989704 bert/encoder/layer_11/intermediate/dense/MatMul:0 -50331648
1608802723989710 global_norm/L2Loss_194:0 256
1608802723989812 gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul:0 -78136320
1608802723989815 global_norm/L2Loss_193:0 256
1608802723989815 global_norm/L2Loss_193:t0 13312
1608802723989901 global_norm/L2Loss_193:t0 -13312
1608802723990531 gradients/bert/encoder/layer_11/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802723991873 bert/encoder/layer_11/intermediate/dense/Pow:0 -50331648
1608802723991873 bert/encoder/layer_11/intermediate/dense/add_1:0 -50331648
1608802723991876 gradients/bert/encoder/layer_11/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 13312
1608802723991933 gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802723992006 gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802723992187 bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802723992187 gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802723992194 global_norm/L2Loss_192:0 256
1608802723992571 gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802723992575 global_norm/L2Loss_191:0 256
1608802723992575 global_norm/L2Loss_191:t0 9216
1608802723992639 global_norm/L2Loss_191:t0 -9216
1608802723992655 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802723992694 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802723992733 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802723993130 global_norm/L2Loss_189:0 256
1608802723993280 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802723993381 bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802723993526 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802723993540 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802723993614 gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802723993644 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802723993654 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802723993758 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802723993763 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802723993949 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -14548992
1608802723994605 global_norm/L2Loss_190:0 256
1608802723994722 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802723994726 gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802723994798 bert/encoder/layer_11/attention/output/LayerNorm/moments/variance:0 -16384
1608802723994888 bert/encoder/layer_11/attention/output/LayerNorm/moments/mean:0 -16384
1608802723994959 bert/encoder/layer_11/attention/output/dense/MatMul:0 -12582912
1608802723994994 gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802723994994 gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802723994998 gradients/bert/encoder/layer_11/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802723995066 gradients/bert/encoder/layer_11/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802723995071 gradients/bert/encoder/layer_11/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802723995120 gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802723995167 gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul_1:0 3810048
1608802723995219 bert/encoder/layer_11/attention/output/dropout/Cast:0 -12582912
1608802723995219 bert/encoder/layer_11/attention/self/transpose_3:0 -12582912
1608802723995226 global_norm/L2Loss_188:0 256
1608802723995272 global_norm/L2Loss_187:0 256
1608802723995272 global_norm/L2Loss_187:t0 12800
1608802723997663 global_norm/L2Loss_187:t0 -12800
1608802723997666 gradients/bert/encoder/layer_11/attention/self/transpose_3_grad/transpose:0 12582912
1608802723998032 gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802723998040 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802723998040 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802723998040 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802723998040 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802723998832 bert/encoder/layer_11/attention/self/transpose_2:0 -12582912
1608802723998832 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802723998832 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802723998832 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802723998841 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802723998841 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802723998841 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802723998841 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802723999720 bert/encoder/layer_11/attention/self/dropout/mul:0 -25165824
1608802723999720 gradients/bert/encoder/layer_11/attention/self/transpose_3_grad/transpose:0 -12582912
1608802723999720 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802723999720 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802723999720 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802723999912 gradients/bert/encoder/layer_11/attention/self/transpose_2_grad/transpose:0 12582912
1608802724000189 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724002668 gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724002728 gradients/bert/encoder/layer_11/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724002799 gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724002878 gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724002932 gradients/bert/encoder/layer_11/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724002938 gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum:0 196608
1608802724002975 bert/encoder/layer_11/attention/self/MatMul:0 -25165824
1608802724002982 global_norm/L2Loss_186:0 256
1608802724003012 global_norm/L2Loss_185:0 256
1608802724003012 global_norm/L2Loss_185:t0 9216
1608802724003070 global_norm/L2Loss_185:t0 -9216
1608802724003103 gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum:0 -196608
1608802724003132 bert/encoder/layer_11/attention/self/dropout/Cast:0 -25165824
1608802724003138 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:0 12582912
1608802724003138 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:t0 3072
1608802724003138 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:t1 3072
1608802724003138 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:t2 3072
1608802724003341 bert/encoder/layer_11/attention/self/transpose_1:0 -12582912
1608802724003341 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724003341 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724003341 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724003350 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724003350 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724003350 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724003350 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724005307 bert/encoder/layer_11/attention/self/transpose:0 -12582912
1608802724005307 gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724005307 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724005307 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724005307 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724005314 gradients/bert/encoder/layer_11/attention/self/transpose_grad/transpose:0 14548992
1608802724005360 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724005369 gradients/bert/encoder/layer_11/attention/self/transpose_1_grad/transpose:0 12582912
1608802724005420 gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724005443 gradients/bert/encoder/layer_11/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724005609 gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724005802 gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724005986 gradients/bert/encoder/layer_11/attention/self/transpose_grad/transpose:0 -14548992
1608802724005992 gradients/bert/encoder/layer_11/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724006033 gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul:0 14548992
1608802724006070 gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul_1:0 3407872
1608802724006116 bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724006116 gradients/bert/encoder/layer_11/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724006123 global_norm/L2Loss_182:0 256
1608802724006156 global_norm/L2Loss_181:0 256
1608802724006156 global_norm/L2Loss_181:t0 9216
1608802724006251 global_norm/L2Loss_181:t0 -9216
1608802724006254 global_norm/L2Loss_184:0 256
1608802724006442 gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724006442 gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724006442 gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul:0 -14548992
1608802724006447 global_norm/L2Loss_183:0 256
1608802724006447 global_norm/L2Loss_183:t0 9216
1608802724006703 global_norm/L2Loss_183:t0 -9216
1608802724006707 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724006816 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724006952 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Neg:0 14548992
1608802724006988 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724007021 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724007184 global_norm/L2Loss_179:0 256
1608802724007377 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724008011 bert/encoder/layer_10/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724008077 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Neg:0 -14548992
1608802724008088 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724008228 gradients/bert/encoder/layer_10/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724008260 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724008270 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724008314 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724008317 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724008352 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724008409 global_norm/L2Loss_180:0 256
1608802724008469 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724008472 gradients/bert/encoder/layer_10/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724008503 bert/encoder/layer_10/output/LayerNorm/moments/variance:0 -16384
1608802724008592 bert/encoder/layer_10/output/LayerNorm/moments/mean:0 -16384
1608802724010860 bert/encoder/layer_10/output/dense/MatMul:0 -12582912
1608802724011086 gradients/bert/encoder/layer_10/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724011086 gradients/bert/encoder/layer_10/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724011095 gradients/bert/encoder/layer_10/output/dropout/mul_1_grad/Mul:0 12582912
1608802724011650 gradients/bert/encoder/layer_10/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724011654 gradients/bert/encoder/layer_10/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724011849 gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul:0 78136320
1608802724012046 gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724012237 bert/encoder/layer_10/output/dropout/Cast:0 -12582912
1608802724012237 bert/encoder/layer_10/intermediate/dense/MatMul:0 -50331648
1608802724012243 global_norm/L2Loss_178:0 256
1608802724012925 gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul:0 -78136320
1608802724012931 global_norm/L2Loss_177:0 256
1608802724012931 global_norm/L2Loss_177:t0 13312
1608802724013319 global_norm/L2Loss_177:t0 -13312
1608802724013599 gradients/bert/encoder/layer_10/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724016215 bert/encoder/layer_10/intermediate/dense/Pow:0 -50331648
1608802724016215 bert/encoder/layer_10/intermediate/dense/add_1:0 -50331648
1608802724016222 gradients/bert/encoder/layer_10/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 13312
1608802724016265 gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724016309 gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724016341 bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724016341 gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724016352 global_norm/L2Loss_176:0 256
1608802724016428 gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724016431 global_norm/L2Loss_175:0 256
1608802724016431 global_norm/L2Loss_175:t0 9216
1608802724016491 global_norm/L2Loss_175:t0 -9216
1608802724016494 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724016528 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724016564 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 14548992
1608802724016615 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724016665 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724017377 global_norm/L2Loss_173:0 256
1608802724018009 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 17920
1608802724018643 bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724018709 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -14548992
1608802724018723 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724018842 gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724019021 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -17920
1608802724019033 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 17920
1608802724019215 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724019218 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724019405 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724019448 global_norm/L2Loss_174:0 256
1608802724019501 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -17920
1608802724019504 gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724019567 bert/encoder/layer_10/attention/output/LayerNorm/moments/variance:0 -16384
1608802724019804 bert/encoder/layer_10/attention/output/LayerNorm/moments/mean:0 -16384
1608802724019962 bert/encoder/layer_10/attention/output/dense/MatMul:0 -12582912
1608802724020124 gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724020124 gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724020127 gradients/bert/encoder/layer_10/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724020373 gradients/bert/encoder/layer_10/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724020377 gradients/bert/encoder/layer_10/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724020425 gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724020467 gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul_1:0 2359296
1608802724020517 bert/encoder/layer_10/attention/output/dropout/Cast:0 -12582912
1608802724020517 bert/encoder/layer_10/attention/self/transpose_3:0 -12582912
1608802724020523 global_norm/L2Loss_172:0 256
1608802724020597 global_norm/L2Loss_171:0 256
1608802724020597 global_norm/L2Loss_171:t0 14336
1608802724021418 global_norm/L2Loss_171:t0 -14336
1608802724021423 gradients/bert/encoder/layer_10/attention/self/transpose_3_grad/transpose:0 12582912
1608802724021482 gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724021488 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724021488 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724021488 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724021488 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724021728 bert/encoder/layer_10/attention/self/transpose_2:0 -12582912
1608802724021728 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724021728 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724021728 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724021735 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724021735 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724021735 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724021735 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724021864 bert/encoder/layer_10/attention/self/dropout/mul:0 -25165824
1608802724021864 gradients/bert/encoder/layer_10/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724021864 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724021864 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724021864 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724021908 gradients/bert/encoder/layer_10/attention/self/transpose_2_grad/transpose:0 12582912
1608802724021938 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724021993 gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724022070 gradients/bert/encoder/layer_10/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724022109 gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724024282 gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724024502 gradients/bert/encoder/layer_10/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724024509 gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum:0 196608
1608802724024866 bert/encoder/layer_10/attention/self/MatMul:0 -25165824
1608802724024871 global_norm/L2Loss_170:0 256
1608802724025061 global_norm/L2Loss_169:0 256
1608802724025061 global_norm/L2Loss_169:t0 10752
1608802724025454 global_norm/L2Loss_169:t0 -10752
1608802724025651 gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum:0 -196608
1608802724025932 bert/encoder/layer_10/attention/self/dropout/Cast:0 -25165824
1608802724025936 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:0 12582912
1608802724025936 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:t0 3072
1608802724025936 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:t1 3072
1608802724025936 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:t2 4608
1608802724026743 bert/encoder/layer_10/attention/self/transpose_1:0 -12582912
1608802724026743 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724026743 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724026743 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:t2 -4608
1608802724026751 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724026751 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724026751 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724026751 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:t2 4608
1608802724029642 bert/encoder/layer_10/attention/self/transpose:0 -12582912
1608802724029642 gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724029642 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724029642 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724029642 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:t2 -4608
1608802724029650 gradients/bert/encoder/layer_10/attention/self/transpose_grad/transpose:0 12582912
1608802724029708 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724029714 gradients/bert/encoder/layer_10/attention/self/transpose_1_grad/transpose:0 12582912
1608802724029743 gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724029766 gradients/bert/encoder/layer_10/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724029806 gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724029842 gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724029886 gradients/bert/encoder/layer_10/attention/self/transpose_grad/transpose:0 -12582912
1608802724029892 gradients/bert/encoder/layer_10/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724029927 gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724029960 gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724030002 bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724030002 gradients/bert/encoder/layer_10/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724030008 global_norm/L2Loss_166:0 256
1608802724030038 global_norm/L2Loss_165:0 256
1608802724030038 global_norm/L2Loss_165:t0 9216
1608802724030092 global_norm/L2Loss_165:t0 -9216
1608802724030095 global_norm/L2Loss_168:0 256
1608802724030170 gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724030170 gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724030170 gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724030175 global_norm/L2Loss_167:0 256
1608802724030175 global_norm/L2Loss_167:t0 9216
1608802724030330 global_norm/L2Loss_167:t0 -9216
1608802724030333 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724030372 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724030404 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724030438 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Sum:0 3584
1608802724030472 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724030537 global_norm/L2Loss_163:0 256
1608802724030568 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 29440
1608802724030593 bert/encoder/layer_9/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724030627 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724030639 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724030705 gradients/bert/encoder/layer_9/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724030731 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -29440
1608802724030741 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum:0 29440
1608802724030771 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724030778 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724030806 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724030848 global_norm/L2Loss_164:0 256
1608802724030904 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum:0 -29440
1608802724030908 gradients/bert/encoder/layer_9/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724030937 bert/encoder/layer_9/output/LayerNorm/moments/variance:0 -16384
1608802724031012 bert/encoder/layer_9/output/LayerNorm/moments/mean:0 -16384
1608802724031068 bert/encoder/layer_9/output/dense/MatMul:0 -12582912
1608802724031101 gradients/bert/encoder/layer_9/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724031101 gradients/bert/encoder/layer_9/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724031104 gradients/bert/encoder/layer_9/output/dropout/mul_1_grad/Mul:0 12582912
1608802724031165 gradients/bert/encoder/layer_9/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724031169 gradients/bert/encoder/layer_9/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724031212 gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul:0 78136320
1608802724031253 gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724031284 bert/encoder/layer_9/output/dropout/Cast:0 -12582912
1608802724031284 bert/encoder/layer_9/intermediate/dense/MatMul:0 -50331648
1608802724031290 global_norm/L2Loss_162:0 256
1608802724031375 gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul:0 -78136320
1608802724031380 global_norm/L2Loss_161:0 256
1608802724031380 global_norm/L2Loss_161:t0 9216
1608802724031441 global_norm/L2Loss_161:t0 -9216
1608802724031474 gradients/bert/encoder/layer_9/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724031562 bert/encoder/layer_9/intermediate/dense/Pow:0 -50331648
1608802724031562 bert/encoder/layer_9/intermediate/dense/add_1:0 -50331648
1608802724031567 gradients/bert/encoder/layer_9/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 12288
1608802724031606 gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724031657 gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724031693 bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724031693 gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724031699 global_norm/L2Loss_160:0 256
1608802724031859 gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724031862 global_norm/L2Loss_159:0 256
1608802724031862 global_norm/L2Loss_159:t0 13056
1608802724031917 global_norm/L2Loss_159:t0 -13056
1608802724031921 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724031953 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724031982 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724032010 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724032040 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724032108 global_norm/L2Loss_157:0 256
1608802724034405 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 29440
1608802724034740 bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724034896 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724034911 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724035585 gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/mean_grad/Tile:0 14548992
1608802724035897 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -29440
1608802724035905 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 29440
1608802724036328 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724036331 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724036639 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724036807 global_norm/L2Loss_158:0 256
1608802724037252 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -29440
1608802724037255 gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724037382 bert/encoder/layer_9/attention/output/LayerNorm/moments/variance:0 -16384
1608802724044368 bert/encoder/layer_9/attention/output/LayerNorm/moments/mean:0 -16384
1608802724044431 bert/encoder/layer_9/attention/output/dense/MatMul:0 -12582912
1608802724044463 gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/mean_grad/Tile:0 -14548992
1608802724044463 gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724044466 gradients/bert/encoder/layer_9/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724044528 gradients/bert/encoder/layer_9/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724044531 gradients/bert/encoder/layer_9/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724044577 cls/predictions/Sum_1:0 -256
1608802724044580 gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724044774 gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul_1:0 3145728
1608802724044895 bert/encoder/layer_9/attention/output/dropout/Cast:0 -12582912
1608802724044895 bert/encoder/layer_9/attention/self/transpose_3:0 -12582912
1608802724044906 global_norm/L2Loss_156:0 256
1608802724044989 global_norm/L2Loss_155:0 256
1608802724044989 global_norm/L2Loss_155:t0 9216
1608802724045113 global_norm/L2Loss_155:t0 -9216
1608802724045117 gradients/bert/encoder/layer_9/attention/self/transpose_3_grad/transpose:0 12582912
1608802724045182 gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724045193 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724045193 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724045193 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724045193 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724045479 bert/encoder/layer_9/attention/self/transpose_2:0 -12582912
1608802724045479 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724045479 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724045479 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724045492 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724045492 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724045492 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724045492 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724045713 bert/encoder/layer_9/attention/self/dropout/mul:0 -25165824
1608802724045713 gradients/bert/encoder/layer_9/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724045713 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724045713 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724045713 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724045782 gradients/bert/encoder/layer_9/attention/self/transpose_2_grad/transpose:0 12582912
1608802724045829 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724045882 gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724045940 gradients/bert/encoder/layer_9/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724046072 gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724046134 gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724046205 gradients/bert/encoder/layer_9/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724046214 gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum:0 196608
1608802724046276 bert/encoder/layer_9/attention/self/MatMul:0 -25165824
1608802724046284 global_norm/L2Loss_154:0 256
1608802724046328 global_norm/L2Loss_153:0 256
1608802724046328 global_norm/L2Loss_153:t0 9216
1608802724046417 global_norm/L2Loss_153:t0 -9216
1608802724046466 gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum:0 -196608
1608802724046510 bert/encoder/layer_9/attention/self/dropout/Cast:0 -25165824
1608802724046514 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:0 12582912
1608802724046514 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:t0 3072
1608802724046514 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:t1 3072
1608802724046514 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:t2 3072
1608802724046753 bert/encoder/layer_9/attention/self/transpose_1:0 -12582912
1608802724046753 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724046753 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724046753 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724046765 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724046765 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724046765 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724046765 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724046973 bert/encoder/layer_9/attention/self/transpose:0 -12582912
1608802724046973 gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724046973 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724046973 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724046973 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724046985 gradients/bert/encoder/layer_9/attention/self/transpose_grad/transpose:0 14548992
1608802724047038 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724047047 gradients/bert/encoder/layer_9/attention/self/transpose_1_grad/transpose:0 12582912
1608802724047089 gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724047123 gradients/bert/encoder/layer_9/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724047182 gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724047240 gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724047308 gradients/bert/encoder/layer_9/attention/self/transpose_grad/transpose:0 -14548992
1608802724047317 gradients/bert/encoder/layer_9/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724047373 gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul:0 14548992
1608802724047424 gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724047490 bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724047490 gradients/bert/encoder/layer_9/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724047499 global_norm/L2Loss_150:0 256
1608802724047552 global_norm/L2Loss_149:0 256
1608802724047552 global_norm/L2Loss_149:t0 9216
1608802724047648 global_norm/L2Loss_149:t0 -9216
1608802724047653 global_norm/L2Loss_152:0 256
1608802724047736 gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724047736 gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724047736 gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul:0 -14548992
1608802724047746 global_norm/L2Loss_151:0 256
1608802724047746 global_norm/L2Loss_151:t0 9216
1608802724047839 global_norm/L2Loss_151:t0 -9216
1608802724047844 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724047902 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724047954 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Neg:0 14548992
1608802724048011 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724048061 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724048165 global_norm/L2Loss_147:0 256
1608802724048209 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 18432
1608802724048251 bert/encoder/layer_8/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724048300 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Neg:0 -14548992
1608802724048318 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724048448 gradients/bert/encoder/layer_8/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724048502 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -18432
1608802724048517 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum:0 18432
1608802724048563 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724048568 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724048617 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724048687 global_norm/L2Loss_148:0 256
1608802724048851 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum:0 -18432
1608802724048856 gradients/bert/encoder/layer_8/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724051334 bert/encoder/layer_8/output/LayerNorm/moments/variance:0 -16384
1608802724051619 bert/encoder/layer_8/output/LayerNorm/moments/mean:0 -16384
1608802724051945 bert/encoder/layer_8/output/dense/MatMul:0 -12582912
1608802724051992 gradients/bert/encoder/layer_8/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724051992 gradients/bert/encoder/layer_8/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724052002 gradients/bert/encoder/layer_8/output/dropout/mul_1_grad/Mul:0 12582912
1608802724052410 gradients/bert/encoder/layer_8/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724052415 gradients/bert/encoder/layer_8/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724052485 gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul:0 78136320
1608802724052720 gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724053174 bert/encoder/layer_8/output/dropout/Cast:0 -12582912
1608802724053174 bert/encoder/layer_8/intermediate/dense/MatMul:0 -50331648
1608802724053183 global_norm/L2Loss_146:0 256
1608802724058200 gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul:0 -78136320
1608802724058226 global_norm/L2Loss_145:0 256
1608802724058226 global_norm/L2Loss_145:t0 14848
1608802724058322 global_norm/L2Loss_145:t0 -14848
1608802724058367 gradients/bert/encoder/layer_8/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724058508 bert/encoder/layer_8/intermediate/dense/Pow:0 -50331648
1608802724058508 bert/encoder/layer_8/intermediate/dense/add_1:0 -50331648
1608802724058519 gradients/bert/encoder/layer_8/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 14848
1608802724058590 gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724058658 gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724058708 bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724058708 gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724058724 global_norm/L2Loss_144:0 256
1608802724058843 gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724058851 global_norm/L2Loss_143:0 256
1608802724058851 global_norm/L2Loss_143:t0 9216
1608802724058947 global_norm/L2Loss_143:t0 -9216
1608802724058951 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724059002 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724059056 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 14548992
1608802724059110 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724059159 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724059260 global_norm/L2Loss_141:0 256
1608802724059300 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 28928
1608802724059347 bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724059395 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -14548992
1608802724059420 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724059520 gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724059563 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -28928
1608802724059578 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 28928
1608802724059622 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724059627 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724059674 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724059743 global_norm/L2Loss_142:0 256
1608802724060270 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -28928
1608802724060274 gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724060943 bert/encoder/layer_8/attention/output/LayerNorm/moments/variance:0 -16384
1608802724061062 bert/encoder/layer_8/attention/output/LayerNorm/moments/mean:0 -16384
1608802724061154 bert/encoder/layer_8/attention/output/dense/MatMul:0 -12582912
1608802724061204 gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724061204 gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724061208 gradients/bert/encoder/layer_8/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724061298 gradients/bert/encoder/layer_8/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724061303 gradients/bert/encoder/layer_8/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724061366 gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724061428 gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul_1:0 2359296
1608802724061501 bert/encoder/layer_8/attention/output/dropout/Cast:0 -12582912
1608802724061501 bert/encoder/layer_8/attention/self/transpose_3:0 -12582912
1608802724061509 global_norm/L2Loss_140:0 256
1608802724061572 global_norm/L2Loss_139:0 256
1608802724061572 global_norm/L2Loss_139:t0 9216
1608802724061660 global_norm/L2Loss_139:t0 -9216
1608802724061664 gradients/bert/encoder/layer_8/attention/self/transpose_3_grad/transpose:0 12582912
1608802724061715 gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724061724 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724061724 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724061724 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724061724 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724062847 bert/encoder/layer_8/attention/self/transpose_2:0 -12582912
1608802724062847 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724062847 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724062847 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724062858 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724062858 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724062858 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724062858 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724063573 bert/encoder/layer_8/attention/self/dropout/mul:0 -25165824
1608802724063573 gradients/bert/encoder/layer_8/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724063573 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724063573 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724063573 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724063635 gradients/bert/encoder/layer_8/attention/self/transpose_2_grad/transpose:0 12582912
1608802724063679 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724063783 gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724063837 gradients/bert/encoder/layer_8/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724063893 gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724063947 gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul_1:0 3145728
1608802724064009 gradients/bert/encoder/layer_8/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724064020 gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum:0 196608
1608802724064068 bert/encoder/layer_8/attention/self/MatMul:0 -25165824
1608802724064076 global_norm/L2Loss_138:0 256
1608802724064119 global_norm/L2Loss_137:0 256
1608802724064119 global_norm/L2Loss_137:t0 9216
1608802724064200 global_norm/L2Loss_137:t0 -9216
1608802724064288 gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum:0 -196608
1608802724064481 bert/encoder/layer_8/attention/self/dropout/Cast:0 -25165824
1608802724064497 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:0 12582912
1608802724064497 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:t0 3072
1608802724064497 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:t1 3072
1608802724064497 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:t2 3072
1608802724066070 bert/encoder/layer_8/attention/self/transpose_1:0 -12582912
1608802724066070 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724066070 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724066070 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724066082 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724066082 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724066082 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724066082 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724067380 bert/encoder/layer_8/attention/self/transpose:0 -12582912
1608802724067380 gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724067380 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724067380 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724067380 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724067391 gradients/bert/encoder/layer_8/attention/self/transpose_grad/transpose:0 12582912
1608802724067442 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724067451 gradients/bert/encoder/layer_8/attention/self/transpose_1_grad/transpose:0 12582912
1608802724067499 gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724067534 gradients/bert/encoder/layer_8/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724067596 gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724067651 gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724067720 gradients/bert/encoder/layer_8/attention/self/transpose_grad/transpose:0 -12582912
1608802724067728 gradients/bert/encoder/layer_8/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724067784 gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724067834 gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724067895 bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724067895 gradients/bert/encoder/layer_8/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724067904 global_norm/L2Loss_134:0 256
1608802724067946 global_norm/L2Loss_133:0 256
1608802724067946 global_norm/L2Loss_133:t0 15104
1608802724068028 global_norm/L2Loss_133:t0 -15104
1608802724068031 global_norm/L2Loss_136:0 256
1608802724068104 gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724068104 gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724068104 gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724068110 global_norm/L2Loss_135:0 256
1608802724068110 global_norm/L2Loss_135:t0 14592
1608802724068186 global_norm/L2Loss_135:t0 -14592
1608802724068191 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724068237 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724068291 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724068343 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724068403 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724068521 global_norm/L2Loss_131:0 256
1608802724068561 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724068606 bert/encoder/layer_7/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724068649 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724068672 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724068766 gradients/bert/encoder/layer_7/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724068815 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724068831 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724068875 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724068882 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724068928 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724068992 global_norm/L2Loss_132:0 256
1608802724071063 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724071068 gradients/bert/encoder/layer_7/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724073530 bert/encoder/layer_7/output/LayerNorm/moments/variance:0 -16384
1608802724073816 bert/encoder/layer_7/output/LayerNorm/moments/mean:0 -16384
1608802724074158 bert/encoder/layer_7/output/dense/MatMul:0 -12582912
1608802724074203 gradients/bert/encoder/layer_7/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724074203 gradients/bert/encoder/layer_7/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724074207 gradients/bert/encoder/layer_7/output/dropout/mul_1_grad/Mul:0 12582912
1608802724074609 gradients/bert/encoder/layer_7/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724074614 gradients/bert/encoder/layer_7/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724074668 gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul:0 78136320
1608802724074913 gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul_1:0 9830400
1608802724075373 bert/encoder/layer_7/output/dropout/Cast:0 -12582912
1608802724075373 bert/encoder/layer_7/intermediate/dense/MatMul:0 -50331648
1608802724075382 global_norm/L2Loss_130:0 256
1608802724080402 gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul:0 -78136320
1608802724080406 global_norm/L2Loss_129:0 256
1608802724080406 global_norm/L2Loss_129:t0 9216
1608802724080489 global_norm/L2Loss_129:t0 -9216
1608802724080536 gradients/bert/encoder/layer_7/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724080674 bert/encoder/layer_7/intermediate/dense/Pow:0 -50331648
1608802724080674 bert/encoder/layer_7/intermediate/dense/add_1:0 -50331648
1608802724080678 gradients/bert/encoder/layer_7/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 12288
1608802724080739 gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724080800 gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724080847 bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724080847 gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724080856 global_norm/L2Loss_128:0 256
1608802724080940 gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724080949 global_norm/L2Loss_127:0 256
1608802724080949 global_norm/L2Loss_127:t0 9216
1608802724081029 global_norm/L2Loss_127:t0 -9216
1608802724081033 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724081087 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724081137 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724081184 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3840
1608802724081229 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724081321 global_norm/L2Loss_125:0 256
1608802724081359 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724081401 bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724081442 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724081463 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724081555 gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724081596 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724081604 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724081651 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724081655 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724081699 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724081761 global_norm/L2Loss_126:0 256
1608802724082469 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724082473 gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724083103 bert/encoder/layer_7/attention/output/LayerNorm/moments/variance:0 -16384
1608802724083210 bert/encoder/layer_7/attention/output/LayerNorm/moments/mean:0 -16384
1608802724083310 bert/encoder/layer_7/attention/output/dense/MatMul:0 -12582912
1608802724083349 gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724083349 gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724083359 gradients/bert/encoder/layer_7/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724083447 gradients/bert/encoder/layer_7/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724083452 gradients/bert/encoder/layer_7/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724083514 gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724083568 gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul_1:0 2359296
1608802724083641 bert/encoder/layer_7/attention/output/dropout/Cast:0 -12582912
1608802724083641 bert/encoder/layer_7/attention/self/transpose_3:0 -12582912
1608802724083649 global_norm/L2Loss_124:0 256
1608802724083710 global_norm/L2Loss_123:0 256
1608802724083710 global_norm/L2Loss_123:t0 12800
1608802724083795 global_norm/L2Loss_123:t0 -12800
1608802724083798 gradients/bert/encoder/layer_7/attention/self/transpose_3_grad/transpose:0 12582912
1608802724083846 gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724083856 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724083856 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724083856 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724083856 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724085007 bert/encoder/layer_7/attention/self/transpose_2:0 -12582912
1608802724085007 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724085007 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724085007 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724085018 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724085018 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724085018 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724085018 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724085729 bert/encoder/layer_7/attention/self/dropout/mul:0 -25165824
1608802724085729 gradients/bert/encoder/layer_7/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724085729 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724085729 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724085729 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724085788 gradients/bert/encoder/layer_7/attention/self/transpose_2_grad/transpose:0 12582912
1608802724085830 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724085942 gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724085993 gradients/bert/encoder/layer_7/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724086042 gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724086094 gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724086154 gradients/bert/encoder/layer_7/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724086161 gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum:0 196608
1608802724086205 bert/encoder/layer_7/attention/self/MatMul:0 -25165824
1608802724086213 global_norm/L2Loss_122:0 256
1608802724086252 global_norm/L2Loss_121:0 256
1608802724086252 global_norm/L2Loss_121:t0 9216
1608802724086325 global_norm/L2Loss_121:t0 -9216
1608802724086429 gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum:0 -196608
1608802724086618 bert/encoder/layer_7/attention/self/dropout/Cast:0 -25165824
1608802724086622 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:0 12582912
1608802724086622 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:t0 3072
1608802724086622 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:t1 3072
1608802724086622 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:t2 3072
1608802724088200 bert/encoder/layer_7/attention/self/transpose_1:0 -12582912
1608802724088200 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724088200 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724088200 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724088211 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724088211 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724088211 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724088211 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724089514 bert/encoder/layer_7/attention/self/transpose:0 -12582912
1608802724089514 gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724089514 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724089514 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724089514 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724089525 gradients/bert/encoder/layer_7/attention/self/transpose_grad/transpose:0 12582912
1608802724089573 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724089581 gradients/bert/encoder/layer_7/attention/self/transpose_1_grad/transpose:0 12582912
1608802724089618 gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724089710 gradients/bert/encoder/layer_7/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724089800 gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724089856 gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724089915 gradients/bert/encoder/layer_7/attention/self/transpose_grad/transpose:0 -12582912
1608802724089923 gradients/bert/encoder/layer_7/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724089967 gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724090104 gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724090157 bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724090157 gradients/bert/encoder/layer_7/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724090165 global_norm/L2Loss_118:0 256
1608802724090200 global_norm/L2Loss_117:0 256
1608802724090200 global_norm/L2Loss_117:t0 9216
1608802724090261 global_norm/L2Loss_117:t0 -9216
1608802724090264 global_norm/L2Loss_120:0 256
1608802724090320 gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724090320 gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724090320 gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724090325 global_norm/L2Loss_119:0 256
1608802724090325 global_norm/L2Loss_119:t0 9216
1608802724090379 global_norm/L2Loss_119:t0 -9216
1608802724090382 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724090417 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724090448 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724090480 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724090515 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724090575 global_norm/L2Loss_115:0 256
1608802724090604 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 23552
1608802724090631 bert/encoder/layer_6/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724090660 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724090672 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724090735 gradients/bert/encoder/layer_6/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724090763 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -23552
1608802724090770 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum:0 23552
1608802724090798 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724090803 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724090829 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724090868 global_norm/L2Loss_116:0 256
1608802724093210 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum:0 -23552
1608802724093222 gradients/bert/encoder/layer_6/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724095676 bert/encoder/layer_6/output/LayerNorm/moments/variance:0 -16384
1608802724095965 bert/encoder/layer_6/output/LayerNorm/moments/mean:0 -16384
1608802724096287 bert/encoder/layer_6/output/dense/MatMul:0 -12582912
1608802724096334 gradients/bert/encoder/layer_6/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724096334 gradients/bert/encoder/layer_6/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724096339 gradients/bert/encoder/layer_6/output/dropout/mul_1_grad/Mul:0 12582912
1608802724096755 gradients/bert/encoder/layer_6/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724096760 gradients/bert/encoder/layer_6/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724096825 gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul:0 50331648
1608802724097079 gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724097528 bert/encoder/layer_6/output/dropout/Cast:0 -12582912
1608802724097528 bert/encoder/layer_6/intermediate/dense/MatMul:0 -50331648
1608802724097538 global_norm/L2Loss_114:0 256
1608802724102559 gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul:0 -50331648
1608802724102564 global_norm/L2Loss_113:0 256
1608802724102564 global_norm/L2Loss_113:t0 9216
1608802724102665 global_norm/L2Loss_113:t0 -9216
1608802724102719 gradients/bert/encoder/layer_6/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724102875 bert/encoder/layer_6/intermediate/dense/Pow:0 -50331648
1608802724102875 bert/encoder/layer_6/intermediate/dense/add_1:0 -50331648
1608802724102883 gradients/bert/encoder/layer_6/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 20480
1608802724102963 gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724103036 gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724103092 bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724103092 gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724103102 global_norm/L2Loss_112:0 256
1608802724103200 gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724103211 global_norm/L2Loss_111:0 256
1608802724103211 global_norm/L2Loss_111:t0 9216
1608802724103304 global_norm/L2Loss_111:t0 -9216
1608802724103308 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724103369 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724103425 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724103476 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724103534 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724103655 global_norm/L2Loss_109:0 256
1608802724103699 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 29696
1608802724103749 bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724103800 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724103827 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724103938 gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724103985 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -29696
1608802724103995 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 29696
1608802724104050 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724104056 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724104105 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724104178 global_norm/L2Loss_110:0 256
1608802724104629 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -29696
1608802724104635 gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724105260 bert/encoder/layer_6/attention/output/LayerNorm/moments/variance:0 -16384
1608802724105380 bert/encoder/layer_6/attention/output/LayerNorm/moments/mean:0 -16384
1608802724105476 bert/encoder/layer_6/attention/output/dense/MatMul:0 -12582912
1608802724105524 gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724105524 gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724105537 gradients/bert/encoder/layer_6/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724105632 gradients/bert/encoder/layer_6/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724105637 gradients/bert/encoder/layer_6/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724105704 gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724105765 gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul_1:0 3145728
1608802724105847 bert/encoder/layer_6/attention/output/dropout/Cast:0 -15728640
1608802724105847 bert/encoder/layer_6/attention/self/transpose_3:0 -12582912
1608802724105858 global_norm/L2Loss_108:0 256
1608802724105925 global_norm/L2Loss_107:0 256
1608802724105925 global_norm/L2Loss_107:t0 9216
1608802724106023 global_norm/L2Loss_107:t0 -9216
1608802724106027 gradients/bert/encoder/layer_6/attention/self/transpose_3_grad/transpose:0 12582912
1608802724106082 gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724106093 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724106093 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724106093 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724106093 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724107165 bert/encoder/layer_6/attention/self/transpose_2:0 -12582912
1608802724107165 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724107165 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724107165 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724107180 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724107180 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724107180 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724107180 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724107893 bert/encoder/layer_6/attention/self/dropout/mul:0 -25165824
1608802724107893 gradients/bert/encoder/layer_6/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724107893 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724107893 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724107893 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724107968 gradients/bert/encoder/layer_6/attention/self/transpose_2_grad/transpose:0 12582912
1608802724108016 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724108099 gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724108163 gradients/bert/encoder/layer_6/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724108223 gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724108281 gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724108378 gradients/bert/encoder/layer_6/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724108388 gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum:0 196608
1608802724108443 bert/encoder/layer_6/attention/self/MatMul:0 -25165824
1608802724108452 global_norm/L2Loss_106:0 256
1608802724108500 global_norm/L2Loss_105:0 256
1608802724108500 global_norm/L2Loss_105:t0 9216
1608802724108588 global_norm/L2Loss_105:t0 -9216
1608802724108637 gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum:0 -196608
1608802724108778 bert/encoder/layer_6/attention/self/dropout/Cast:0 -25165824
1608802724108783 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:0 12582912
1608802724108783 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:t0 3072
1608802724108783 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:t1 3072
1608802724108783 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:t2 3072
1608802724110371 bert/encoder/layer_6/attention/self/transpose_1:0 -12582912
1608802724110371 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724110371 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724110371 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724110385 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724110385 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724110385 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724110385 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724111688 bert/encoder/layer_6/attention/self/transpose:0 -12582912
1608802724111688 gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724111688 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724111688 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724111688 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724111700 gradients/bert/encoder/layer_6/attention/self/transpose_grad/transpose:0 12582912
1608802724111756 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724111766 gradients/bert/encoder/layer_6/attention/self/transpose_1_grad/transpose:0 12582912
1608802724111810 gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724111847 gradients/bert/encoder/layer_6/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724111909 gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724111966 gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724112036 gradients/bert/encoder/layer_6/attention/self/transpose_grad/transpose:0 -12582912
1608802724112045 gradients/bert/encoder/layer_6/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724112105 gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724112157 gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724112224 bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724112224 gradients/bert/encoder/layer_6/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724112234 global_norm/L2Loss_102:0 256
1608802724112284 global_norm/L2Loss_101:0 256
1608802724112284 global_norm/L2Loss_101:t0 15872
1608802724112385 global_norm/L2Loss_101:t0 -15872
1608802724112389 global_norm/L2Loss_104:0 256
1608802724112471 gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724112471 gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724112471 gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724112476 global_norm/L2Loss_103:0 256
1608802724112476 global_norm/L2Loss_103:t0 15360
1608802724112557 global_norm/L2Loss_103:t0 -15360
1608802724112571 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724112628 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724112684 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724112739 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724112798 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724112904 global_norm/L2Loss_99:0 256
1608802724112946 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724112995 bert/encoder/layer_5/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724113039 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724113063 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724113165 gradients/bert/encoder/layer_5/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724113217 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724113234 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724113281 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724113285 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724113333 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724113400 global_norm/L2Loss_100:0 256
1608802724115360 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724115364 gradients/bert/encoder/layer_5/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724117833 bert/encoder/layer_5/output/LayerNorm/moments/variance:0 -16384
1608802724118120 bert/encoder/layer_5/output/LayerNorm/moments/mean:0 -16384
1608802724118465 bert/encoder/layer_5/output/dense/MatMul:0 -12582912
1608802724118511 gradients/bert/encoder/layer_5/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724118511 gradients/bert/encoder/layer_5/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724118518 gradients/bert/encoder/layer_5/output/dropout/mul_1_grad/Mul:0 12582912
1608802724118924 gradients/bert/encoder/layer_5/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724118931 gradients/bert/encoder/layer_5/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724118993 gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul:0 50331648
1608802724119228 gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724119692 bert/encoder/layer_5/output/dropout/Cast:0 -12582912
1608802724119692 bert/encoder/layer_5/intermediate/dense/MatMul:0 -50331648
1608802724119701 global_norm/L2Loss_98:0 256
1608802724124705 gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul:0 -50331648
1608802724124710 global_norm/L2Loss_97:0 256
1608802724124710 global_norm/L2Loss_97:t0 9216
1608802724124798 global_norm/L2Loss_97:t0 -9216
1608802724124848 gradients/bert/encoder/layer_5/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724124985 bert/encoder/layer_5/intermediate/dense/Pow:0 -50331648
1608802724124985 bert/encoder/layer_5/intermediate/dense/add_1:0 -50331648
1608802724124990 gradients/bert/encoder/layer_5/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 12288
1608802724125057 gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724125124 gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724125174 bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724125174 gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724125182 global_norm/L2Loss_96:0 256
1608802724125268 gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724125279 global_norm/L2Loss_95:0 256
1608802724125279 global_norm/L2Loss_95:t0 9216
1608802724125361 global_norm/L2Loss_95:t0 -9216
1608802724125365 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724125427 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724125479 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724125532 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 4608
1608802724125578 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724125679 global_norm/L2Loss_93:0 256
1608802724125718 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724125766 bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724125809 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724125831 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724125931 gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724125973 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724125982 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724126033 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724126038 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724126084 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724126149 global_norm/L2Loss_94:0 256
1608802724126773 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724126780 gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724127410 bert/encoder/layer_5/attention/output/LayerNorm/moments/variance:0 -16384
1608802724127526 bert/encoder/layer_5/attention/output/LayerNorm/moments/mean:0 -16384
1608802724127620 bert/encoder/layer_5/attention/output/dense/MatMul:0 -12582912
1608802724127663 gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724127663 gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724127685 gradients/bert/encoder/layer_5/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724127782 gradients/bert/encoder/layer_5/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724127787 gradients/bert/encoder/layer_5/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724127850 gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724127908 gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul_1:0 2359296
1608802724127987 bert/encoder/layer_5/attention/output/dropout/Cast:0 -12582912
1608802724127987 bert/encoder/layer_5/attention/self/transpose_3:0 -12582912
1608802724127996 global_norm/L2Loss_92:0 256
1608802724128064 global_norm/L2Loss_91:0 256
1608802724128064 global_norm/L2Loss_91:t0 12800
1608802724128154 global_norm/L2Loss_91:t0 -12800
1608802724128158 gradients/bert/encoder/layer_5/attention/self/transpose_3_grad/transpose:0 12582912
1608802724128211 gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724128221 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724128221 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724128221 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724128221 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724129302 bert/encoder/layer_5/attention/self/transpose_2:0 -12582912
1608802724129302 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724129302 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724129302 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724129315 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724129315 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724129315 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724129315 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724130033 bert/encoder/layer_5/attention/self/dropout/mul:0 -25165824
1608802724130033 gradients/bert/encoder/layer_5/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724130033 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724130033 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724130033 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724130097 gradients/bert/encoder/layer_5/attention/self/transpose_2_grad/transpose:0 12582912
1608802724130142 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724130241 gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724130296 gradients/bert/encoder/layer_5/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724130354 gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724130408 gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul_1:0 3145728
1608802724130472 gradients/bert/encoder/layer_5/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724130481 gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum:0 196608
1608802724130530 bert/encoder/layer_5/attention/self/MatMul:0 -25165824
1608802724130541 global_norm/L2Loss_90:0 256
1608802724130586 global_norm/L2Loss_89:0 256
1608802724130586 global_norm/L2Loss_89:t0 9216
1608802724130673 global_norm/L2Loss_89:t0 -9216
1608802724130729 gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum:0 -196608
1608802724130921 bert/encoder/layer_5/attention/self/dropout/Cast:0 -25165824
1608802724130925 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:0 12582912
1608802724130925 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:t0 3072
1608802724130925 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:t1 3072
1608802724130925 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:t2 3072
1608802724132508 bert/encoder/layer_5/attention/self/transpose_1:0 -12582912
1608802724132508 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724132508 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724132508 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724132521 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724132521 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724132521 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724132521 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724133868 bert/encoder/layer_5/attention/self/transpose:0 -12582912
1608802724133868 gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724133868 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724133868 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724133868 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724133879 gradients/bert/encoder/layer_5/attention/self/transpose_grad/transpose:0 12582912
1608802724133931 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724133941 gradients/bert/encoder/layer_5/attention/self/transpose_1_grad/transpose:0 12582912
1608802724133981 gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724134013 gradients/bert/encoder/layer_5/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724134071 gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724134124 gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724134189 gradients/bert/encoder/layer_5/attention/self/transpose_grad/transpose:0 -12582912
1608802724134198 gradients/bert/encoder/layer_5/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724134288 gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724134340 gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724134404 bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724134404 gradients/bert/encoder/layer_5/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724134413 global_norm/L2Loss_86:0 256
1608802724134461 global_norm/L2Loss_85:0 256
1608802724134461 global_norm/L2Loss_85:t0 9216
1608802724134542 global_norm/L2Loss_85:t0 -9216
1608802724134546 global_norm/L2Loss_88:0 256
1608802724134620 gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724134620 gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724134620 gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724134628 global_norm/L2Loss_87:0 256
1608802724134628 global_norm/L2Loss_87:t0 9216
1608802724134704 global_norm/L2Loss_87:t0 -9216
1608802724134715 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724134767 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724134819 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724134869 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724134916 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724135016 global_norm/L2Loss_83:0 256
1608802724135054 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 29696
1608802724135100 bert/encoder/layer_4/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724135144 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724135168 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724135265 gradients/bert/encoder/layer_4/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724135310 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -29696
1608802724135325 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum:0 29696
1608802724135368 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724135373 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724135417 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724135482 global_norm/L2Loss_84:0 256
1608802724137559 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum:0 -29696
1608802724137565 gradients/bert/encoder/layer_4/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724140019 bert/encoder/layer_4/output/LayerNorm/moments/variance:0 -16384
1608802724140309 bert/encoder/layer_4/output/LayerNorm/moments/mean:0 -16384
1608802724140644 bert/encoder/layer_4/output/dense/MatMul:0 -12582912
1608802724140706 gradients/bert/encoder/layer_4/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724140706 gradients/bert/encoder/layer_4/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724140713 gradients/bert/encoder/layer_4/output/dropout/mul_1_grad/Mul:0 12582912
1608802724141104 gradients/bert/encoder/layer_4/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724141110 gradients/bert/encoder/layer_4/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724141170 gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul:0 50331648
1608802724141410 gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724141867 bert/encoder/layer_4/output/dropout/Cast:0 -12582912
1608802724141867 bert/encoder/layer_4/intermediate/dense/MatMul:0 -50331648
1608802724141879 global_norm/L2Loss_82:0 256
1608802724146879 gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul:0 -50331648
1608802724146883 global_norm/L2Loss_81:0 256
1608802724146883 global_norm/L2Loss_81:t0 9216
1608802724146967 global_norm/L2Loss_81:t0 -9216
1608802724147013 gradients/bert/encoder/layer_4/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724147155 bert/encoder/layer_4/intermediate/dense/Pow:0 -50331648
1608802724147155 bert/encoder/layer_4/intermediate/dense/add_1:0 -50331648
1608802724147160 gradients/bert/encoder/layer_4/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 12288
1608802724147228 gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724147291 gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724147339 bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724147339 gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724147347 global_norm/L2Loss_80:0 256
1608802724147434 gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724147446 global_norm/L2Loss_79:0 256
1608802724147446 global_norm/L2Loss_79:t0 14336
1608802724147524 global_norm/L2Loss_79:t0 -14336
1608802724147527 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724147593 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724147643 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724147696 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724147744 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724147840 global_norm/L2Loss_77:0 256
1608802724147878 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 30208
1608802724147925 bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724147966 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724147988 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724148084 gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724148123 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -30208
1608802724148132 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 30208
1608802724148179 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724148183 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724148227 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724148290 global_norm/L2Loss_78:0 256
1608802724148970 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -30208
1608802724148975 gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724149602 bert/encoder/layer_4/attention/output/LayerNorm/moments/variance:0 -16384
1608802724149711 bert/encoder/layer_4/attention/output/LayerNorm/moments/mean:0 -16384
1608802724149807 bert/encoder/layer_4/attention/output/dense/MatMul:0 -12582912
1608802724149853 gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724149853 gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724149864 gradients/bert/encoder/layer_4/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724149954 gradients/bert/encoder/layer_4/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724149961 gradients/bert/encoder/layer_4/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724150026 gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724150082 gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul_1:0 2359296
1608802724150157 bert/encoder/layer_4/attention/output/dropout/Cast:0 -12582912
1608802724150157 bert/encoder/layer_4/attention/self/transpose_3:0 -12582912
1608802724150166 global_norm/L2Loss_76:0 256
1608802724150228 global_norm/L2Loss_75:0 256
1608802724150228 global_norm/L2Loss_75:t0 9216
1608802724150314 global_norm/L2Loss_75:t0 -9216
1608802724150319 gradients/bert/encoder/layer_4/attention/self/transpose_3_grad/transpose:0 12582912
1608802724150366 gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724150377 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724150377 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:t0 4608
1608802724150377 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724150377 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724151492 bert/encoder/layer_4/attention/self/transpose_2:0 -12582912
1608802724151492 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:t0 -4608
1608802724151492 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724151492 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724151505 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724151505 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:t0 4608
1608802724151505 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724151505 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724152222 bert/encoder/layer_4/attention/self/dropout/mul:0 -25165824
1608802724152222 gradients/bert/encoder/layer_4/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724152222 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:t0 -4608
1608802724152222 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724152222 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724152284 gradients/bert/encoder/layer_4/attention/self/transpose_2_grad/transpose:0 12582912
1608802724152330 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724152436 gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724152516 gradients/bert/encoder/layer_4/attention/self/value/BiasAdd_grad/BiasAddGrad:0 4608
1608802724152568 gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724152623 gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724152687 gradients/bert/encoder/layer_4/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724152696 gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum:0 196608
1608802724152741 bert/encoder/layer_4/attention/self/MatMul:0 -25165824
1608802724152750 global_norm/L2Loss_74:0 256
1608802724152788 global_norm/L2Loss_73:0 256
1608802724152788 global_norm/L2Loss_73:t0 9216
1608802724152861 global_norm/L2Loss_73:t0 -9216
1608802724152909 gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum:0 -196608
1608802724153100 bert/encoder/layer_4/attention/self/dropout/Cast:0 -25165824
1608802724153108 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:0 12582912
1608802724153108 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:t0 3072
1608802724153108 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:t1 3072
1608802724153108 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:t2 3072
1608802724154683 bert/encoder/layer_4/attention/self/transpose_1:0 -12582912
1608802724154683 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724154683 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724154683 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724154694 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724154694 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724154694 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724154694 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724155990 bert/encoder/layer_4/attention/self/transpose:0 -12582912
1608802724155990 gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724155990 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724155990 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724155990 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724156000 gradients/bert/encoder/layer_4/attention/self/transpose_grad/transpose:0 12582912
1608802724156048 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724156057 gradients/bert/encoder/layer_4/attention/self/transpose_1_grad/transpose:0 12582912
1608802724156096 gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724156126 gradients/bert/encoder/layer_4/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724156179 gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724156230 gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724156291 gradients/bert/encoder/layer_4/attention/self/transpose_grad/transpose:0 -12582912
1608802724156300 gradients/bert/encoder/layer_4/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724156355 gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724156416 gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724156475 bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724156475 gradients/bert/encoder/layer_4/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724156484 global_norm/L2Loss_70:0 256
1608802724156527 global_norm/L2Loss_69:0 256
1608802724156527 global_norm/L2Loss_69:t0 9216
1608802724156602 global_norm/L2Loss_69:t0 -9216
1608802724156605 global_norm/L2Loss_72:0 256
1608802724156675 gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724156675 gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724156675 gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724156683 global_norm/L2Loss_71:0 256
1608802724156683 global_norm/L2Loss_71:t0 9216
1608802724156752 global_norm/L2Loss_71:t0 -9216
1608802724156764 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724156812 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724156860 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724156907 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724156951 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724157046 global_norm/L2Loss_67:0 256
1608802724157084 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 19200
1608802724157127 bert/encoder/layer_3/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724157164 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724157190 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724157279 gradients/bert/encoder/layer_3/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724157322 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -19200
1608802724157336 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum:0 19200
1608802724157377 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724157381 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724157426 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724157488 global_norm/L2Loss_68:0 256
1608802724159688 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum:0 -19200
1608802724159693 gradients/bert/encoder/layer_3/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724162152 bert/encoder/layer_3/output/LayerNorm/moments/variance:0 -16384
1608802724162441 bert/encoder/layer_3/output/LayerNorm/moments/mean:0 -16384
1608802724162763 bert/encoder/layer_3/output/dense/MatMul:0 -12582912
1608802724162805 gradients/bert/encoder/layer_3/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724162805 gradients/bert/encoder/layer_3/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724162809 gradients/bert/encoder/layer_3/output/dropout/mul_1_grad/Mul:0 12582912
1608802724163235 gradients/bert/encoder/layer_3/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724163240 gradients/bert/encoder/layer_3/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724163294 gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul:0 75497472
1608802724163538 gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724164004 bert/encoder/layer_3/output/dropout/Cast:0 -12582912
1608802724164004 bert/encoder/layer_3/intermediate/dense/MatMul:0 -50331648
1608802724164013 global_norm/L2Loss_66:0 256
1608802724169014 gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul:0 -75497472
1608802724169021 global_norm/L2Loss_65:0 256
1608802724169021 global_norm/L2Loss_65:t0 15616
1608802724169093 global_norm/L2Loss_65:t0 -15616
1608802724169136 gradients/bert/encoder/layer_3/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724169288 bert/encoder/layer_3/intermediate/dense/Pow:0 -50331648
1608802724169288 bert/encoder/layer_3/intermediate/dense/add_1:0 -50331648
1608802724169293 gradients/bert/encoder/layer_3/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 15616
1608802724169353 gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724169408 gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724169454 bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724169454 gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724169462 global_norm/L2Loss_64:0 256
1608802724169544 gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724169555 global_norm/L2Loss_63:0 256
1608802724169555 global_norm/L2Loss_63:t0 9216
1608802724169630 global_norm/L2Loss_63:t0 -9216
1608802724169634 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724169683 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724169738 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724169784 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724169829 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724169915 global_norm/L2Loss_61:0 256
1608802724169950 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 28928
1608802724169990 bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724170028 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724170049 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724170133 gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724170169 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -28928
1608802724170177 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 28928
1608802724170221 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724170224 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724170292 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724170356 global_norm/L2Loss_62:0 256
1608802724171104 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -28928
1608802724171110 gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724171740 bert/encoder/layer_3/attention/output/LayerNorm/moments/variance:0 -16384
1608802724171849 bert/encoder/layer_3/attention/output/LayerNorm/moments/mean:0 -16384
1608802724171942 bert/encoder/layer_3/attention/output/dense/MatMul:0 -12582912
1608802724171978 gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724171978 gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724171989 gradients/bert/encoder/layer_3/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724172068 gradients/bert/encoder/layer_3/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724172073 gradients/bert/encoder/layer_3/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724172128 gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724172178 gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul_1:0 3145728
1608802724172246 bert/encoder/layer_3/attention/output/dropout/Cast:0 -12582912
1608802724172246 bert/encoder/layer_3/attention/self/transpose_3:0 -12582912
1608802724172254 global_norm/L2Loss_60:0 256
1608802724172309 global_norm/L2Loss_59:0 256
1608802724172309 global_norm/L2Loss_59:t0 9216
1608802724172398 global_norm/L2Loss_59:t0 -9216
1608802724172401 gradients/bert/encoder/layer_3/attention/self/transpose_3_grad/transpose:0 12582912
1608802724172467 gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724172477 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724172477 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724172477 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724172477 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724173640 bert/encoder/layer_3/attention/self/transpose_2:0 -12582912
1608802724173640 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724173640 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724173640 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724173653 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724173653 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724173653 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724173653 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724174408 bert/encoder/layer_3/attention/self/dropout/mul:0 -25165824
1608802724174408 gradients/bert/encoder/layer_3/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724174408 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724174408 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724174408 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724174466 gradients/bert/encoder/layer_3/attention/self/transpose_2_grad/transpose:0 12582912
1608802724174504 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724174626 gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724174674 gradients/bert/encoder/layer_3/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724174724 gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724174769 gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724174826 gradients/bert/encoder/layer_3/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724174834 gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum:0 196608
1608802724174881 bert/encoder/layer_3/attention/self/MatMul:0 -25165824
1608802724174891 global_norm/L2Loss_58:0 256
1608802724174927 global_norm/L2Loss_57:0 256
1608802724174927 global_norm/L2Loss_57:t0 9216
1608802724174996 global_norm/L2Loss_57:t0 -9216
1608802724175122 gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum:0 -196608
1608802724175313 bert/encoder/layer_3/attention/self/dropout/Cast:0 -25165824
1608802724175318 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:0 12582912
1608802724175318 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:t0 3072
1608802724175318 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:t1 3072
1608802724175318 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:t2 3072
1608802724176901 bert/encoder/layer_3/attention/self/transpose_1:0 -12582912
1608802724176901 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724176901 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724176901 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724176911 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724176911 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724176911 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724176911 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724178206 bert/encoder/layer_3/attention/self/transpose:0 -12582912
1608802724178206 gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724178206 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724178206 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724178206 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724178219 gradients/bert/encoder/layer_3/attention/self/transpose_grad/transpose:0 12582912
1608802724178263 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724178270 gradients/bert/encoder/layer_3/attention/self/transpose_1_grad/transpose:0 12582912
1608802724178305 gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724178333 gradients/bert/encoder/layer_3/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724178382 gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724178428 gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724178484 gradients/bert/encoder/layer_3/attention/self/transpose_grad/transpose:0 -12582912
1608802724178491 gradients/bert/encoder/layer_3/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724178539 gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724178581 gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724178638 bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724178638 gradients/bert/encoder/layer_3/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724178647 global_norm/L2Loss_54:0 256
1608802724178686 global_norm/L2Loss_53:0 256
1608802724178686 global_norm/L2Loss_53:t0 15104
1608802724178752 global_norm/L2Loss_53:t0 -15104
1608802724178758 global_norm/L2Loss_56:0 256
1608802724178820 gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724178820 gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724178820 gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724178828 global_norm/L2Loss_55:0 256
1608802724178828 global_norm/L2Loss_55:t0 14592
1608802724178888 global_norm/L2Loss_55:t0 -14592
1608802724178901 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724178944 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724178988 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724179031 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724179071 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724179158 global_norm/L2Loss_51:0 256
1608802724179191 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724179228 bert/encoder/layer_2/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724179264 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724179286 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724179368 gradients/bert/encoder/layer_2/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724179408 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724179422 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724179459 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724179462 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724179512 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724179576 global_norm/L2Loss_52:0 256
1608802724181906 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724181910 gradients/bert/encoder/layer_2/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724184379 bert/encoder/layer_2/output/LayerNorm/moments/variance:0 -16384
1608802724184665 bert/encoder/layer_2/output/LayerNorm/moments/mean:0 -16384
1608802724184985 bert/encoder/layer_2/output/dense/MatMul:0 -12582912
1608802724185021 gradients/bert/encoder/layer_2/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724185021 gradients/bert/encoder/layer_2/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724185025 gradients/bert/encoder/layer_2/output/dropout/mul_1_grad/Mul:0 12582912
1608802724185460 gradients/bert/encoder/layer_2/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724185467 gradients/bert/encoder/layer_2/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724185519 gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul:0 78136320
1608802724185763 gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724186226 bert/encoder/layer_2/output/dropout/Cast:0 -12582912
1608802724186226 bert/encoder/layer_2/intermediate/dense/MatMul:0 -50331648
1608802724186233 global_norm/L2Loss_50:0 256
1608802724191233 gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul:0 -78136320
1608802724191236 global_norm/L2Loss_49:0 256
1608802724191236 global_norm/L2Loss_49:t0 9216
1608802724191312 global_norm/L2Loss_49:t0 -9216
1608802724191352 gradients/bert/encoder/layer_2/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724191517 bert/encoder/layer_2/intermediate/dense/Pow:0 -50331648
1608802724191517 bert/encoder/layer_2/intermediate/dense/add_1:0 -50331648
1608802724191520 gradients/bert/encoder/layer_2/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 12288
1608802724191584 gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724191633 gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724191679 bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724191679 gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724191686 global_norm/L2Loss_48:0 256
1608802724191760 gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724191770 global_norm/L2Loss_47:0 256
1608802724191770 global_norm/L2Loss_47:t0 9216
1608802724191838 global_norm/L2Loss_47:t0 -9216
1608802724191841 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724191903 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724191960 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724192000 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3840
1608802724192036 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724192135 global_norm/L2Loss_45:0 256
1608802724192167 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724192204 bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724192237 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724192258 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724192337 gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724192381 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724192388 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724192430 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724192434 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724192514 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724192578 global_norm/L2Loss_46:0 256
1608802724193327 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724193331 gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724193956 bert/encoder/layer_2/attention/output/LayerNorm/moments/variance:0 -16384
1608802724194046 bert/encoder/layer_2/attention/output/LayerNorm/moments/mean:0 -16384
1608802724194163 bert/encoder/layer_2/attention/output/dense/MatMul:0 -12582912
1608802724194200 gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724194200 gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724194209 gradients/bert/encoder/layer_2/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724194283 gradients/bert/encoder/layer_2/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724194288 gradients/bert/encoder/layer_2/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724194339 gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724194384 gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul_1:0 2359296
1608802724194446 bert/encoder/layer_2/attention/output/dropout/Cast:0 -12582912
1608802724194446 bert/encoder/layer_2/attention/self/transpose_3:0 -12582912
1608802724194455 global_norm/L2Loss_44:0 256
1608802724194507 global_norm/L2Loss_43:0 256
1608802724194507 global_norm/L2Loss_43:t0 12800
1608802724194576 global_norm/L2Loss_43:t0 -12800
1608802724194579 gradients/bert/encoder/layer_2/attention/self/transpose_3_grad/transpose:0 12582912
1608802724194685 gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724194694 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724194694 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724194694 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724194694 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724195856 bert/encoder/layer_2/attention/self/transpose_2:0 -12582912
1608802724195856 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724195856 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724195856 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724195866 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724195866 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724195866 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724195866 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724196582 bert/encoder/layer_2/attention/self/dropout/mul:0 -25165824
1608802724196582 gradients/bert/encoder/layer_2/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724196582 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724196582 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724196582 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724196633 gradients/bert/encoder/layer_2/attention/self/transpose_2_grad/transpose:0 12582912
1608802724196668 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724196802 gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724196846 gradients/bert/encoder/layer_2/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724196891 gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724196935 gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul_1:0 3145728
1608802724196986 gradients/bert/encoder/layer_2/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724196993 gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum:0 196608
1608802724197058 bert/encoder/layer_2/attention/self/MatMul:0 -25165824
1608802724197067 global_norm/L2Loss_42:0 256
1608802724197102 global_norm/L2Loss_41:0 256
1608802724197102 global_norm/L2Loss_41:t0 9216
1608802724197165 global_norm/L2Loss_41:t0 -9216
1608802724197300 gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum:0 -196608
1608802724197491 bert/encoder/layer_2/attention/self/dropout/Cast:0 -25165824
1608802724197497 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:0 12582912
1608802724197497 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:t0 3072
1608802724197497 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:t1 3072
1608802724197497 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:t2 3072
1608802724199079 bert/encoder/layer_2/attention/self/transpose_1:0 -12582912
1608802724199079 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724199079 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724199079 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724199088 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724199088 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724199088 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724199088 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724200390 bert/encoder/layer_2/attention/self/transpose:0 -12582912
1608802724200390 gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724200390 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724200390 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724200390 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724200400 gradients/bert/encoder/layer_2/attention/self/transpose_grad/transpose:0 12582912
1608802724200440 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724200448 gradients/bert/encoder/layer_2/attention/self/transpose_1_grad/transpose:0 12582912
1608802724200479 gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724200507 gradients/bert/encoder/layer_2/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724200553 gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724200606 gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724200655 gradients/bert/encoder/layer_2/attention/self/transpose_grad/transpose:0 -12582912
1608802724200662 gradients/bert/encoder/layer_2/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724200704 gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724200745 gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724200795 bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724200795 gradients/bert/encoder/layer_2/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724200804 global_norm/L2Loss_38:0 256
1608802724200839 global_norm/L2Loss_37:0 256
1608802724200839 global_norm/L2Loss_37:t0 9216
1608802724200898 global_norm/L2Loss_37:t0 -9216
1608802724200902 global_norm/L2Loss_40:0 256
1608802724200959 gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724200959 gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724200959 gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724200965 global_norm/L2Loss_39:0 256
1608802724200965 global_norm/L2Loss_39:t0 9216
1608802724201027 global_norm/L2Loss_39:t0 -9216
1608802724201035 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724201076 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724201134 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724201171 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724201207 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724201310 global_norm/L2Loss_35:0 256
1608802724201341 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 23552
1608802724201378 bert/encoder/layer_1/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724201410 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724201428 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724201502 gradients/bert/encoder/layer_1/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724201538 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -23552
1608802724201557 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum:0 23552
1608802724201591 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724201596 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724201680 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724201744 global_norm/L2Loss_36:0 256
1608802724204085 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum:0 -23552
1608802724204089 gradients/bert/encoder/layer_1/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724206549 bert/encoder/layer_1/output/LayerNorm/moments/variance:0 -18688
1608802724206834 bert/encoder/layer_1/output/LayerNorm/moments/mean:0 -16384
1608802724207154 bert/encoder/layer_1/output/dense/MatMul:0 -12582912
1608802724207191 gradients/bert/encoder/layer_1/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724207191 gradients/bert/encoder/layer_1/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724207194 gradients/bert/encoder/layer_1/output/dropout/mul_1_grad/Mul:0 12582912
1608802724207629 gradients/bert/encoder/layer_1/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724207634 gradients/bert/encoder/layer_1/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724207680 gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul:0 50331648
1608802724207929 gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724208391 bert/encoder/layer_1/output/dropout/Cast:0 -12582912
1608802724208391 bert/encoder/layer_1/intermediate/dense/MatMul:0 -50331648
1608802724208401 global_norm/L2Loss_34:0 256
1608802724213397 gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul:0 -50331648
1608802724213402 global_norm/L2Loss_33:0 256
1608802724213402 global_norm/L2Loss_33:t0 9216
1608802724213477 global_norm/L2Loss_33:t0 -9216
1608802724213513 gradients/bert/encoder/layer_1/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724213683 bert/encoder/layer_1/intermediate/dense/Pow:0 -50331648
1608802724213683 bert/encoder/layer_1/intermediate/dense/add_1:0 -50331648
1608802724213687 gradients/bert/encoder/layer_1/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 20480
1608802724213736 gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724213782 gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul_1:0 12582912
1608802724213844 bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724213844 gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724213850 global_norm/L2Loss_32:0 256
1608802724213923 gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724213931 global_norm/L2Loss_31:0 256
1608802724213931 global_norm/L2Loss_31:t0 9216
1608802724214003 global_norm/L2Loss_31:t0 -9216
1608802724214006 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724214068 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724214125 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 20447232
1608802724214159 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724214197 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724214302 global_norm/L2Loss_29:0 256
1608802724214331 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 32000
1608802724214368 bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724214399 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -20447232
1608802724214415 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724214487 gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean_grad/Tile:0 20447232
1608802724214517 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -32000
1608802724214524 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 32000
1608802724214578 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724214582 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724214688 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724214742 global_norm/L2Loss_30:0 256
1608802724215493 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -32000
1608802724215496 gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724216126 bert/encoder/layer_1/attention/output/LayerNorm/moments/variance:0 -16384
1608802724216268 bert/encoder/layer_1/attention/output/LayerNorm/moments/mean:0 -16384
1608802724216336 bert/encoder/layer_1/attention/output/dense/MatMul:0 -12582912
1608802724216374 gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean_grad/Tile:0 -20447232
1608802724216374 gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724216377 gradients/bert/encoder/layer_1/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724216438 gradients/bert/encoder/layer_1/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724216444 gradients/bert/encoder/layer_1/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724216498 gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724216538 gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul_1:0 2359296
1608802724216589 bert/encoder/layer_1/attention/output/dropout/Cast:0 -12582912
1608802724216589 bert/encoder/layer_1/attention/self/transpose_3:0 -12582912
1608802724216595 global_norm/L2Loss_28:0 256
1608802724216636 global_norm/L2Loss_27:0 256
1608802724216636 global_norm/L2Loss_27:t0 9216
1608802724216739 global_norm/L2Loss_27:t0 -9216
1608802724216742 gradients/bert/encoder/layer_1/attention/self/transpose_3_grad/transpose:0 12582912
1608802724216845 gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724216853 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:0 37748736
1608802724216853 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724216853 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724216853 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724218018 bert/encoder/layer_1/attention/self/transpose_2:0 -12582912
1608802724218018 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724218018 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724218018 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724218026 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724218026 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724218026 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724218026 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724218736 bert/encoder/layer_1/attention/self/dropout/mul:0 -25165824
1608802724218736 gradients/bert/encoder/layer_1/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724218736 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724218736 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724218736 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724218784 gradients/bert/encoder/layer_1/attention/self/transpose_2_grad/transpose:0 20447232
1608802724218823 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724218969 gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul:0 -37748736
1608802724219011 gradients/bert/encoder/layer_1/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724219051 gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724219095 gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724219143 gradients/bert/encoder/layer_1/attention/self/transpose_2_grad/transpose:0 -20447232
1608802724219149 gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum:0 196608
1608802724219212 bert/encoder/layer_1/attention/self/MatMul:0 -25165824
1608802724219219 global_norm/L2Loss_26:0 256
1608802724219249 global_norm/L2Loss_25:0 256
1608802724219249 global_norm/L2Loss_25:t0 9216
1608802724219306 global_norm/L2Loss_25:t0 -9216
1608802724219442 gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum:0 -196608
1608802724219632 bert/encoder/layer_1/attention/self/dropout/Cast:0 -25165824
1608802724219636 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:0 20447232
1608802724219636 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:t0 3072
1608802724219636 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:t1 3072
1608802724219636 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:t2 3072
1608802724221223 bert/encoder/layer_1/attention/self/transpose_1:0 -12582912
1608802724221223 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724221223 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724221223 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724221232 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:0 12582912
1608802724221232 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724221232 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724221232 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724222521 bert/encoder/layer_1/attention/self/transpose:0 -12582912
1608802724222521 gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724222521 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724222521 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724222521 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724222530 gradients/bert/encoder/layer_1/attention/self/transpose_grad/transpose:0 12582912
1608802724222567 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul:0 -20447232
1608802724222574 gradients/bert/encoder/layer_1/attention/self/transpose_1_grad/transpose:0 20447232
1608802724222615 gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1:0 -12582912
1608802724222642 gradients/bert/encoder/layer_1/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724222686 gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724222728 gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724222777 gradients/bert/encoder/layer_1/attention/self/transpose_grad/transpose:0 -12582912
1608802724222783 gradients/bert/encoder/layer_1/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724222824 gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724222860 gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724222906 bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724222906 gradients/bert/encoder/layer_1/attention/self/transpose_1_grad/transpose:0 -20447232
1608802724222915 global_norm/L2Loss_22:0 256
1608802724222968 global_norm/L2Loss_21:0 256
1608802724222968 global_norm/L2Loss_21:t0 18176
1608802724223025 global_norm/L2Loss_21:t0 -18176
1608802724223029 global_norm/L2Loss_24:0 256
1608802724223086 gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724223086 gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724223086 gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724223091 global_norm/L2Loss_23:0 256
1608802724223091 global_norm/L2Loss_23:t0 17664
1608802724223154 global_norm/L2Loss_23:t0 -17664
1608802724223163 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724223219 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 20447232
1608802724223277 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724223313 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724223347 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724223453 global_norm/L2Loss_19:0 256
1608802724223481 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724223515 bert/encoder/layer_0/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724223545 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724223561 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724223629 gradients/bert/encoder/layer_0/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724223662 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724223674 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724223738 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724223741 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724223827 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -20447232
1608802724223894 global_norm/L2Loss_20:0 256
1608802724226221 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724226225 gradients/bert/encoder/layer_0/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724228690 bert/encoder/layer_0/output/LayerNorm/moments/variance:0 -16384
1608802724228977 bert/encoder/layer_0/output/LayerNorm/moments/mean:0 -16384
1608802724229294 bert/encoder/layer_0/output/dense/MatMul:0 -12582912
1608802724229325 gradients/bert/encoder/layer_0/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724229325 gradients/bert/encoder/layer_0/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724229328 gradients/bert/encoder/layer_0/output/dropout/mul_1_grad/Mul:0 12582912
1608802724229775 gradients/bert/encoder/layer_0/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724229780 gradients/bert/encoder/layer_0/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724229822 gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul:0 62914560
1608802724230073 gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul_1:0 12582912
1608802724230548 bert/encoder/layer_0/output/dropout/Cast:0 -12582912
1608802724230548 bert/encoder/layer_0/intermediate/dense/MatMul:0 -50331648
1608802724230554 global_norm/L2Loss_18:0 256
1608802724235556 gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul:0 -62914560
1608802724235559 global_norm/L2Loss_17:0 256
1608802724235559 global_norm/L2Loss_17:t0 9216
1608802724235629 global_norm/L2Loss_17:t0 -9216
1608802724235664 gradients/bert/encoder/layer_0/intermediate/dense/mul_3_grad/Mul_1:0 -50331648
1608802724235834 bert/encoder/layer_0/intermediate/dense/Pow:0 -50331648
1608802724235834 bert/encoder/layer_0/intermediate/dense/add_1:0 -50331648
1608802724235837 gradients/bert/encoder/layer_0/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 12288
1608802724235882 gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul:0 12582912
1608802724235931 gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul_1:0 9437184
1608802724235996 bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724235996 gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/Pow:0 -50331648
1608802724236019 global_norm/L2Loss_16:0 256
1608802724236077 gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul:0 -12582912
1608802724236084 global_norm/L2Loss_15:0 256
1608802724236084 global_norm/L2Loss_15:t0 9216
1608802724236154 global_norm/L2Loss_15:t0 -9216
1608802724236157 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724236219 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 12582912
1608802724236276 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724236313 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724236345 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724236452 global_norm/L2Loss_13:0 256
1608802724236480 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 16384
1608802724236513 bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul:0 -12582912
1608802724236541 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724236559 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724236640 gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724236668 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum:0 -16384
1608802724236674 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 16384
1608802724236728 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724236731 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 3584
1608802724236828 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -12582912
1608802724236893 global_norm/L2Loss_14:0 256
1608802724237640 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum:0 -16384
1608802724237645 gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724238321 bert/encoder/layer_0/attention/output/LayerNorm/moments/variance:0 -16384
1608802724238397 bert/encoder/layer_0/attention/output/LayerNorm/moments/mean:0 -16384
1608802724238527 bert/encoder/layer_0/attention/output/dense/MatMul:0 -12582912
1608802724238556 gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724238556 gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724238565 gradients/bert/encoder/layer_0/attention/output/dropout/mul_1_grad/Mul:0 12582912
1608802724238626 gradients/bert/encoder/layer_0/attention/output/dropout/mul_1_grad/Mul:0 -12582912
1608802724238630 gradients/bert/encoder/layer_0/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 3072
1608802724238692 gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul:0 12582912
1608802724238731 gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul_1:0 3145728
1608802724238781 bert/encoder/layer_0/attention/output/dropout/Cast:0 -23068672
1608802724238781 bert/encoder/layer_0/attention/self/transpose_3:0 -12582912
1608802724238787 global_norm/L2Loss_12:0 256
1608802724238831 global_norm/L2Loss_11:0 256
1608802724238831 global_norm/L2Loss_11:t0 12800
1608802724238926 global_norm/L2Loss_11:t0 -12800
1608802724238928 gradients/bert/encoder/layer_0/attention/self/transpose_3_grad/transpose:0 12582912
1608802724239032 gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul:0 -12582912
1608802724239039 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:0 25165824
1608802724239039 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:t0 3072
1608802724239039 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:t1 3072
1608802724239039 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:t2 3072
1608802724240207 bert/encoder/layer_0/attention/self/transpose_2:0 -12582912
1608802724240207 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:t0 -3072
1608802724240207 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:t1 -3072
1608802724240207 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:t2 -3072
1608802724240214 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:0 12582912
1608802724240214 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:t0 3072
1608802724240214 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:t1 3072
1608802724240214 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:t2 3072
1608802724240923 bert/encoder/layer_0/attention/self/dropout/mul:0 -25165824
1608802724240923 gradients/bert/encoder/layer_0/attention/self/transpose_3_grad/transpose:0 -12582912
1608802724240923 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:t0 -3072
1608802724240923 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:t1 -3072
1608802724240923 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:t2 -3072
1608802724240965 gradients/bert/encoder/layer_0/attention/self/transpose_2_grad/transpose:0 12582912
1608802724241005 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1:0 -12582912
1608802724241150 gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul:0 -25165824
1608802724241188 gradients/bert/encoder/layer_0/attention/self/value/BiasAdd_grad/BiasAddGrad:0 3072
1608802724241227 gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul:0 12582912
1608802724241263 gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul_1:0 2359296
1608802724241309 gradients/bert/encoder/layer_0/attention/self/transpose_2_grad/transpose:0 -12582912
1608802724241316 gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum:0 196608
1608802724241403 bert/encoder/layer_0/attention/self/MatMul:0 -25165824
1608802724241409 global_norm/L2Loss_10:0 256
1608802724241439 global_norm/L2Loss_9:0 256
1608802724241439 global_norm/L2Loss_9:t0 9216
1608802724241492 global_norm/L2Loss_9:t0 -9216
1608802724241646 gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum:0 -196608
1608802724241835 bert/encoder/layer_0/attention/self/dropout/Cast:0 -25165824
1608802724241838 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:0 12582912
1608802724241838 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:t0 3072
1608802724241838 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:t1 3072
1608802724241838 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:t2 3072
1608802724243416 bert/encoder/layer_0/attention/self/transpose_1:0 -12582912
1608802724243416 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:t0 -3072
1608802724243416 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:t1 -3072
1608802724243416 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:t2 -3072
1608802724243424 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:0 23068672
1608802724243424 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:t0 3072
1608802724243424 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:t1 3072
1608802724243424 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:t2 3072
1608802724244716 bert/encoder/layer_0/attention/self/transpose:0 -12582912
1608802724244716 gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul_1:0 -25165824
1608802724244716 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:t0 -3072
1608802724244716 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:t1 -3072
1608802724244716 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:t2 -3072
1608802724244725 gradients/bert/encoder/layer_0/attention/self/transpose_grad/transpose:0 12582912
1608802724244761 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul:0 -12582912
1608802724244767 gradients/bert/encoder/layer_0/attention/self/transpose_1_grad/transpose:0 12582912
1608802724244811 gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1:0 -23068672
1608802724244833 gradients/bert/encoder/layer_0/attention/self/query/BiasAdd_grad/BiasAddGrad:0 3072
1608802724244873 gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul:0 12582912
1608802724244908 gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul_1:0 2359296
1608802724244953 gradients/bert/encoder/layer_0/attention/self/transpose_grad/transpose:0 -12582912
1608802724244960 gradients/bert/encoder/layer_0/attention/self/key/BiasAdd_grad/BiasAddGrad:0 3072
1608802724244997 gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul:0 12582912
1608802724245030 gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul_1:0 2359296
1608802724245071 bert/embeddings/LayerNorm/batchnorm/mul_1:0 -12582912
1608802724245071 gradients/bert/encoder/layer_0/attention/self/transpose_1_grad/transpose:0 -12582912
1608802724245080 global_norm/L2Loss_6:0 256
1608802724245110 global_norm/L2Loss_5:0 256
1608802724245110 global_norm/L2Loss_5:t0 9216
1608802724247206 global_norm/L2Loss_5:t0 -9216
1608802724247211 global_norm/L2Loss_8:0 256
1608802724247282 gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul:0 -12582912
1608802724247282 gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul:0 -12582912
1608802724247282 gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul:0 -12582912
1608802724247293 global_norm/L2Loss_7:0 256
1608802724247293 global_norm/L2Loss_7:t0 9216
1608802724247349 global_norm/L2Loss_7:t0 -9216
1608802724247397 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724247404 gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul:0 12582912
1608802724247445 gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 23068672
1608802724247482 gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Neg:0 12582912
1608802724247514 gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Sum:0 3072
1608802724247552 bert/embeddings/dropout/Cast:0 -12582912
1608802724247617 global_norm/L2Loss_3:0 256
1608802724247646 gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Sum:0 29696
1608802724247677 bert/embeddings/LayerNorm/batchnorm/mul:0 -12582912
1608802724247707 gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Neg:0 -12582912
1608802724247723 gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Mul:0 12582912
1608802724247787 gradients/bert/embeddings/LayerNorm/moments/mean_grad/Tile:0 12582912
1608802724247822 gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Sum:0 -29696
1608802724247829 gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum:0 29696
1608802724247856 gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Mul:0 -12582912
1608802724247864 gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum_1:0 3072
1608802724247891 gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul_1:0 -23068672
1608802724247939 global_norm/L2Loss_4:0 256
1608802724250890 gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum:0 -29696
1608802724250898 gradients/bert/embeddings/LayerNorm/moments/variance_grad/Tile:0 12582912
1608802724250928 bert/embeddings/LayerNorm/moments/variance:0 -16384
1608802724251460 bert/embeddings/LayerNorm/moments/mean:0 -16384
1608802724251523 bert/embeddings/GatherV2:0 -12582912
1608802724251772 gradients/bert/embeddings/LayerNorm/moments/mean_grad/Tile:0 -12582912
1608802724251772 gradients/bert/embeddings/LayerNorm/moments/variance_grad/Tile:0 -12582912
1608802724251775 gradients/bert/embeddings/add_1_grad/Sum:0 393216
1608802724251984 gradients/bert/embeddings/Slice_grad/Pad:0 1572864
1608802724252250 gradients/bert/embeddings/add_1_grad/Sum:0 -393216
1608802724252256 gradients/bert/embeddings/MatMul_grad/MatMul_1:0 6144
1608802724252315 bert/embeddings/one_hot:0 -32768
1608802724252318 gradients/AddN_90/inputs_1:0 93769728
1608802724255235 gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul:0 -12582912
1608802724255247 global_norm/L2Loss_2:0 256
1608802724255247 global_norm/L2Loss_2:t0 16384
1608802724257724 global_norm/L2Loss_2:t0 -16384
1608802724257727 global_norm/L2Loss_1:0 256
1608802724257819 gradients/cls/predictions/MatMul_grad/MatMul_1:0 -93763584
1608802724257838 global_norm/L2Loss:0 256
1608802724257838 global_norm/L2Loss:t0 16384
1608802724257896 global_norm/L2Loss:t0 -16384
1608802724257899 global_norm/stack:0 1024
1608802724257899 global_norm/stack:t0 1792
1608802724258163 global_norm/L2Loss_205:0 -256
1608802724258163 global_norm/L2Loss_204:0 -256
1608802724258163 global_norm/L2Loss_198:0 -256
1608802724258163 global_norm/L2Loss_197:0 -256
1608802724258163 global_norm/L2Loss_203:0 -256
1608802724258163 global_norm/L2Loss_201:0 -256
1608802724258163 global_norm/L2Loss_202:0 -256
1608802724258163 global_norm/L2Loss_200:0 -256
1608802724258163 global_norm/L2Loss_199:0 -256
1608802724258163 global_norm/L2Loss_195:0 -256
1608802724258163 global_norm/L2Loss_196:0 -256
1608802724258163 global_norm/L2Loss_194:0 -256
1608802724258163 global_norm/L2Loss_193:0 -256
1608802724258163 global_norm/L2Loss_192:0 -256
1608802724258163 global_norm/L2Loss_191:0 -256
1608802724258163 global_norm/L2Loss_189:0 -256
1608802724258163 global_norm/L2Loss_190:0 -256
1608802724258163 global_norm/L2Loss_188:0 -256
1608802724258163 global_norm/L2Loss_187:0 -256
1608802724258163 global_norm/L2Loss_186:0 -256
1608802724258163 global_norm/L2Loss_185:0 -256
1608802724258163 global_norm/L2Loss_182:0 -256
1608802724258163 global_norm/L2Loss_181:0 -256
1608802724258163 global_norm/L2Loss_184:0 -256
1608802724258163 global_norm/L2Loss_183:0 -256
1608802724258163 global_norm/L2Loss_179:0 -256
1608802724258163 global_norm/L2Loss_180:0 -256
1608802724258163 global_norm/L2Loss_178:0 -256
1608802724258163 global_norm/L2Loss_177:0 -256
1608802724258163 global_norm/L2Loss_176:0 -256
1608802724258163 global_norm/L2Loss_175:0 -256
1608802724258163 global_norm/L2Loss_173:0 -256
1608802724258163 global_norm/L2Loss_174:0 -256
1608802724258163 global_norm/L2Loss_172:0 -256
1608802724258163 global_norm/L2Loss_171:0 -256
1608802724258163 global_norm/L2Loss_170:0 -256
1608802724258163 global_norm/L2Loss_169:0 -256
1608802724258163 global_norm/L2Loss_166:0 -256
1608802724258163 global_norm/L2Loss_165:0 -256
1608802724258163 global_norm/L2Loss_168:0 -256
1608802724258163 global_norm/L2Loss_167:0 -256
1608802724258163 global_norm/L2Loss_163:0 -256
1608802724258163 global_norm/L2Loss_164:0 -256
1608802724258163 global_norm/L2Loss_162:0 -256
1608802724258163 global_norm/L2Loss_161:0 -256
1608802724258163 global_norm/L2Loss_160:0 -256
1608802724258163 global_norm/L2Loss_159:0 -256
1608802724258163 global_norm/L2Loss_157:0 -256
1608802724258163 global_norm/L2Loss_158:0 -256
1608802724258163 global_norm/L2Loss_156:0 -256
1608802724258163 global_norm/L2Loss_155:0 -256
1608802724258163 global_norm/L2Loss_154:0 -256
1608802724258163 global_norm/L2Loss_153:0 -256
1608802724258163 global_norm/L2Loss_150:0 -256
1608802724258163 global_norm/L2Loss_149:0 -256
1608802724258163 global_norm/L2Loss_152:0 -256
1608802724258163 global_norm/L2Loss_151:0 -256
1608802724258163 global_norm/L2Loss_147:0 -256
1608802724258163 global_norm/L2Loss_148:0 -256
1608802724258163 global_norm/L2Loss_146:0 -256
1608802724258163 global_norm/L2Loss_145:0 -256
1608802724258163 global_norm/L2Loss_144:0 -256
1608802724258163 global_norm/L2Loss_143:0 -256
1608802724258163 global_norm/L2Loss_141:0 -256
1608802724258163 global_norm/L2Loss_142:0 -256
1608802724258163 global_norm/L2Loss_140:0 -256
1608802724258163 global_norm/L2Loss_139:0 -256
1608802724258163 global_norm/L2Loss_138:0 -256
1608802724258163 global_norm/L2Loss_137:0 -256
1608802724258163 global_norm/L2Loss_134:0 -256
1608802724258163 global_norm/L2Loss_133:0 -256
1608802724258163 global_norm/L2Loss_136:0 -256
1608802724258163 global_norm/L2Loss_135:0 -256
1608802724258163 global_norm/L2Loss_131:0 -256
1608802724258163 global_norm/L2Loss_132:0 -256
1608802724258163 global_norm/L2Loss_130:0 -256
1608802724258163 global_norm/L2Loss_129:0 -256
1608802724258163 global_norm/L2Loss_128:0 -256
1608802724258163 global_norm/L2Loss_127:0 -256
1608802724258163 global_norm/L2Loss_125:0 -256
1608802724258163 global_norm/L2Loss_126:0 -256
1608802724258163 global_norm/L2Loss_124:0 -256
1608802724258163 global_norm/L2Loss_123:0 -256
1608802724258163 global_norm/L2Loss_122:0 -256
1608802724258163 global_norm/L2Loss_121:0 -256
1608802724258163 global_norm/L2Loss_118:0 -256
1608802724258163 global_norm/L2Loss_117:0 -256
1608802724258163 global_norm/L2Loss_120:0 -256
1608802724258163 global_norm/L2Loss_119:0 -256
1608802724258163 global_norm/L2Loss_115:0 -256
1608802724258163 global_norm/L2Loss_116:0 -256
1608802724258163 global_norm/L2Loss_114:0 -256
1608802724258163 global_norm/L2Loss_113:0 -256
1608802724258163 global_norm/L2Loss_112:0 -256
1608802724258163 global_norm/L2Loss_111:0 -256
1608802724258163 global_norm/L2Loss_109:0 -256
1608802724258163 global_norm/L2Loss_110:0 -256
1608802724258163 global_norm/L2Loss_108:0 -256
1608802724258163 global_norm/L2Loss_107:0 -256
1608802724258163 global_norm/L2Loss_106:0 -256
1608802724258163 global_norm/L2Loss_105:0 -256
1608802724258163 global_norm/L2Loss_102:0 -256
1608802724258163 global_norm/L2Loss_101:0 -256
1608802724258163 global_norm/L2Loss_104:0 -256
1608802724258163 global_norm/L2Loss_103:0 -256
1608802724258163 global_norm/L2Loss_99:0 -256
1608802724258163 global_norm/L2Loss_100:0 -256
1608802724258163 global_norm/L2Loss_98:0 -256
1608802724258163 global_norm/L2Loss_97:0 -256
1608802724258163 global_norm/L2Loss_96:0 -256
1608802724258163 global_norm/L2Loss_95:0 -256
1608802724258163 global_norm/L2Loss_93:0 -256
1608802724258163 global_norm/L2Loss_94:0 -256
1608802724258163 global_norm/L2Loss_92:0 -256
1608802724258163 global_norm/L2Loss_91:0 -256
1608802724258163 global_norm/L2Loss_90:0 -256
1608802724258163 global_norm/L2Loss_89:0 -256
1608802724258163 global_norm/L2Loss_86:0 -256
1608802724258163 global_norm/L2Loss_85:0 -256
1608802724258163 global_norm/L2Loss_88:0 -256
1608802724258163 global_norm/L2Loss_87:0 -256
1608802724258163 global_norm/L2Loss_83:0 -256
1608802724258163 global_norm/L2Loss_84:0 -256
1608802724258163 global_norm/L2Loss_82:0 -256
1608802724258163 global_norm/L2Loss_81:0 -256
1608802724258163 global_norm/L2Loss_80:0 -256
1608802724258163 global_norm/L2Loss_79:0 -256
1608802724258163 global_norm/L2Loss_77:0 -256
1608802724258163 global_norm/L2Loss_78:0 -256
1608802724258163 global_norm/L2Loss_76:0 -256
1608802724258163 global_norm/L2Loss_75:0 -256
1608802724258163 global_norm/L2Loss_74:0 -256
1608802724258163 global_norm/L2Loss_73:0 -256
1608802724258163 global_norm/L2Loss_70:0 -256
1608802724258163 global_norm/L2Loss_69:0 -256
1608802724258163 global_norm/L2Loss_72:0 -256
1608802724258163 global_norm/L2Loss_71:0 -256
1608802724258163 global_norm/L2Loss_67:0 -256
1608802724258163 global_norm/L2Loss_68:0 -256
1608802724258163 global_norm/L2Loss_66:0 -256
1608802724258163 global_norm/L2Loss_65:0 -256
1608802724258163 global_norm/L2Loss_64:0 -256
1608802724258163 global_norm/L2Loss_63:0 -256
1608802724258163 global_norm/L2Loss_61:0 -256
1608802724258163 global_norm/L2Loss_62:0 -256
1608802724258163 global_norm/L2Loss_60:0 -256
1608802724258163 global_norm/L2Loss_59:0 -256
1608802724258163 global_norm/L2Loss_58:0 -256
1608802724258163 global_norm/L2Loss_57:0 -256
1608802724258163 global_norm/L2Loss_54:0 -256
1608802724258163 global_norm/L2Loss_53:0 -256
1608802724258163 global_norm/L2Loss_56:0 -256
1608802724258163 global_norm/L2Loss_55:0 -256
1608802724258163 global_norm/L2Loss_51:0 -256
1608802724258163 global_norm/L2Loss_52:0 -256
1608802724258163 global_norm/L2Loss_50:0 -256
1608802724258163 global_norm/L2Loss_49:0 -256
1608802724258163 global_norm/L2Loss_48:0 -256
1608802724258163 global_norm/L2Loss_47:0 -256
1608802724258163 global_norm/L2Loss_45:0 -256
1608802724258163 global_norm/L2Loss_46:0 -256
1608802724258163 global_norm/L2Loss_44:0 -256
1608802724258163 global_norm/L2Loss_43:0 -256
1608802724258163 global_norm/L2Loss_42:0 -256
1608802724258163 global_norm/L2Loss_41:0 -256
1608802724258163 global_norm/L2Loss_38:0 -256
1608802724258163 global_norm/L2Loss_37:0 -256
1608802724258163 global_norm/L2Loss_40:0 -256
1608802724258163 global_norm/L2Loss_39:0 -256
1608802724258163 global_norm/L2Loss_35:0 -256
1608802724258163 global_norm/L2Loss_36:0 -256
1608802724258163 global_norm/L2Loss_34:0 -256
1608802724258163 global_norm/L2Loss_33:0 -256
1608802724258163 global_norm/L2Loss_32:0 -256
1608802724258163 global_norm/L2Loss_31:0 -256
1608802724258163 global_norm/L2Loss_29:0 -256
1608802724258163 global_norm/L2Loss_30:0 -256
1608802724258163 global_norm/L2Loss_28:0 -256
1608802724258163 global_norm/L2Loss_27:0 -256
1608802724258163 global_norm/L2Loss_26:0 -256
1608802724258163 global_norm/L2Loss_25:0 -256
1608802724258163 global_norm/L2Loss_22:0 -256
1608802724258163 global_norm/L2Loss_21:0 -256
1608802724258163 global_norm/L2Loss_24:0 -256
1608802724258163 global_norm/L2Loss_23:0 -256
1608802724258163 global_norm/L2Loss_19:0 -256
1608802724258163 global_norm/L2Loss_20:0 -256
1608802724258163 global_norm/L2Loss_18:0 -256
1608802724258163 global_norm/L2Loss_17:0 -256
1608802724258163 global_norm/L2Loss_16:0 -256
1608802724258163 global_norm/L2Loss_15:0 -256
1608802724258163 global_norm/L2Loss_13:0 -256
1608802724258163 global_norm/L2Loss_14:0 -256
1608802724258163 global_norm/L2Loss_12:0 -256
1608802724258163 global_norm/L2Loss_11:0 -256
1608802724258163 global_norm/L2Loss_10:0 -256
1608802724258163 global_norm/L2Loss_9:0 -256
1608802724258163 global_norm/L2Loss_6:0 -256
1608802724258163 global_norm/L2Loss_5:0 -256
1608802724258163 global_norm/L2Loss_8:0 -256
1608802724258163 global_norm/L2Loss_7:0 -256
1608802724258163 global_norm/L2Loss_3:0 -256
1608802724258163 global_norm/L2Loss_4:0 -256
1608802724258163 global_norm/L2Loss_2:0 -256
1608802724258163 global_norm/L2Loss_1:0 -256
1608802724258163 global_norm/L2Loss:0 -256
1608802724258163 global_norm/stack:t0 -1792
1608802724258214 global_norm/Sum:0 256
1608802724258250 global_norm/stack:0 -1024
1608802724258315 clip_by_global_norm/IsFinite:0 256
1608802724258456 clip_by_global_norm/IsFinite:0 -256
1608802724290558 global_norm/Sum:0 -256
1608802724290563 Square_71:0 2359296
1608802724292845 Square_75:0 2359296
1608802724295347 Square_72:0 3072
1608802724295884 Square_64:0 16640
1608802724295941 Square_87:0 2359296
1608802724296404 Square_83:0 3072
1608802724297041 Square_61:0 3072
1608802724299660 Square_65:0 12582912
1608802724302170 Square_69:0 2359296
1608802724302248 Square_88:0 3072
1608802724302320 Square_76:0 3328
1608802724302446 Square_79:0 9437184
1608802724302594 Square_84:0 3072
1608802724302672 Square_89:0 3145728
1608802724302767 Square_73:0 2359296
1608802724302871 Square_77:0 3072
1608802724302935 Square_80:0 17408
1608802724303054 Square_62:0 3072
1608802724303113 Square_66:0 3072
1608802724303169 Square_91:0 2359296
1608802724303251 Mul_517:0 9437184
1608802724303421 Square_78:0 3072
1608802724303601 Square_81:0 9437184
1608802724304869 Square_85:0 2359296
1608802724304931 Square_90:0 3072
1608802724305081 Square_92:0 3072
1608802724305144 Square_70:0 3072
1608802724305242 Square_74:0 3072
1608802724305299 Mul_560:0 2359296
1608802724305484 Square_82:0 3072
1608802724305780 Square_86:0 3072
1608802724306069 Square:0 93763584
1608802724307426 Square_2:0 1966080
1608802724307514 Square_3:0 5376
1608802724307572 Square_6:0 3072
1608802724307754 Square_4:0 3072
1608802724307813 Square_7:0 4194304
1608802724307994 Square_10:0 3072
1608802724308051 Square_5:0 2359296
1608802724308230 Square_8:0 3072
1608802724308610 Square_11:0 2359296
1608802724309321 Square_14:0 3072
1608802724310636 Square_99:0 3072
1608802724311280 Square_9:0 2359296
1608802724311343 Square_12:0 3072
1608802724311406 Square_15:0 9437184
1608802724311463 Square_39:0 2359296
1608802724311576 Mul_641:0 3072
1608802724311702 Mul_663:0 3072
1608802724311845 Square_13:0 5888
1608802724311932 Square_16:0 12288
1608802724312011 Mul_555:0 3072
1608802724312124 Square_23:0 2359296
1608802724312191 Mul_577:0 3072
1608802724312304 Square_31:0 9437184
1608802724312373 Mul_620:0 3072
1608802724312436 Square_17:0 13369344
1608802724312520 Mul_668:0 2359296
1608802724312686 Mul_684:0 3072
1608802724312755 Mul_507:0 3072
1608802724313036 Square_53:0 2359296
1608802724313218 Square_56:0 3072
1608802724313274 Mul_566:0 4608
1608802724313900 Mul_582:0 2359296
1608802724313969 Mul_598:0 3072
1608802724314044 Mul_625:0 3072
1608802724314098 Mul_646:0 2359296
1608802724314161 Square_34:0 3072
1608802724314218 Square_36:0 3072
1608802724314276 Mul_706:0 3072
1608802724314329 Mul_512:0 3072
1608802724314383 Mul_523:0 12288
1608802724314436 Square_54:0 3072
1608802724314489 Square_57:0 2359296
1608802724314550 Mul_813:0 3072
1608802724314608 Mul_603:0 9437184
1608802724314670 Mul_630:0 3072
1608802724314728 Mul_652:0 3072
1608802724314791 Mul_674:0 3072
1608802724314848 Mul_689:0 9437184
1608802724314906 Mul_711:0 3072
1608802724314961 Square_37:0 2359296
1608802724315023 Mul_749:0 3072
1608802724315081 Square_97:0 9437184
1608802724315138 Mul_544:0 3072
1608802724315192 Mul_818:0 2359296
1608802724315256 Square_58:0 3072
1608802724315314 Mul_609:0 12288
1608802724315367 Mul_635:0 2359296
1608802724315428 Mul_657:0 2359296
1608802724315491 Mul_679:0 3072
1608802724315544 Mul_695:0 12800
1608802724315600 Mul_716:0 3072
1608802724315655 Mul_727:0 3072
1608802724315707 Mul_754:0 2359296
1608802724315766 Square_38:0 3072
1608802724315822 Square_98:0 3072
1608802724315875 Mul_549:0 2359296
1608802724315932 Square_47:0 9437184
1608802724315995 Mul_588:0 3072
1608802724316050 Mul_614:0 9437184
1608802724316106 Mul_893:0 2359296
1608802724316163 Square_169:0 2359296
1608802724316237 Square_173:0 3072
1608802724316296 Mul_700:0 9437184
1608802724316365 Square_181:0 2359296
1608802724316435 Mul_732:0 2359296
1608802724316496 Mul_760:0 3072
1608802724316554 Mul_770:0 3072
1608802724316610 Mul_792:0 3072
1608802724316664 Mul_824:0 3072
1608802724316718 Mul_829:0 2359296
1608802724316777 Square_43:0 2359296
1608802724316834 Mul_593:0 3072
1608802724316889 Mul_899:0 3072
1608802724316940 Square_170:0 3072
1608802724317001 Square_174:0 3072
1608802724317054 Square_177:0 9437184
1608802724317112 Square_182:0 3072
1608802724317164 Mul_1007:0 3072
1608802724317223 Mul_738:0 3072
1608802724317274 Mul_765:0 3072
1608802724317330 Mul_775:0 9437184
1608802724317387 Mul_797:0 3072
1608802724317443 Square_28:0 3072
1608802724317495 Mul_835:0 3072
1608802724317546 Mul_851:0 3072
1608802724317609 Square_40:0 3072
1608802724317664 Mul_904:0 4033536
1608802724317725 Square_171:0 2359296
1608802724317786 Square_175:0 9437184
1608802724317841 Square_178:0 3072
1608802724317896 Square_184:0 3072
1608802724317953 Mul_1012:0 2359296
1608802724318012 Mul_721:0 2359296
1608802724318069 Mul_743:0 2359296
1608802724318126 Mul_1071:0 3072
1608802724318185 Mul_781:0 18432
1608802724318240 Mul_802:0 3072
1608802724318291 Square_20:0 3072
1608802724318344 Mul_840:0 2359296
1608802724318404 Mul_856:0 3072
1608802724318460 Mul_872:0 9437184
1608802724318519 Mul_571:0 2359296
1608802724318578 Square_41:0 2359296
1608802724318635 Square_176:0 12288
1608802724318690 Square_179:0 3072
1608802724318746 Mul_1001:0 2359296
1608802724318805 Mul_1018:0 3072
1608802724318863 Mul_1028:0 3072
1608802724318918 Mul_1050:0 3072
1608802724318984 Square_199:0 2359296
1608802724319043 Square_202:0 3072
1608802724319097 Mul_786:0 9437184
1608802724319160 Mul_807:0 2359296
1608802724319216 Mul_846:0 3072
1608802724319270 Mul_861:0 9437184
1608802724319324 Mul_878:0 3072
1608802724319384 Square_168:0 3072
1608802724319437 Square_172:0 3072
1608802724319492 Square_180:0 3072
1608802724319546 Square_44:0 3072
1608802724319600 Mul_1023:0 3072
1608802724319655 Mul_1033:0 9437184
1608802724319715 Mul_1055:0 3072
1608802724319769 Square_200:0 3072
1608802724319822 Mul_1097:0 122112
1608802724319878 Square_1:0 6144
1608802724319929 Mul_867:0 12288
1608802724319982 Mul_883:0 3072
1608802724320032 Square_24:0 3072
1608802724320089 Square_35:0 3072
1608802724320142 Square_42:0 3072
1608802724320196 Mul_1039:0 12288
1608802724320252 Mul_1060:0 3584
1608802724320312 Square_201:0 3072
1608802724320382 Mul_1102:0 6144
1608802724320438 Square_59:0 2359296
1608802724320497 Mul_888:0 3072
1608802724320552 Square_19:0 3072
1608802724320605 Square_32:0 12288
1608802724320660 Mul_1044:0 9437184
1608802724320714 Mul_1065:0 2359296
1608802724320771 Square_45:0 3072
1608802724320823 Square_205:0 256
1608802724320876 Square_51:0 3072
1608802724320933 Square_55:0 2359296
1608802724320989 Square_183:0 2359296
1608802724321052 Square_48:0 12288
1608802724321105 Square_49:0 9437184
1608802724321161 Square_50:0 3072
1608802724321217 Square_18:0 3072
1608802724321274 Square_21:0 2359296
1608802724321331 Square_25:0 2359296
1608802724321392 Square_29:0 3072
1608802724321446 Square_22:0 3072
1608802724321503 Square_26:0 3072
1608802724321556 Square_30:0 3072
1608802724321610 Square_46:0 3072
1608802724321666 Square_27:0 2359296
1608802724321721 Square_52:0 3072
1608802724321787 Square_33:0 9437184
1608802724321848 Square_60:0 3072
1608802724321904 Square_63:0 9437184
1608802724321984 Square_67:0 3072
1608802724322036 Square_68:0 3072
1608802724322134 gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724322188 gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724322238 gradients/bert/encoder/layer_4/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724322292 gradients/bert/encoder/layer_3/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -15616
1608802724322351 gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724322406 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724322462 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724322516 gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724322576 gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724322631 gradients/bert/encoder/layer_5/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724322690 gradients/bert/encoder/layer_4/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724322745 gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724322801 gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724322856 gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul_1:0 -3145728
1608802724322915 gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724322968 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724323023 gradients/bert/encoder/layer_4/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -12288
1608802724323078 gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724323130 gradients/bert/encoder/layer_3/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724323202 gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724323229 Mul_517:0 -9437184
1608802724323314 gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724323367 gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724323425 gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724323487 gradients/bert/encoder/layer_5/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724323544 gradients/bert/encoder/layer_5/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724323597 gradients/bert/encoder/layer_4/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724323649 gradients/bert/encoder/layer_4/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -4608
1608802724323678 Mul_560:0 -2359296
1608802724323762 gradients/bert/encoder/layer_4/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724323824 gradients/bert/encoder/layer_5/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724323877 gradients/AddN_90/inputs_1:0 -93769728
1608802724323935 gradients/bert/embeddings/Slice_grad/Pad:0 -1572864
1608802724323991 gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724324043 gradients/bert/encoder/layer_0/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724324106 gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724324160 gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724324216 gradients/bert/encoder/layer_0/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724324270 gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724324322 gradients/bert/encoder/layer_0/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724324389 gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul_1:0 -3145728
1608802724324445 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3584
1608802724324504 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724324564 gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724324620 gradients/bert/encoder/layer_0/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724324679 gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul_1:0 -9437184
1608802724324737 gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724324764 Mul_641:0 -3072
1608802724324822 Mul_663:0 -3072
1608802724324903 gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724324961 gradients/bert/encoder/layer_0/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -12288
1608802724324987 Mul_555:0 -3072
1608802724325071 gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724325098 Mul_577:0 -3072
1608802724325182 gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724325213 Mul_620:0 -3072
1608802724325297 gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724325324 Mul_668:0 -2359296
1608802724325381 Mul_684:0 -3072
1608802724325438 Mul_507:0 -3072
1608802724325519 gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724325580 gradients/bert/encoder/layer_3/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724325608 Mul_566:0 -4608
1608802724325662 Mul_582:0 -2359296
1608802724325720 Mul_598:0 -3072
1608802724325774 Mul_625:0 -3072
1608802724325831 Mul_646:0 -2359296
1608802724325912 gradients/bert/encoder/layer_1/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724325974 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724326003 Mul_706:0 -3072
1608802724326060 Mul_512:0 -3072
1608802724326114 Mul_523:0 -12288
1608802724326200 gradients/bert/encoder/layer_3/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724326255 gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724326285 Mul_813:0 -3072
1608802724326342 Mul_603:0 -9437184
1608802724326399 Mul_630:0 -3072
1608802724326452 Mul_652:0 -3072
1608802724326508 Mul_674:0 -3072
1608802724326568 Mul_689:0 -9437184
1608802724326622 Mul_711:0 -3072
1608802724326705 gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724326733 Mul_749:0 -3072
1608802724326817 gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724326846 Mul_544:0 -3072
1608802724326899 Mul_818:0 -2359296
1608802724326979 gradients/bert/encoder/layer_3/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724327005 Mul_609:0 -12288
1608802724327058 Mul_635:0 -2359296
1608802724327117 Mul_657:0 -2359296
1608802724327174 Mul_679:0 -3072
1608802724327230 Mul_695:0 -12800
1608802724327284 Mul_716:0 -3072
1608802724327343 Mul_727:0 -3072
1608802724327407 Mul_754:0 -2359296
1608802724327495 gradients/bert/encoder/layer_2/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724327548 gradients/bert/encoder/layer_5/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724327581 Mul_549:0 -2359296
1608802724327665 gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724327690 Mul_588:0 -3072
1608802724327750 Mul_614:0 -9437184
1608802724327805 Mul_893:0 -2359296
1608802724327890 gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724327942 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724327965 Mul_700:0 -9437184
1608802724328045 gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724328075 Mul_732:0 -2359296
1608802724328129 Mul_760:0 -3072
1608802724328184 Mul_770:0 -3072
1608802724328237 Mul_792:0 -3072
1608802724328296 Mul_824:0 -3072
1608802724328352 Mul_829:0 -2359296
1608802724328446 gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724328474 Mul_593:0 -3072
1608802724328530 Mul_899:0 -3072
1608802724328617 gradients/bert/encoder/layer_10/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724328671 gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724328749 gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724328804 gradients/bert/encoder/layer_11/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724328828 Mul_1007:0 -3072
1608802724328887 Mul_738:0 -3072
1608802724328942 Mul_765:0 -3072
1608802724328998 Mul_775:0 -9437184
1608802724329053 Mul_797:0 -3072
1608802724329140 gradients/bert/encoder/layer_1/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724329165 Mul_835:0 -3072
1608802724329222 Mul_851:0 -3072
1608802724329299 gradients/bert/encoder/layer_2/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724329332 Mul_904:0 -4033536
1608802724329415 gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724329473 gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724329524 gradients/bert/encoder/layer_10/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724329576 gradients/bert/encoder/layer_11/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724329606 Mul_1012:0 -2359296
1608802724329661 Mul_721:0 -2359296
1608802724329719 Mul_743:0 -2359296
1608802724329777 Mul_1071:0 -3072
1608802724329831 Mul_781:0 -18432
1608802724329887 Mul_802:0 -3072
1608802724329975 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724330002 Mul_840:0 -2359296
1608802724330063 Mul_856:0 -3072
1608802724330119 Mul_872:0 -9437184
1608802724330173 Mul_571:0 -2359296
1608802724330265 gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul_1:0 -3145728
1608802724330319 gradients/bert/encoder/layer_10/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -13312
1608802724330374 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724330403 Mul_1001:0 -2359296
1608802724330457 Mul_1018:0 -3072
1608802724330511 Mul_1028:0 -3072
1608802724330568 Mul_1050:0 -3072
1608802724330649 gradients/cls/predictions/transform/dense/MatMul_grad/MatMul_1:0 -2818048
1608802724330702 gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724330732 Mul_786:0 -9437184
1608802724330786 Mul_807:0 -2359296
1608802724330839 Mul_846:0 -3072
1608802724330899 Mul_861:0 -9437184
1608802724330957 Mul_878:0 -3072
1608802724331047 gradients/bert/encoder/layer_10/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724331104 gradients/bert/encoder/layer_10/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724331158 gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724331212 gradients/bert/encoder/layer_2/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724331241 Mul_1023:0 -3072
1608802724331294 Mul_1033:0 -9437184
1608802724331350 Mul_1055:0 -3072
1608802724331429 gradients/cls/predictions/transform/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724331454 Mul_1097:0 -122112
1608802724331537 gradients/bert/embeddings/MatMul_grad/MatMul_1:0 -6144
1608802724331566 Mul_867:0 -12288
1608802724331627 Mul_883:0 -3072
1608802724331711 gradients/bert/encoder/layer_1/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724331768 gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724331822 gradients/bert/encoder/layer_2/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724331851 Mul_1039:0 -12288
1608802724331905 Mul_1060:0 -3584
1608802724331988 gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724332015 Mul_1102:0 -6144
1608802724332104 gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul_1:0 -3145728
1608802724332134 Mul_888:0 -3072
1608802724332211 gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724332912 gradients/bert/encoder/layer_1/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -20480
1608802724332937 Mul_1044:0 -9437184
1608802724332985 Mul_1065:0 -2359296
1608802724333057 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3840
1608802724333103 gradients/cls/seq_relationship/BiasAdd_grad/BiasAddGrad:0 -256
1608802724333150 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724333207 gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724333268 gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul_1:0 -3407872
1608802724333332 gradients/bert/encoder/layer_2/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -12288
1608802724333386 gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724333439 gradients/bert/encoder/layer_2/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724333493 gradients/bert/encoder/layer_0/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724333545 gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724333598 gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724333657 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724333711 gradients/bert/encoder/layer_1/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724333765 gradients/bert/encoder/layer_1/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724333817 gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724333870 gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724333929 gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724333984 gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724334038 gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724334091 gradients/bert/encoder/layer_3/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724334144 gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724334200 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724334252 gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724334277 Square_71:0 -2359296
1608802724334364 Square_75:0 -2359296
1608802724334435 Square_72:0 -3072
1608802724334501 Square_64:0 -16640
1608802724334570 Square_87:0 -2359296
1608802724334653 Square_83:0 -3072
1608802724334727 Square_61:0 -3072
1608802724334794 Square_65:0 -12582912
1608802724334857 Square_69:0 -2359296
1608802724334919 Square_88:0 -3072
1608802724334991 Square_76:0 -3328
1608802724335059 Square_79:0 -9437184
1608802724335130 Square_84:0 -3072
1608802724335192 Square_89:0 -3145728
1608802724335253 Square_73:0 -2359296
1608802724335312 Square_77:0 -3072
1608802724335381 Square_80:0 -17408
1608802724335445 Square_62:0 -3072
1608802724335509 Square_66:0 -3072
1608802724335570 Square_91:0 -2359296
1608802724335668 gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724335697 Square_78:0 -3072
1608802724335760 Square_81:0 -9437184
1608802724335825 Square_85:0 -2359296
1608802724335888 Square_90:0 -3072
1608802724335952 Square_92:0 -3072
1608802724336017 Square_70:0 -3072
1608802724336094 Square_74:0 -3072
1608802724336182 gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724336210 Square_82:0 -3072
1608802724336280 Square_86:0 -3072
1608802724336343 Square:0 -93763584
1608802724336419 Square_2:0 -1966080
1608802724336485 Square_3:0 -5376
1608802724336551 Square_6:0 -3072
1608802724336616 Square_4:0 -3072
1608802724336677 Square_7:0 -4194304
1608802724336736 Square_10:0 -3072
1608802724336799 Square_5:0 -2359296
1608802724336860 Square_8:0 -3072
1608802724336921 Square_11:0 -2359296
1608802724336983 Square_14:0 -3072
1608802724337044 Square_99:0 -3072
1608802724337107 Square_9:0 -2359296
1608802724337166 Square_12:0 -3072
1608802724337230 Square_15:0 -9437184
1608802724337291 Square_39:0 -2359296
1608802724337389 gradients/bert/encoder/layer_7/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724337452 gradients/bert/encoder/layer_7/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724337483 Square_13:0 -5888
1608802724337542 Square_16:0 -12288
1608802724337635 gradients/bert/encoder/layer_6/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724337660 Square_23:0 -2359296
1608802724337753 gradients/bert/encoder/layer_6/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724337783 Square_31:0 -9437184
1608802724337887 gradients/bert/encoder/layer_6/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724337914 Mul_99:0 -9437184
1608802724338010 gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724338070 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724338136 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -4608
1608802724338164 Square_53:0 -2359296
1608802724338228 Square_56:0 -3072
1608802724338326 gradients/bert/encoder/layer_6/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724338386 gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul_1:0 -3145728
1608802724338451 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724338512 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724338574 gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724338605 Square_34:0 -3072
1608802724338669 Square_36:0 -3072
1608802724338764 gradients/bert/encoder/layer_7/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724338826 gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724338889 gradients/bert/encoder/layer_5/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -12288
1608802724338918 Square_54:0 -3072
1608802724338981 Square_57:0 -2359296
1608802724339080 gradients/bert/encoder/layer_9/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724339140 gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724339200 gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724339261 gradients/bert/encoder/layer_7/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724339321 gradients/bert/encoder/layer_7/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724339389 gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724339461 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724339487 Square_37:0 -2359296
1608802724339582 gradients/bert/encoder/layer_8/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724339611 Square_97:0 -9437184
1608802724339705 gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724339764 gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724339790 Square_58:0 -3072
1608802724339883 gradients/bert/encoder/layer_6/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -20480
1608802724339943 gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724340009 gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724340071 gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3840
1608802724340133 gradients/bert/encoder/layer_7/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -12288
1608802724340198 gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724340259 gradients/bert/encoder/layer_8/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724340326 gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724340354 Square_38:0 -3072
1608802724340425 Square_98:0 -3072
1608802724340517 gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724340549 Square_47:0 -9437184
1608802724340647 gradients/bert/encoder/layer_6/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724340709 gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724340770 gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724340801 Square_169:0 -2359296
1608802724340864 Square_173:0 -3072
1608802724340961 gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul_1:0 -9830400
1608802724340987 Square_181:0 -2359296
1608802724341081 gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724341142 gradients/bert/encoder/layer_8/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724341204 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724341271 gradients/bert/encoder/layer_8/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724341334 gradients/bert/encoder/layer_9/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724341398 gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724341428 Square_43:0 -2359296
1608802724341517 gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724341584 gradients/bert/encoder/layer_10/attention/self/query/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724341612 Square_170:0 -3072
1608802724341673 Square_174:0 -3072
1608802724341742 Square_177:0 -9437184
1608802724341801 Square_182:0 -3072
1608802724341897 gradients/bert/encoder/layer_11/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724341959 gradients/bert/encoder/layer_8/attention/self/key/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724342019 gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724342080 gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724342143 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724342174 Square_28:0 -3072
1608802724342268 gradients/bert/encoder/layer_9/attention/self/value/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724342330 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724342359 Square_40:0 -3072
1608802724342451 gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul_1:0 -2359296
1608802724342482 Square_171:0 -2359296
1608802724342552 Square_175:0 -9437184
1608802724342612 Square_178:0 -3072
1608802724342675 Square_184:0 -3072
1608802724342763 gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul_1:0 -3810048
1608802724342829 gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724342889 gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul_1:0 -3145728
1608802724342952 gradients/bert/pooler/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724343016 gradients/bert/encoder/layer_8/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -14848
1608802724343076 gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724343109 Square_20:0 -3072
1608802724343202 gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul_1:0 -3145728
1608802724343263 gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724343324 gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724343385 gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724343418 Square_41:0 -2359296
1608802724343477 Square_176:0 -12288
1608802724343537 Square_179:0 -3072
1608802724343632 gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul_1:0 -2359296
1608802724343692 gradients/bert/encoder/layer_11/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724343760 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724343822 gradients/bert/encoder/layer_11/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724343849 Square_199:0 -2359296
1608802724343913 Square_202:0 -3072
1608802724344004 gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724344070 gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul_1:0 -2359296
1608802724344139 gradients/bert/encoder/layer_9/attention/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724344200 gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724344260 gradients/bert/encoder/layer_9/output/dense/BiasAdd_grad/BiasAddGrad:0 -3072
1608802724344291 Square_168:0 -3072
1608802724344353 Square_172:0 -3072
1608802724344429 Square_180:0 -3072
1608802724344489 Square_44:0 -3072
1608802724344582 gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3072
1608802724344645 gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724344710 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3584
1608802724344739 Square_200:0 -3072
1608802724344833 gradients/cls/predictions/BiasAdd_grad/BiasAddGrad:0 -122112
1608802724344862 Square_1:0 -6144
1608802724344957 gradients/bert/encoder/layer_9/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -12288
1608802724345022 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Sum:0 -3584
1608802724345047 Square_24:0 -3072
1608802724345108 Square_35:0 -3072
1608802724345174 Square_42:0 -3072
1608802724345264 gradients/bert/encoder/layer_11/intermediate/dense/BiasAdd_grad/BiasAddGrad:0 -13312
1608802724345328 gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724345355 Square_201:0 -3072
1608802724345450 gradients/cls/seq_relationship/MatMul_grad/MatMul_1:0 -6144
1608802724345480 Square_59:0 -2359296
1608802724345570 gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum_1:0 -3072
1608802724345600 Square_19:0 -3072
1608802724345671 Square_32:0 -12288
1608802724345764 gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul_1:0 -12582912
1608802724345825 gradients/bert/pooler/dense/MatMul_grad/MatMul_1:0 -2359296
1608802724345856 Square_45:0 -3072
1608802724345917 Square_205:0 -256
1608802724345979 Square_51:0 -3072
1608802724346041 Square_55:0 -2359296
1608802724346105 Square_183:0 -2359296
1608802724346167 Square_48:0 -12288
1608802724346228 Square_49:0 -9437184
1608802724346290 Square_50:0 -3072
1608802724346350 Square_18:0 -3072
1608802724346417 Square_21:0 -2359296
1608802724346477 Square_25:0 -2359296
1608802724346535 Square_29:0 -3072
1608802724346595 Square_22:0 -3072
1608802724346656 Square_26:0 -3072
1608802724346720 Square_30:0 -3072
1608802724346781 Square_46:0 -3072
1608802724346844 Square_27:0 -2359296
1608802724346905 Square_52:0 -3072
1608802724346970 Square_33:0 -9437184
1608802724347028 Square_60:0 -3072
1608802724347091 Square_63:0 -9437184
1608802724347153 Square_67:0 -3072
1608802724347213 Square_68:0 -3072
1608802724347256 Sqrt_71:0 2359296
1608802724347322 Mul_389:0 -2359296
1608802724347324 Sqrt_75:0 2359296
1608802724347387 Mul_411:0 -2359296
1608802724347394 Sqrt_72:0 3072
1608802724347453 Mul_395:0 -3072
1608802724347456 Sqrt_64:0 20992
1608802724347513 Mul_352:0 -12288
1608802724347515 Sqrt_87:0 2359296
1608802724347769 Mul_475:0 -2359296
1608802724347772 Sqrt_83:0 3072
1608802724347824 Mul_454:0 -3072
1608802724347827 Sqrt_61:0 3072
1608802724347880 Mul_336:0 -3072
1608802724347883 Sqrt_65:0 12582912
1608802724347939 Mul_357:0 -9437184
1608802724347941 Sqrt_69:0 2359296
1608802724347995 Mul_378:0 -2359296
1608802724347997 Sqrt_88:0 3072
1608802724348050 Mul_481:0 -3072
1608802724348052 Sqrt_76:0 3072
1608802724348104 Mul_417:0 -3072
1608802724348106 Sqrt_79:0 9437184
1608802724348159 Mul_432:0 -9437184
1608802724348162 Sqrt_84:0 3072
1608802724348212 Mul_459:0 -3072
1608802724348214 Sqrt_89:0 2359296
1608802724348268 Mul_486:0 -2359296
1608802724348270 Sqrt_73:0 2359296
1608802724348324 Mul_400:0 -2359296
1608802724348327 Sqrt_77:0 3072
1608802724348387 Mul_422:0 -3072
1608802724348390 Sqrt_80:0 12288
1608802724348445 Mul_438:0 -12288
1608802724348448 Sqrt_62:0 3072
1608802724348499 Mul_341:0 -3072
1608802724348501 Sqrt_66:0 3072
1608802724348560 Mul_363:0 -3072
1608802724348563 Sqrt_91:0 2359296
1608802724348623 Mul_497:0 -2359296
1608802724348626 Sqrt_95:0 11796480
1608802724348681 Mul_518:0 -9437184
1608802724348684 Sqrt_78:0 3072
1608802724348738 Mul_427:0 -3072
1608802724348740 Sqrt_81:0 9437184
1608802724348796 Mul_443:0 -9437184
1608802724348798 Sqrt_85:0 2359296
1608802724348852 Mul_464:0 -2359296
1608802724348854 Sqrt_90:0 3072
1608802724348908 Mul_492:0 -3072
1608802724348910 Sqrt_92:0 3072
1608802724348964 Mul_503:0 -3072
1608802724348966 Sqrt_70:0 3072
1608802724349018 Mul_384:0 -3072
1608802724349021 Sqrt_74:0 3072
1608802724349071 Mul_406:0 -3072
1608802724349074 Sqrt_103:0 2359296
1608802724349127 Mul_561:0 -2359296
1608802724349130 Sqrt_82:0 3072
1608802724349188 Mul_449:0 -3072
1608802724349190 Sqrt_86:0 3072
1608802724349240 Mul_470:0 -3072
1608802724349242 Sqrt:0 93769728
1608802724349302 Mul_6:0 -93763584
1608802724349304 Sqrt_2:0 2359296
1608802724349357 Mul_18:0 -1572864
1608802724349360 Sqrt_3:0 3072
1608802724349411 Mul_24:0 -3072
1608802724349414 Sqrt_6:0 3072
1608802724349465 Mul_40:0 -3072
1608802724349468 Sqrt_4:0 3072
1608802724349518 Mul_29:0 -3072
1608802724349520 Sqrt_7:0 2359296
1608802724349574 Mul_45:0 -2359296
1608802724349576 Sqrt_10:0 3072
1608802724349629 Mul_62:0 -3072
1608802724349631 Sqrt_5:0 2359296
1608802724349684 Mul_34:0 -2359296
1608802724349686 Sqrt_8:0 3072
1608802724349736 Mul_51:0 -3072
1608802724349738 Sqrt_11:0 2359296
1608802724349794 Mul_67:0 -2359296
1608802724349797 Sqrt_14:0 3072
1608802724349847 Mul_83:0 -3072
1608802724349851 Sqrt_99:0 3072
1608802724349904 Mul_540:0 -3072
1608802724349906 Sqrt_9:0 2359296
1608802724349960 Mul_56:0 -2359296
1608802724349968 Sqrt_12:0 3072
1608802724350027 Mul_73:0 -3072
1608802724350029 Sqrt_15:0 9437184
1608802724350092 Mul_88:0 -9437184
1608802724350095 Sqrt_39:0 2359296
1608802724350157 Mul_217:0 -2359296
1608802724350160 Sqrt_118:0 3072
1608802724350217 Mul_642:0 -3072
1608802724350219 Sqrt_122:0 3072
1608802724350290 Mul_664:0 -3072
1608802724350292 Sqrt_13:0 3072
1608802724350348 Mul_78:0 -3072
1608802724350351 Sqrt_16:0 12288
1608802724350408 Mul_94:0 -12288
1608802724350411 Sqrt_102:0 3072
1608802724350468 Mul_556:0 -3072
1608802724350471 Sqrt_23:0 2359296
1608802724350534 Mul_131:0 -2359296
1608802724350541 Sqrt_106:0 3072
1608802724350600 Mul_578:0 -3072
1608802724350602 Sqrt_31:0 9437184
1608802724350664 Mul_174:0 -9437184
1608802724350666 Sqrt_114:0 3072
1608802724350724 Mul_621:0 -3072
1608802724350727 Sqrt_17:0 9437184
1608802724350788 Square_17:0 -13369344
1608802724350791 Sqrt_123:0 2359296
1608802724350857 Mul_669:0 -2359296
1608802724350860 Sqrt_126:0 3072
1608802724350917 Mul_685:0 -3072
1608802724350920 Sqrt_93:0 3072
1608802724350977 Mul_508:0 -3072
1608802724350980 Sqrt_53:0 2359296
1608802724351043 Mul_292:0 -2359296
1608802724351046 Sqrt_56:0 3072
1608802724351105 Mul_309:0 -3072
1608802724351111 Sqrt_104:0 3072
1608802724351167 Mul_567:0 -3072
1608802724351170 Sqrt_107:0 2359296
1608802724351231 Mul_583:0 -2359296
1608802724351234 Sqrt_110:0 3072
1608802724351292 Mul_599:0 -3072
1608802724351295 Sqrt_115:0 3072
1608802724351351 Mul_626:0 -3072
1608802724351354 Sqrt_119:0 2359296
1608802724351421 Mul_647:0 -2359296
1608802724351424 Sqrt_34:0 3072
1608802724351483 Mul_191:0 -3072
1608802724351485 Sqrt_36:0 3072
1608802724351543 Mul_201:0 -3072
1608802724351545 Sqrt_130:0 3072
1608802724351604 Mul_707:0 -3072
1608802724351607 Sqrt_94:0 3072
1608802724351666 Mul_513:0 -3072
1608802724351669 Sqrt_96:0 15360
1608802724351728 Mul_524:0 -12288
1608802724351731 Sqrt_54:0 3072
1608802724351790 Mul_298:0 -3072
1608802724351793 Sqrt_57:0 2359296
1608802724351860 Mul_314:0 -2359296
1608802724351863 Sqrt_150:0 3072
1608802724351921 Mul_814:0 -3072
1608802724351924 Sqrt_111:0 12582912
1608802724351987 Mul_604:0 -9437184
1608802724351994 Sqrt_116:0 3072
1608802724352069 Mul_631:0 -3072
1608802724352073 Sqrt_120:0 3072
1608802724352125 Mul_653:0 -3072
1608802724352127 Sqrt_124:0 3072
1608802724352177 Mul_675:0 -3072
1608802724352179 Sqrt_127:0 9437184
1608802724352236 Mul_690:0 -9437184
1608802724352239 Sqrt_131:0 3072
1608802724352291 Mul_712:0 -3072
1608802724352293 Sqrt_37:0 2359296
1608802724352349 Mul_206:0 -2359296
1608802724352352 Sqrt_138:0 3072
1608802724352420 Mul_750:0 -3072
1608802724352423 Sqrt_97:0 9437184
1608802724352486 Mul_529:0 -9437184
1608802724352488 Sqrt_100:0 3072
1608802724352548 Mul_545:0 -3072
1608802724352557 Sqrt_151:0 2359296
1608802724352619 Mul_819:0 -2359296
1608802724352623 Sqrt_58:0 3072
1608802724352679 Mul_320:0 -3072
1608802724352681 Sqrt_112:0 12288
1608802724352738 Mul_610:0 -12288
1608802724352740 Sqrt_117:0 2359296
1608802724352803 Mul_636:0 -2359296
1608802724352805 Sqrt_121:0 2359296
1608802724352869 Mul_658:0 -2359296
1608802724352871 Sqrt_125:0 3072
1608802724352927 Mul_680:0 -3072
1608802724352930 Sqrt_128:0 12288
1608802724352985 Mul_696:0 -12288
1608802724352989 Sqrt_132:0 3072
1608802724353048 Mul_717:0 -3072
1608802724353050 Sqrt_134:0 3072
1608802724353108 Mul_728:0 -3072
1608802724353115 Sqrt_139:0 2359296
1608802724353175 Mul_755:0 -2359296
1608802724353178 Sqrt_38:0 3072
1608802724353235 Mul_212:0 -3072
1608802724353238 Sqrt_98:0 3072
1608802724353294 Mul_535:0 -3072
1608802724353297 Sqrt_101:0 2359296
1608802724353367 Mul_550:0 -2359296
1608802724353370 Sqrt_47:0 9437184
1608802724353436 Mul_260:0 -9437184
1608802724353439 Sqrt_108:0 3072
1608802724353500 Mul_589:0 -3072
1608802724353502 Sqrt_113:0 9437184
1608802724353562 Mul_615:0 -9437184
1608802724353565 Sqrt_165:0 2359296
1608802724353625 Mul_894:0 -2359296
1608802724353628 Sqrt_169:0 2359296
1608802724353698 Mul_916:0 -2359296
1608802724353700 Sqrt_173:0 3072
1608802724353761 Mul_938:0 -3072
1608802724353764 Sqrt_129:0 9437184
1608802724353825 Mul_701:0 -9437184
1608802724353828 Sqrt_181:0 2359296
1608802724353890 Mul_980:0 -2359296
1608802724353892 Sqrt_135:0 2359296
1608802724353952 Mul_733:0 -2359296
1608802724353959 Sqrt_140:0 3072
1608802724354016 Mul_761:0 -3072
1608802724354019 Sqrt_142:0 3072
1608802724354075 Mul_771:0 -3072
1608802724354077 Sqrt_146:0 3072
1608802724354135 Mul_793:0 -3072
1608802724354137 Sqrt_152:0 3072
1608802724354192 Mul_825:0 -3072
1608802724354194 Sqrt_153:0 2359296
1608802724354260 Mul_830:0 -2359296
1608802724354263 Sqrt_43:0 2359296
1608802724354325 Mul_239:0 -2359296
1608802724354328 Sqrt_109:0 3072
1608802724354384 Mul_594:0 -3072
1608802724354387 Sqrt_166:0 3072
1608802724354446 Mul_900:0 -3072
1608802724354449 Sqrt_170:0 3072
1608802724354508 Mul_922:0 -3072
1608802724354514 Sqrt_174:0 3072
1608802724354573 Mul_943:0 -3072
1608802724354575 Sqrt_177:0 9437184
1608802724354635 Mul_959:0 -9437184
1608802724354638 Sqrt_182:0 3072
1608802724354697 Mul_986:0 -3072
1608802724354700 Sqrt_186:0 3072
1608802724354759 Mul_1008:0 -3072
1608802724354763 Sqrt_136:0 3072
1608802724354822 Mul_739:0 -3072
1608802724354825 Sqrt_141:0 3072
1608802724354890 Mul_766:0 -3072
1608802724354893 Sqrt_143:0 9437184
1608802724354954 Mul_776:0 -9437184
1608802724354958 Sqrt_147:0 3072
1608802724355014 Mul_798:0 -3072
1608802724355016 Sqrt_28:0 3072
1608802724355075 Mul_159:0 -3072
1608802724355081 Sqrt_154:0 3072
1608802724355138 Mul_836:0 -3072
1608802724355140 Sqrt_157:0 3072
1608802724355196 Mul_852:0 -3072
1608802724355199 Sqrt_40:0 3072
1608802724355256 Mul_223:0 -3072
1608802724355258 Sqrt_167:0 2359296
1608802724355319 Mul_905:0 -2359296
1608802724355322 Sqrt_171:0 2359296
1608802724355390 Mul_927:0 -2359296
1608802724355392 Sqrt_175:0 9437184
1608802724355456 Mul_948:0 -9437184
1608802724355459 Sqrt_178:0 3072
1608802724355515 Mul_965:0 -3072
1608802724355518 Sqrt_184:0 3072
1608802724355574 Mul_997:0 -3072
1608802724355576 Sqrt_187:0 2359296
1608802724355642 Mul_1013:0 -2359296
1608802724355644 Sqrt_133:0 2359296
1608802724355706 Mul_722:0 -2359296
1608802724355708 Sqrt_137:0 2359296
1608802724355769 Mul_744:0 -2359296
1608802724355771 Sqrt_198:0 3072
1608802724355830 Mul_1072:0 -3072
1608802724355833 Sqrt_144:0 12288
1608802724355889 Mul_782:0 -12288
1608802724355896 Sqrt_148:0 3072
1608802724355954 Mul_803:0 -3072
1608802724355957 Sqrt_20:0 3072
1608802724356015 Mul_115:0 -3072
1608802724356017 Sqrt_155:0 2359296
1608802724356077 Mul_841:0 -2359296
1608802724356079 Sqrt_158:0 3072
1608802724356138 Mul_857:0 -3072
1608802724356141 Sqrt_161:0 9437184
1608802724356208 Mul_873:0 -9437184
1608802724356211 Sqrt_105:0 2359296
1608802724356272 Mul_572:0 -3145728
1608802724356274 Sqrt_41:0 2359296
1608802724356335 Mul_228:0 -2359296
1608802724356337 Sqrt_176:0 12288
1608802724356404 Mul_954:0 -12288
1608802724356406 Sqrt_179:0 3072
1608802724356471 Mul_970:0 -3072
1608802724356479 Sqrt_185:0 2359296
1608802724356540 Mul_1002:0 -2359296
1608802724356543 Sqrt_188:0 3072
1608802724356600 Mul_1019:0 -3072
1608802724356602 Sqrt_190:0 3072
1608802724356661 Mul_1029:0 -3072
1608802724356664 Sqrt_194:0 3072
1608802724356719 Mul_1051:0 -3072
1608802724356721 Sqrt_199:0 2359296
1608802724356786 Mul_1077:0 -2359296
1608802724356788 Sqrt_202:0 3072
1608802724356848 Mul_1093:0 -3072
1608802724356851 Sqrt_145:0 9437184
1608802724356912 Mul_787:0 -9437184
1608802724356914 Sqrt_149:0 2359296
1608802724356976 Mul_808:0 -2359296
1608802724356978 Sqrt_156:0 3072
1608802724357035 Mul_847:0 -3072
1608802724357042 Sqrt_159:0 9437184
1608802724357104 Mul_862:0 -9437184
1608802724357107 Sqrt_162:0 3072
1608802724357163 Mul_879:0 -3072
1608802724357167 Sqrt_168:0 3072
1608802724357223 Mul_911:0 -3072
1608802724357225 Sqrt_172:0 3072
1608802724357282 Mul_933:0 -3072
1608802724357289 Sqrt_180:0 3072
1608802724357346 Mul_975:0 -3072
1608802724357350 Sqrt_44:0 3072
1608802724357408 Mul_245:0 -3072
1608802724357411 Sqrt_189:0 3072
1608802724357468 Mul_1024:0 -3072
1608802724357471 Sqrt_191:0 9437184
1608802724357533 Mul_1034:0 -9437184
1608802724357536 Sqrt_195:0 3072
1608802724357593 Mul_1056:0 -3072
1608802724357599 Sqrt_200:0 3072
1608802724357658 Mul_1083:0 -3072
1608802724357661 Sqrt_203:0 169472
1608802724357722 Mul_1098:0 -122112
1608802724357725 Sqrt_1:0 6144
1608802724357784 Mul_12:0 -6144
1608802724357786 Sqrt_160:0 23040
1608802724357843 Mul_868:0 -12288
1608802724357846 Sqrt_163:0 3072
1608802724357907 Mul_884:0 -3072
1608802724357909 Sqrt_24:0 3072
1608802724357976 Mul_137:0 -3072
1608802724357979 Sqrt_35:0 3072
1608802724358037 Mul_196:0 -3072
1608802724358040 Sqrt_42:0 3072
1608802724358097 Mul_234:0 -3072
1608802724358099 Sqrt_192:0 12288
1608802724358158 Mul_1040:0 -12288
1608802724358165 Sqrt_196:0 3072
1608802724358222 Mul_1061:0 -3072
1608802724358225 Sqrt_201:0 3072
1608802724358280 Mul_1088:0 -3072
1608802724358283 Sqrt_204:0 6144
1608802724358337 Mul_1103:0 -6144
1608802724358340 Sqrt_59:0 2359296
1608802724358400 Mul_325:0 -2359296
1608802724358403 Sqrt_164:0 3072
1608802724358463 Mul_889:0 -3072
1608802724358466 Sqrt_19:0 3072
1608802724358523 Mul_110:0 -3072
1608802724358525 Sqrt_32:0 12288
1608802724358582 Mul_180:0 -12288
1608802724358584 Sqrt_193:0 9437184
1608802724358644 Mul_1045:0 -9437184
1608802724358647 Sqrt_197:0 2359296
1608802724358707 Mul_1066:0 -2359296
1608802724358710 Sqrt_45:0 3072
1608802724358775 Mul_250:0 -3072
1608802724358778 Sqrt_205:0 256
1608802724358835 Mul_1109:0 -256
1608802724358838 Sqrt_51:0 3072
1608802724358898 Mul_282:0 -3072
1608802724358901 Sqrt_55:0 2359296
1608802724358962 Mul_303:0 -2359296
1608802724358964 Sqrt_183:0 2359296
1608802724359023 Mul_991:0 -2359296
1608802724359029 Sqrt_48:0 12288
1608802724359088 Mul_266:0 -12288
1608802724359090 Sqrt_49:0 9437184
1608802724359152 Mul_271:0 -9437184
1608802724359155 Sqrt_50:0 3072
1608802724359213 Mul_277:0 -3072
1608802724359216 Sqrt_18:0 3072
1608802724359274 Mul_105:0 -3072
1608802724359277 Sqrt_21:0 2359296
1608802724359342 Mul_120:0 -2359296
1608802724359345 Sqrt_25:0 2359296
1608802724359406 Mul_142:0 -2359296
1608802724359408 Sqrt_29:0 3072
1608802724359518 Mul_164:0 -3072
1608802724359521 Sqrt_22:0 3072
1608802724359575 Mul_126:0 -3072
1608802724359578 Sqrt_26:0 3072
1608802724359628 Mul_148:0 -3072
1608802724359630 Sqrt_30:0 3072
1608802724359680 Mul_169:0 -3072
1608802724359683 Sqrt_46:0 3072
1608802724359734 Mul_255:0 -3072
1608802724359736 Sqrt_27:0 2359296
1608802724359790 Mul_153:0 -2359296
1608802724359793 Sqrt_52:0 3072
1608802724359849 Mul_287:0 -3072
1608802724359852 Sqrt_33:0 9437184
1608802724359909 Mul_185:0 -9437184
1608802724359912 Sqrt_60:0 3072
1608802724359961 Mul_331:0 -3072
1608802724359964 Sqrt_63:0 9437184
1608802724360019 Mul_346:0 -9437184
1608802724360022 Sqrt_67:0 3072
1608802724360072 Mul_368:0 -3072
1608802724360074 Sqrt_68:0 3072
1608802724360124 Mul_373:0 -3072
1608802724367148 Sqrt_71:0 -2359296
1608802724367179 Sqrt_75:0 -2359296
1608802724367205 Sqrt_72:0 -3072
1608802724367237 Sqrt_64:0 -20992
1608802724367266 Sqrt_87:0 -2359296
1608802724367297 Sqrt_83:0 -3072
1608802724367327 Sqrt_61:0 -3072
1608802724367353 Sqrt_65:0 -12582912
1608802724367383 Sqrt_69:0 -2359296
1608802724367409 Sqrt_88:0 -3072
1608802724367438 Sqrt_76:0 -3072
1608802724367467 Sqrt_79:0 -9437184
1608802724367494 Sqrt_84:0 -3072
1608802724367522 Sqrt_89:0 -2359296
1608802724367548 Sqrt_73:0 -2359296
1608802724367578 Sqrt_77:0 -3072
1608802724367606 Sqrt_80:0 -12288
1608802724367631 Sqrt_62:0 -3072
1608802724367659 Sqrt_66:0 -3072
1608802724367683 Sqrt_91:0 -2359296
1608802724367715 Sqrt_95:0 -11796480
1608802724367745 Sqrt_78:0 -3072
1608802724367770 Sqrt_81:0 -9437184
1608802724367799 Sqrt_85:0 -2359296
1608802724367822 Sqrt_90:0 -3072
1608802724367851 Sqrt_92:0 -3072
1608802724367878 Sqrt_70:0 -3072
1608802724367904 Sqrt_74:0 -3072
1608802724367932 Sqrt_103:0 -2359296
1608802724367958 Sqrt_82:0 -3072
1608802724367987 Sqrt_86:0 -3072
1608802724368018 Sqrt:0 -93769728
1608802724368043 Sqrt_2:0 -2359296
1608802724368076 Sqrt_3:0 -3072
1608802724368100 Sqrt_6:0 -3072
1608802724368130 Sqrt_4:0 -3072
1608802724368158 Sqrt_7:0 -2359296
1608802724368183 Sqrt_10:0 -3072
1608802724368212 Sqrt_5:0 -2359296
1608802724368235 Sqrt_8:0 -3072
1608802724368263 Sqrt_11:0 -2359296
1608802724368292 Sqrt_14:0 -3072
1608802724368317 Sqrt_99:0 -3072
1608802724368346 Sqrt_9:0 -2359296
1608802724368394 Sqrt_12:0 -3072
1608802724368427 Sqrt_15:0 -9437184
1608802724368460 Sqrt_39:0 -2359296
1608802724368488 Sqrt_118:0 -3072
1608802724368525 Sqrt_122:0 -3072
1608802724368550 Sqrt_13:0 -3072
1608802724368578 Sqrt_16:0 -12288
1608802724368608 Sqrt_102:0 -3072
1608802724368635 Sqrt_23:0 -2359296
1608802724368665 Sqrt_106:0 -3072
1608802724368690 Sqrt_31:0 -9437184
1608802724368720 Sqrt_114:0 -3072
1608802724368751 Sqrt_17:0 -9437184
1608802724368777 Sqrt_123:0 -2359296
1608802724368806 Sqrt_126:0 -3072
1608802724368832 Sqrt_93:0 -3072
1608802724368862 Sqrt_53:0 -2359296
1608802724368891 Sqrt_56:0 -3072
1608802724368917 Sqrt_104:0 -3072
1608802724368946 Sqrt_107:0 -2359296
1608802724368971 Sqrt_110:0 -3072
1608802724369001 Sqrt_115:0 -3072
1608802724369030 Sqrt_119:0 -2359296
1608802724369055 Sqrt_34:0 -3072
1608802724369085 Sqrt_36:0 -3072
1608802724369112 Sqrt_130:0 -3072
1608802724369143 Sqrt_94:0 -3072
1608802724369170 Sqrt_96:0 -15360
1608802724369196 Sqrt_54:0 -3072
1608802724369224 Sqrt_57:0 -2359296
1608802724369250 Sqrt_150:0 -3072
1608802724369281 Sqrt_111:0 -12582912
1608802724369310 Sqrt_116:0 -3072
1608802724369335 Sqrt_120:0 -3072
1608802724369362 Sqrt_124:0 -3072
1608802724369387 Sqrt_127:0 -9437184
1608802724369416 Sqrt_131:0 -3072
1608802724369444 Sqrt_37:0 -2359296
1608802724369470 Sqrt_138:0 -3072
1608802724369499 Sqrt_97:0 -9437184
1608802724369522 Sqrt_100:0 -3072
1608802724369552 Sqrt_151:0 -2359296
1608802724369581 Sqrt_58:0 -3072
1608802724369608 Sqrt_112:0 -12288
1608802724369636 Sqrt_117:0 -2359296
1608802724369661 Sqrt_121:0 -2359296
1608802724369689 Sqrt_125:0 -3072
1608802724369718 Sqrt_128:0 -12288
1608802724369745 Sqrt_132:0 -3072
1608802724369773 Sqrt_134:0 -3072
1608802724369797 Sqrt_139:0 -2359296
1608802724369826 Sqrt_38:0 -3072
1608802724369854 Sqrt_98:0 -3072
1608802724369880 Sqrt_101:0 -2359296
1608802724369908 Sqrt_47:0 -9437184
1608802724369933 Sqrt_108:0 -3072
1608802724369972 Sqrt_113:0 -9437184
1608802724370000 Sqrt_165:0 -2359296
1608802724370028 Sqrt_169:0 -2359296
1608802724370059 Sqrt_173:0 -3072
1608802724370086 Sqrt_129:0 -9437184
1608802724370115 Sqrt_181:0 -2359296
1608802724370143 Sqrt_135:0 -2359296
1608802724370169 Sqrt_140:0 -3072
1608802724370198 Sqrt_142:0 -3072
1608802724370222 Sqrt_146:0 -3072
1608802724370251 Sqrt_152:0 -3072
1608802724370279 Sqrt_153:0 -2359296
1608802724370305 Sqrt_43:0 -2359296
1608802724370333 Sqrt_109:0 -3072
1608802724370358 Sqrt_166:0 -3072
1608802724370388 Sqrt_170:0 -3072
1608802724370415 Sqrt_174:0 -3072
1608802724370442 Sqrt_177:0 -9437184
1608802724370470 Sqrt_182:0 -3072
1608802724370497 Sqrt_186:0 -3072
1608802724370525 Sqrt_136:0 -3072
1608802724370549 Sqrt_141:0 -3072
1608802724370578 Sqrt_143:0 -9437184
1608802724370607 Sqrt_147:0 -3072
1608802724370633 Sqrt_28:0 -3072
1608802724370662 Sqrt_154:0 -3072
1608802724370686 Sqrt_157:0 -3072
1608802724370717 Sqrt_40:0 -3072
1608802724370747 Sqrt_167:0 -2359296
1608802724370776 Sqrt_171:0 -2359296
1608802724370806 Sqrt_175:0 -9437184
1608802724370831 Sqrt_178:0 -3072
1608802724370861 Sqrt_184:0 -3072
1608802724370890 Sqrt_187:0 -2359296
1608802724370918 Sqrt_133:0 -2359296
1608802724370947 Sqrt_137:0 -2359296
1608802724370972 Sqrt_198:0 -3072
1608802724371003 Sqrt_144:0 -12288
1608802724371031 Sqrt_148:0 -3072
1608802724371057 Sqrt_20:0 -3072
1608802724371088 Sqrt_155:0 -2359296
1608802724371113 Sqrt_158:0 -3072
1608802724371141 Sqrt_161:0 -9437184
1608802724371170 Sqrt_105:0 -2359296
1608802724371196 Sqrt_41:0 -2359296
1608802724371228 Sqrt_176:0 -12288
1608802724371252 Sqrt_179:0 -3072
1608802724371283 Sqrt_185:0 -2359296
1608802724371311 Sqrt_188:0 -3072
1608802724371336 Sqrt_190:0 -3072
1608802724371373 Sqrt_194:0 -3072
1608802724371397 Sqrt_199:0 -2359296
1608802724371426 Sqrt_202:0 -3072
1608802724371456 Sqrt_145:0 -9437184
1608802724371482 Sqrt_149:0 -2359296
1608802724371511 Sqrt_156:0 -3072
1608802724371536 Sqrt_159:0 -9437184
1608802724371565 Sqrt_162:0 -3072
1608802724371594 Sqrt_168:0 -3072
1608802724371621 Sqrt_172:0 -3072
1608802724371647 Sqrt_180:0 -3072
1608802724371672 Sqrt_44:0 -3072
1608802724371703 Sqrt_189:0 -3072
1608802724371731 Sqrt_191:0 -9437184
1608802724371758 Sqrt_195:0 -3072
1608802724371787 Sqrt_200:0 -3072
1608802724371812 Sqrt_203:0 -169472
1608802724371842 Sqrt_1:0 -6144
1608802724371866 Sqrt_160:0 -23040
1608802724371897 Sqrt_163:0 -3072
1608802724371928 Sqrt_24:0 -3072
1608802724371954 Sqrt_35:0 -3072
1608802724371983 Sqrt_42:0 -3072
1608802724372009 Sqrt_192:0 -12288
1608802724372041 Sqrt_196:0 -3072
1608802724372070 Sqrt_201:0 -3072
1608802724372096 Sqrt_204:0 -6144
1608802724372128 Sqrt_59:0 -2359296
1608802724372152 Sqrt_164:0 -3072
1608802724372183 Sqrt_19:0 -3072
1608802724372211 Sqrt_32:0 -12288
1608802724372235 Sqrt_193:0 -9437184
1608802724372265 Sqrt_197:0 -2359296
1608802724372289 Sqrt_45:0 -3072
1608802724372319 Sqrt_205:0 -256
1608802724372350 Sqrt_51:0 -3072
1608802724372387 Sqrt_55:0 -2359296
1608802724372416 Sqrt_183:0 -2359296
1608802724372443 Sqrt_48:0 -12288
1608802724372475 Sqrt_49:0 -9437184
1608802724372503 Sqrt_50:0 -3072
1608802724372530 Sqrt_18:0 -3072
1608802724372559 Sqrt_21:0 -2359296
1608802724372583 Sqrt_25:0 -2359296
1608802724372615 Sqrt_29:0 -3072
1608802724372640 Sqrt_22:0 -3072
1608802724372670 Sqrt_26:0 -3072
1608802724372699 Sqrt_30:0 -3072
1608802724372724 Sqrt_46:0 -3072
1608802724372761 Sqrt_27:0 -2359296
1608802724372786 Sqrt_52:0 -3072
1608802724372818 Sqrt_33:0 -9437184
1608802724372847 Sqrt_60:0 -3072
1608802724372873 Sqrt_63:0 -9437184
1608802724372902 Sqrt_67:0 -3072
1608802724372927 Sqrt_68:0 -3072
1608802724372958 Mul_387:0 -2359296
1608802724372991 Mul_409:0 -2359296
1608802724373075 Mul_473:0 -2359296
1608802724373165 Mul_355:0 -9437184
1608802724373197 Mul_376:0 -2359296
1608802724373278 Mul_430:0 -9437184
1608802724373332 Mul_484:0 -2359296
1608802724373361 Mul_398:0 -2359296
1608802724373494 Mul_495:0 -2359296
1608802724373524 Mul_516:0 -9437184
1608802724373577 Mul_441:0 -9437184
1608802724373606 Mul_462:0 -2359296
1608802724373739 Mul_559:0 -2359296
1608802724373817 Mul_4:0 -93763584
1608802724373847 Mul_16:0 -1572864
1608802724373951 Mul_43:0 -2359296
1608802724374010 Mul_32:0 -2359296
1608802724374062 Mul_65:0 -2359296
1608802724374152 Mul_54:0 -2359296
1608802724374207 Mul_86:0 -9437184
1608802724374232 Mul_215:0 -2359296
1608802724374397 Mul_129:0 -2359296
1608802724374451 Mul_172:0 -9437184
1608802724374504 Mul_97:0 -9437184
1608802724374533 Mul_667:0 -2359296
1608802724374616 Mul_290:0 -2359296
1608802724374697 Mul_581:0 -2359296
1608802724374773 Mul_645:0 -2359296
1608802724374961 Mul_312:0 -2359296
1608802724375018 Mul_602:0 -9437184
1608802724375122 Mul_688:0 -9437184
1608802724375177 Mul_204:0 -2359296
1608802724375233 Mul_527:0 -9437184
1608802724375286 Mul_817:0 -2359296
1608802724375366 Mul_634:0 -2359296
1608802724375392 Mul_656:0 -2359296
1608802724375529 Mul_753:0 -2359296
1608802724375616 Mul_548:0 -2359296
1608802724375646 Mul_258:0 -9437184
1608802724375701 Mul_613:0 -9437184
1608802724375727 Mul_892:0 -2359296
1608802724375756 Mul_914:0 -2359296
1608802724375811 Mul_699:0 -9437184
1608802724375839 Mul_978:0 -2359296
1608802724375864 Mul_731:0 -2359296
1608802724375996 Mul_828:0 -2359296
1608802724376028 Mul_237:0 -2359296
1608802724376165 Mul_957:0 -9437184
1608802724376296 Mul_774:0 -9437184
1608802724376464 Mul_903:0 -2359296
1608802724376495 Mul_925:0 -2359296
1608802724376526 Mul_946:0 -9437184
1608802724376603 Mul_1011:0 -2359296
1608802724376633 Mul_720:0 -2359296
1608802724376662 Mul_742:0 -2359296
1608802724376797 Mul_839:0 -2359296
1608802724376854 Mul_871:0 -9437184
1608802724376879 Mul_570:0 -2359296
1608802724376918 Mul_226:0 -2359296
1608802724376999 Mul_1000:0 -2359296
1608802724377109 Mul_1075:0 -2359296
1608802724377162 Mul_785:0 -9437184
1608802724377191 Mul_806:0 -2359296
1608802724377246 Mul_860:0 -9437184
1608802724377429 Mul_1032:0 -9437184
1608802724377538 Mul_10:0 -6144
1608802724377769 Mul_1101:0 -8448
1608802724377798 Mul_323:0 -2359296
1608802724377904 Mul_1043:0 -9437184
1608802724377932 Mul_1064:0 -2359296
1608802724378037 Mul_301:0 -2359296
1608802724378066 Mul_989:0 -2359296
1608802724378121 Mul_269:0 -9437184
1608802724378201 Mul_118:0 -2359296
1608802724378228 Mul_140:0 -2359296
1608802724378391 Mul_151:0 -2359296
1608802724378444 Mul_183:0 -9437184
1608802724378497 Mul_344:0 -9437184
1608802724391441 PolynomialDecay/Cast_2:0 -256
1608802724391767 Mul_393:0 -3072
1608802724391843 Mul_350:0 -12288
1608802724391991 Mul_452:0 -3072
1608802724392061 Mul_334:0 -3072
1608802724392260 Mul_479:0 -3072
1608802724392333 Mul_415:0 -3072
1608802724392484 Mul_457:0 -3072
1608802724392680 Mul_420:0 -3072
1608802724392748 Mul_436:0 -12288
1608802724392823 Mul_339:0 -3072
1608802724392888 Mul_361:0 -3072
1608802724393087 Mul_425:0 -3072
1608802724393280 Mul_490:0 -3072
1608802724393352 Mul_501:0 -3072
1608802724393417 Mul_382:0 -3072
1608802724393492 Mul_404:0 -3072
1608802724393628 Mul_447:0 -3072
1608802724393701 Mul_468:0 -3072
1608802724393908 Mul_22:0 -3072
1608802724393976 Mul_38:0 -3072
1608802724394048 Mul_27:0 -3072
1608802724394182 Mul_60:0 -3072
1608802724394316 Mul_49:0 -3072
1608802724394454 Mul_81:0 -3072
1608802724394530 Mul_538:0 -3072
1608802724394663 Mul_71:0 -3072
1608802724394866 Mul_640:0 -3072
1608802724394939 Mul_662:0 -3072
1608802724395007 Mul_76:0 -3072
1608802724395080 Mul_92:0 -12288
1608802724395146 Mul_554:0 -3072
1608802724395290 Mul_576:0 -3072
1608802724395424 Mul_619:0 -3072
1608802724395630 Mul_683:0 -3072
1608802724395699 Mul_506:0 -3072
1608802724395840 Mul_307:0 -3072
1608802724395916 Mul_565:0 -3072
1608802724396052 Mul_597:0 -3072
1608802724396126 Mul_624:0 -3072
1608802724396258 Mul_189:0 -3072
1608802724396329 Mul_199:0 -3072
1608802724396409 Mul_705:0 -3072
1608802724396491 Mul_511:0 -3072
1608802724396552 Mul_522:0 -12288
1608802724396620 Mul_296:0 -3072
1608802724396749 Mul_812:0 -3072
1608802724396877 Mul_629:0 -3072
1608802724396939 Mul_651:0 -3072
1608802724397003 Mul_673:0 -3072
1608802724397124 Mul_710:0 -3072
1608802724397245 Mul_748:0 -3072
1608802724397369 Mul_543:0 -3072
1608802724397499 Mul_318:0 -3072
1608802724397561 Mul_608:0 -12288
1608802724397741 Mul_678:0 -3072
1608802724397807 Mul_694:0 -12288
1608802724397867 Mul_715:0 -3072
1608802724397935 Mul_726:0 -3072
1608802724398061 Mul_210:0 -3072
1608802724398128 Mul_533:0 -3072
1608802724398309 Mul_587:0 -3072
1608802724398548 Mul_936:0 -3072
1608802724398787 Mul_759:0 -3072
1608802724398849 Mul_769:0 -3072
1608802724398919 Mul_791:0 -3072
1608802724398979 Mul_823:0 -3072
1608802724399157 Mul_592:0 -3072
1608802724399223 Mul_898:0 -3072
1608802724399283 Mul_920:0 -3072
1608802724399354 Mul_941:0 -3072
1608802724399476 Mul_984:0 -3072
1608802724399542 Mul_1006:0 -3072
1608802724399629 Mul_737:0 -3072
1608802724399698 Mul_764:0 -3072
1608802724399824 Mul_796:0 -3072
1608802724399890 Mul_157:0 -3072
1608802724399952 Mul_834:0 -3072
1608802724400022 Mul_850:0 -3072
1608802724400082 Mul_221:0 -3072
1608802724400316 Mul_963:0 -3072
1608802724400393 Mul_995:0 -3072
1608802724400623 Mul_1070:0 -3072
1608802724400690 Mul_780:0 -12288
1608802724400751 Mul_801:0 -3072
1608802724400818 Mul_113:0 -3072
1608802724400942 Mul_855:0 -3072
1608802724401168 Mul_952:0 -12288
1608802724401241 Mul_968:0 -3072
1608802724401362 Mul_1017:0 -3072
1608802724401431 Mul_1027:0 -3072
1608802724401490 Mul_1049:0 -3072
1608802724401627 Mul_1091:0 -3072
1608802724401801 Mul_845:0 -3072
1608802724401929 Mul_877:0 -3072
1608802724401991 Mul_909:0 -3072
1608802724402063 Mul_931:0 -3072
1608802724402124 Mul_973:0 -3072
1608802724402192 Mul_243:0 -3072
1608802724402257 Mul_1022:0 -3072
1608802724402383 Mul_1054:0 -3072
1608802724402442 Mul_1081:0 -3072
1608802724402509 Mul_1096:0 -122112
1608802724402633 Mul_866:0 -12288
1608802724402699 Mul_882:0 -3072
1608802724402758 Mul_135:0 -3072
1608802724402824 Mul_194:0 -3072
1608802724402888 Mul_232:0 -3072
1608802724402949 Mul_1038:0 -12288
1608802724403019 Mul_1059:0 -3072
1608802724403078 Mul_1086:0 -3072
1608802724403255 Mul_887:0 -3072
1608802724403322 Mul_108:0 -3072
1608802724403383 Mul_178:0 -12288
1608802724403560 Mul_248:0 -3072
1608802724403627 Mul_1107:0 -256
1608802724403687 Mul_280:0 -3072
1608802724403879 Mul_264:0 -12288
1608802724404001 Mul_275:0 -3072
1608802724404070 Mul_103:0 -3072
1608802724404250 Mul_162:0 -3072
1608802724404310 Mul_124:0 -3072
1608802724404387 Mul_146:0 -3072
1608802724404455 Mul_167:0 -3072
1608802724404517 Mul_253:0 -3072
1608802724404639 Mul_285:0 -3072
1608802724404766 Mul_329:0 -3072
1608802724404888 Mul_366:0 -3072
1608802724404948 Mul_371:0 -3072
1608802724405020 mul_391:0 -2359296
1608802724405086 mul_413:0 -2359296
1608802724405147 mul_477:0 -2359296
1608802724405214 mul_359:0 -9437184
1608802724405273 mul_380:0 -2359296
1608802724405339 mul_434:0 -9437184
1608802724405403 mul_488:0 -2359296
1608802724405461 mul_402:0 -2359296
1608802724405524 mul_499:0 -2359296
1608802724405587 mul_520:0 -9437184
1608802724405648 mul_445:0 -9437184
1608802724405712 mul_466:0 -2359296
1608802724405771 mul_563:0 -2359296
1608802724405850 mul_8:0 -93763584
1608802724405917 mul_20:0 -1572864
1608802724405978 mul_47:0 -2359296
1608802724406042 mul_36:0 -2359296
1608802724406100 mul_69:0 -2359296
1608802724406164 mul_58:0 -2359296
1608802724406227 mul_90:0 -9437184
1608802724406289 mul_219:0 -2359296
1608802724406354 mul_133:0 -2359296
1608802724406412 mul_176:0 -9437184
1608802724406478 mul_101:0 -9437184
1608802724406541 mul_671:0 -2359296
1608802724406605 mul_294:0 -2359296
1608802724406672 mul_585:0 -2359296
1608802724406731 mul_649:0 -2359296
1608802724406799 mul_316:0 -2359296
1608802724406865 mul_606:0 -9437184
1608802724406926 mul_692:0 -9437184
1608802724406993 mul_208:0 -2359296
1608802724407053 mul_531:0 -9437184
1608802724407119 mul_821:0 -2359296
1608802724407182 mul_638:0 -2359296
1608802724407240 mul_660:0 -2359296
1608802724407306 mul_757:0 -2359296
1608802724407369 mul_552:0 -2359296
1608802724407432 mul_262:0 -9437184
1608802724407498 mul_617:0 -9437184
1608802724407561 mul_896:0 -2359296
1608802724407629 mul_918:0 -2359296
1608802724407692 mul_703:0 -9437184
1608802724407753 mul_982:0 -2359296
1608802724407818 mul_735:0 -2359296
1608802724407878 mul_832:0 -2359296
1608802724407945 mul_241:0 -2359296
1608802724408008 mul_961:0 -9437184
1608802724408068 mul_778:0 -9437184
1608802724408133 mul_907:0 -2359296
1608802724408191 mul_929:0 -2359296
1608802724408259 mul_950:0 -9437184
1608802724408324 mul_1015:0 -2359296
1608802724408398 mul_724:0 -2359296
1608802724408480 mul_746:0 -2359296
1608802724408537 mul_843:0 -2359296
1608802724408599 mul_875:0 -9437184
1608802724408663 mul_574:0 -2359296
1608802724408720 mul_230:0 -2359296
1608802724408782 mul_1004:0 -2359296
1608802724408837 mul_1079:0 -2359296
1608802724408898 mul_789:0 -9437184
1608802724408957 mul_810:0 -2359296
1608802724409014 mul_864:0 -9437184
1608802724409074 mul_1036:0 -9437184
1608802724409129 mul_14:0 -6144
1608802724409192 mul_1105:0 -6144
1608802724409253 mul_327:0 -2359296
1608802724409309 mul_1047:0 -9437184
1608802724409372 mul_1068:0 -2359296
1608802724409426 mul_305:0 -2359296
1608802724409491 mul_993:0 -2359296
1608802724409553 mul_273:0 -9437184
1608802724409630 mul_122:0 -2359296
1608802724409694 mul_144:0 -2359296
1608802724409749 mul_155:0 -2359296
1608802724409816 mul_187:0 -9437184
1608802724409878 mul_348:0 -9437184
