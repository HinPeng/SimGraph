edge_3447_Reshape_1@@MemcpyHtoD
edge_1822_bert/embeddings/Reshape@@MemcpyHtoD
edge_1721_bert/embeddings/Reshape_2@@MemcpyHtoD
edge_1725_cls/predictions/Reshape@@MemcpyHtoD
edge_686_IteratorGetNext@@MemcpyHtoD
edge_1731_cls/seq_relationship/Reshape@@MemcpyHtoD
edge_13_bert/embeddings/assert_less_equal/All@@MemcpyDtoH
edge_1735_bert/encoder/Reshape@@MemcpyHtoD
edge_1712_Cast@@MemcpyDtoH
edge_1817_Less@@MemcpyHtoD
global_step/add/_2560
add_1/_2558
_SOURCE
bert/encoder/layer_5/attention/output/dense/kernel/adam_v
global_step
bert/embeddings/Reshape_1/shape
bert/embeddings/assert_less_equal/y
bert/embeddings/assert_less_equal/All
bert/encoder/layer_0/attention/self/key/kernel
bert/encoder/ones/shape_as_tensor
bert/embeddings/dropout/truediv
bert/encoder/layer_6/attention/self/key/kernel/adam_m
bert/encoder/layer_0/attention/self/ExpandDims/dim
bert/embeddings/position_embeddings
bert/encoder/layer_0/attention/output/LayerNorm/gamma
bert/encoder/layer_6/attention/self/value/kernel/adam_v
mul_552/x
bert/encoder/layer_1/attention/output/LayerNorm/beta
bert/encoder/layer_1/output/LayerNorm/beta
bert/encoder/layer_2/attention/self/value/kernel
bert/encoder/layer_0/attention/output/dense/kernel
bert/encoder/layer_2/attention/self/query/bias
bert/encoder/layer_2/attention/self/value/bias
bert/encoder/layer_1/attention/self/key/kernel
bert/encoder/layer_1/output/dense/bias
bert/encoder/layer_0/attention/output/dense/bias
Mul_546/x
bert/encoder/layer_0/output/LayerNorm/beta
bert/encoder/layer_1/attention/self/key/bias
bert/encoder/layer_7/attention/output/dense/bias
bert/encoder/layer_7/attention/self/query/bias/adam_v
bert/encoder/layer_7/output/dense/kernel
bert/encoder/layer_8/attention/output/LayerNorm/beta
bert/encoder/layer_8/output/dense/kernel
bert/encoder/layer_9/attention/self/key/kernel
bert/encoder/layer_9/attention/output/LayerNorm/beta
bert/encoder/layer_9/output/LayerNorm/beta
bert/encoder/layer_10/attention/self/value/kernel
bert/encoder/layer_10/intermediate/dense/kernel
bert/encoder/layer_11/attention/self/query/bias
bert/encoder/layer_11/attention/output/dense/bias
bert/encoder/layer_11/intermediate/dense/bias
bert/pooler/strided_slice/stack
bert/encoder/layer_7/intermediate/dense/bias/adam_m
cls/predictions/one_hot/depth
Const_2
bert/encoder/layer_7/output/dense/bias/adam_m
gradients/cls/predictions/transform/dense/mul_grad/Mul_1
bert/encoder/layer_8/attention/self/query/kernel/adam_v
cls/seq_relationship/output_weights/adam_m
bert/encoder/layer_8/attention/output/dense/bias/adam_m
bert/encoder/layer_8/attention/self/query/bias/adam_m
gradients/bert/pooler/Squeeze_grad/Shape
bert/encoder/layer_7/output/LayerNorm/gamma/adam_m
bert/encoder/layer_11/output/dense/bias/adam_v
bert/encoder/layer_8/attention/self/value/kernel/adam_m
bert/encoder/layer_8/attention/output/dense/bias/adam_v
bert/encoder/layer_9/output/LayerNorm/gamma
bert/encoder/layer_9/attention/output/dense/bias/adam_m
bert/encoder/layer_8/attention/self/key/kernel
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_8/output/dense/bias
bert/encoder/layer_9/attention/self/key/bias
bert/encoder/layer_9/attention/output/LayerNorm/gamma
cls/predictions/transform/dense/bias
bert/encoder/layer_10/attention/self/value/bias
bert/encoder/layer_11/intermediate/dense/kernel/adam_m
bert/encoder/layer_11/intermediate/dense/bias/adam_m
bert/encoder/layer_11/intermediate/dense/kernel/adam_v
bert/encoder/layer_11/output/dense/kernel
bert/pooler/strided_slice/stack_1
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m
cls/predictions/Const
truediv
bert/encoder/layer_9/attention/self/query/kernel/adam_m
bert/encoder/layer_7/output/LayerNorm/gamma/adam_v
bert/encoder/layer_8/attention/self/query/bias/adam_v
bert/encoder/layer_11/output/dense/kernel/adam_v
bert/pooler/strided_slice/stack_2
bert/encoder/layer_8/intermediate/dense/kernel/adam_m
cls/seq_relationship/output_weights/adam_v
bert/encoder/layer_10/attention/self/query/kernel
bert/encoder/layer_11/output/LayerNorm/gamma/adam_v
bert/encoder/layer_9/attention/output/dense/bias/adam_v
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_11/output/dense/bias
bert/encoder/layer_8/intermediate/dense/kernel/adam_v
cls/predictions/transform/LayerNorm/beta
bert/encoder/layer_7/attention/self/value/kernel/adam_v
bert/encoder/layer_7/attention/self/value/bias/adam_m
bert/encoder/layer_7/output/dense/bias/adam_v
bert/encoder/layer_10/attention/output/dense/kernel
bert/encoder/layer_7/attention/output/dense/bias/adam_v
bert/encoder/layer_8/attention/self/value/kernel/adam_v
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_11/output/LayerNorm/beta
bert/encoder/layer_11/output/LayerNorm/beta/adam_m
bert/encoder/layer_9/attention/self/query/kernel/adam_v
cls/predictions/add/y
gradients/cls/predictions/Sum_1_grad/Const
bert/encoder/layer_9/intermediate/dense/kernel
gradients/cls/predictions/transform/dense/Pow_grad/sub
bert/encoder/layer_8/attention/self/key/kernel/adam_m
bert/encoder/layer_10/intermediate/dense/bias
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_8/intermediate/dense/bias/adam_m
bert/encoder/layer_8/output/dense/bias/adam_v
bert/pooler/dense/kernel
bert/encoder/layer_9/attention/self/key/bias/adam_v
bert/pooler/dense/kernel/adam_m
bert/encoder/layer_7/output/dense/kernel/adam_m
bert/encoder/layer_7/output/LayerNorm/beta/adam_m
gradients/Reshape_2_grad/Reshape/strided_slice
bert/encoder/layer_10/attention/self/query/bias
bert/encoder/layer_10/attention/output/dense/bias
bert/encoder/layer_11/output/dense/bias/adam_m
bert/encoder/layer_8/intermediate/dense/bias/adam_v
bert/encoder/layer_8/output/LayerNorm/beta/adam_m
bert/encoder/layer_9/attention/self/query/bias/adam_m
bert/encoder/layer_9/attention/self/value/kernel/adam_m
cls/predictions/transform/LayerNorm/gamma
cls/seq_relationship/output_weights
bert/encoder/layer_7/output/dense/kernel/adam_v
gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/Tile/multiples
bert/encoder/layer_8/attention/self/query/kernel/adam_m
bert/encoder/layer_8/attention/self/key/kernel/adam_v
bert/encoder/layer_8/attention/self/value/bias/adam_m
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_8/output/dense/kernel/adam_m
bert/encoder/layer_11/output/LayerNorm/beta/adam_v
bert/encoder/layer_9/attention/self/query/bias/adam_v
bert/encoder/layer_9/attention/self/value/kernel/adam_v
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_7/intermediate/dense/bias/adam_v
cls/seq_relationship/output_bias
gradients/cls/predictions/Sum_grad/Reshape/shape
bert/encoder/layer_7/output/LayerNorm/beta/adam_v
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Reshape/shape
bert/encoder/layer_8/attention/self/key/bias/adam_m
bert/encoder/layer_8/attention/self/value/bias/adam_v
add_344/y
bert/encoder/layer_8/output/dense/kernel/adam_v
bert/encoder/layer_8/output/LayerNorm/gamma/adam_v
bert/encoder/layer_9/attention/self/key/kernel/adam_m
bert/encoder/layer_9/attention/self/value/bias/adam_m
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_0/intermediate/dense/kernel
bert/encoder/layer_0/output/LayerNorm/gamma
cls/seq_relationship/Sum/reduction_indices
bert/encoder/layer_1/attention/output/LayerNorm/gamma
bert/encoder/layer_1/output/LayerNorm/gamma
bert/encoder/layer_2/attention/output/LayerNorm/beta
bert/encoder/layer_11/output/dense/kernel/adam_m
bert/encoder/layer_8/attention/output/dense/kernel/adam_m
bert/encoder/layer_3/attention/output/dense/kernel
bert/encoder/layer_8/output/dense/bias/adam_m
bert/encoder/layer_8/output/LayerNorm/beta/adam_v
bert/encoder/layer_9/attention/self/key/bias/adam_m
bert/encoder/layer_9/attention/output/dense/kernel/adam_v
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m
bert/encoder/Reshape_1/shape
bert/encoder/layer_0/attention/self/value/kernel
bert/encoder/layer_0/attention/output/LayerNorm/beta
bert/encoder/layer_0/intermediate/dense/bias
bert/encoder/layer_1/attention/self/query/bias
bert/encoder/layer_1/attention/output/dense/kernel
bert/encoder/layer_1/output/dense/kernel
bert/encoder/layer_8/attention/self/key/bias/adam_v
bert/encoder/layer_8/attention/output/dense/kernel/adam_v
bert/encoder/layer_2/intermediate/dense/kernel
bert/encoder/layer_3/attention/self/query/kernel
bert/encoder/layer_8/output/LayerNorm/gamma/adam_m
bert/encoder/layer_11/output/LayerNorm/gamma/adam_m
bert/encoder/layer_9/attention/self/value/bias/adam_v
bert/pooler/dense/kernel/adam_v
bert/encoder/layer_6/attention/self/query/kernel/adam_m
bert/embeddings/assert_less_equal/x
bert/embeddings/LayerNorm/beta
bert/encoder/layer_6/attention/self/query/bias/adam_m
bert/encoder/layer_0/attention/self/value/bias
bert/encoder/layer_6/attention/self/key/kernel/adam_v
bert/encoder/layer_0/intermediate/dense/Pow/y
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_1/attention/self/value/kernel
bert/encoder/layer_1/intermediate/dense/kernel
bert/encoder/layer_2/attention/self/query/kernel
bert/encoder/layer_6/attention/output/dense/kernel/adam_m
bert/encoder/layer_2/intermediate/dense/bias
bert/encoder/layer_9/attention/self/key/kernel/adam_v
bert/encoder/layer_9/attention/output/dense/kernel/adam_m
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_4/attention/self/key/kernel
bert/encoder/layer_4/attention/output/dense/bias
Mul_1007/x
bert/embeddings/ExpandDims/dim
bert/embeddings/token_type_embeddings
bert/embeddings/LayerNorm/gamma
bert/encoder/layer_6/attention/self/query/bias/adam_v
bert/encoder/layer_0/attention/self/Reshape/shape
bert/encoder/layer_11/attention/output/dense/kernel/adam_m
bert/encoder/layer_0/intermediate/dense/mul/x
bert/encoder/layer_6/attention/self/key/bias/adam_m
bert/encoder/layer_1/attention/self/value/bias
bert/encoder/layer_1/intermediate/dense/bias
bert/encoder/layer_6/attention/self/value/bias/adam_m
bert/encoder/layer_6/attention/output/dense/kernel/adam_v
bert/encoder/layer_2/output/dense/kernel
bert/encoder/layer_3/attention/self/query/bias
bert/encoder/layer_3/attention/output/dense/bias
bert/encoder/layer_3/output/dense/kernel
bert/encoder/layer_4/attention/self/key/bias
bert/encoder/layer_4/attention/output/LayerNorm/beta
bert/encoder/layer_4/output/dense/bias
bert/encoder/layer_5/output/LayerNorm/gamma/adam_v
bert/encoder/layer_5/intermediate/dense/kernel
bert/embeddings/one_hot/on_value
bert/embeddings/LayerNorm/moments/mean/reduction_indices
bert/encoder/layer_0/attention/self/query/kernel
bert/encoder/layer_7/attention/self/query/kernel/adam_m
bert/encoder/layer_0/attention/self/dropout/Shape
bert/encoder/layer_0/intermediate/dense/mul_1/x
bert/encoder/layer_6/attention/self/key/bias/adam_v
bert/encoder/layer_1/attention/output/dense/bias
bert/encoder/layer_8/output/LayerNorm/beta
bert/encoder/layer_2/attention/self/key/kernel
bert/encoder/layer_2/attention/output/dense/kernel
bert/encoder/layer_2/output/dense/bias
bert/encoder/layer_3/attention/self/key/kernel
bert/encoder/layer_3/attention/output/LayerNorm/beta
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_11/attention/output/dense/bias/adam_v
bert/encoder/layer_4/attention/output/LayerNorm/gamma
bert/encoder/layer_4/output/LayerNorm/beta
bert/encoder/layer_6/output/dense/kernel/adam_m
global_step/Initializer/zeros
bert/encoder/layer_5/output/LayerNorm/gamma
bert/encoder/layer_6/output/LayerNorm/beta/adam_v
bert/encoder/layer_6/intermediate/dense/bias
bert/encoder/layer_7/attention/self/key/kernel
bert/encoder/layer_7/attention/output/LayerNorm/beta
bert/encoder/layer_7/output/dense/bias
bert/encoder/layer_0/intermediate/dense/mul_2/x
bert/encoder/layer_1/attention/self/query/kernel
bert/encoder/layer_11/attention/output/dense/kernel/adam_v
bert/encoder/layer_9/attention/self/value/kernel
bert/encoder/layer_6/attention/self/value/bias/adam_v
bert/encoder/layer_2/attention/output/LayerNorm/gamma
bert/encoder/layer_2/output/LayerNorm/beta
bert/encoder/layer_3/attention/output/LayerNorm/gamma
bert/encoder/layer_3/output/dense/bias
bert/encoder/layer_6/intermediate/dense/kernel/adam_m
bert/encoder/layer_4/intermediate/dense/kernel
bert/encoder/layer_6/intermediate/dense/bias/adam_v
bert/encoder/layer_5/attention/self/value/kernel
bert/encoder/layer_5/intermediate/dense/bias
Mul_543/x
bert/encoder/layer_6/attention/self/query/kernel/adam_v
bert/encoder/layer_6/output/LayerNorm/gamma/adam_v
bert/encoder/layer_7/attention/self/key/bias
bert/encoder/layer_7/attention/output/LayerNorm/gamma
bert/encoder/layer_7/output/LayerNorm/beta
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_0/output/dense/kernel
bert/encoder/layer_0/output/dense/bias
bert/encoder/layer_6/attention/self/value/kernel/adam_m
bert/encoder/layer_7/attention/self/value/bias/adam_v
bert/encoder/layer_2/attention/self/key/bias
bert/encoder/layer_2/attention/output/dense/bias
bert/encoder/layer_10/output/dense/kernel
bert/encoder/layer_3/attention/self/key/bias
bert/encoder/layer_3/intermediate/dense/kernel
cls/predictions/output_bias/adam_m
bert/encoder/layer_4/attention/self/value/kernel
bert/encoder/layer_4/intermediate/dense/bias
bert/encoder/layer_4/output/LayerNorm/gamma
bert/encoder/layer_5/attention/self/value/bias
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_6/attention/self/query/kernel
bert/encoder/layer_6/attention/self/value/bias
bert/encoder/layer_6/output/dense/kernel
bert/encoder/layer_7/attention/self/query/kernel/adam_v
bert/encoder/layer_7/intermediate/dense/kernel
bert/encoder/layer_7/output/LayerNorm/gamma
bert/encoder/layer_8/attention/self/key/bias
bert/encoder/layer_8/attention/output/LayerNorm/gamma
bert/encoder/layer_8/output/LayerNorm/gamma
bert/encoder/layer_9/attention/self/value/bias
bert/encoder/layer_9/intermediate/dense/bias
bert/encoder/layer_7/attention/output/dense/kernel/adam_m
bert/encoder/layer_10/attention/output/LayerNorm/beta
bert/encoder/layer_10/output/dense/bias
bert/encoder/layer_11/attention/self/key/kernel
bert/encoder/layer_3/attention/self/value/kernel
bert/encoder/layer_3/intermediate/dense/bias
bert/encoder/layer_3/output/LayerNorm/beta
bert/encoder/layer_4/attention/self/value/bias
bert/encoder/layer_6/intermediate/dense/bias/adam_m
bert/encoder/layer_5/attention/self/query/kernel
bert/encoder/layer_6/output/dense/kernel/adam_v
bert/encoder/layer_6/output/dense/bias/adam_m
bert/encoder/layer_6/attention/self/query/bias
bert/encoder/layer_6/attention/output/dense/kernel
bert/encoder/layer_6/output/dense/bias
bert/encoder/layer_7/attention/self/value/kernel
bert/encoder/layer_7/attention/self/query/bias/adam_m
bert/encoder/layer_8/attention/self/query/kernel
bert/encoder/layer_7/attention/self/key/kernel/adam_v
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_0/attention/self/Reshape_3/shape
bert/encoder/layer_9/attention/output/dense/kernel
bert/encoder/layer_9/output/dense/kernel
bert/encoder/layer_10/attention/self/key/kernel
bert/encoder/layer_10/attention/output/LayerNorm/gamma
bert/encoder/layer_10/output/LayerNorm/beta
bert/encoder/layer_11/attention/self/key/bias
bert/encoder/layer_6/attention/output/dense/bias/adam_m
bert/encoder/layer_3/attention/self/value/bias
bert/encoder/layer_11/attention/output/dense/bias/adam_m
bert/encoder/layer_3/output/LayerNorm/gamma
bert/encoder/layer_6/intermediate/dense/kernel/adam_v
bert/encoder/layer_4/output/dense/kernel
bert/encoder/layer_5/attention/self/query/bias
bert/encoder/layer_5/attention/output/dense/kernel
bert/encoder/layer_5/output/dense/kernel
bert/encoder/layer_6/attention/self/key/kernel
bert/encoder/layer_6/attention/output/dense/bias
bert/encoder/layer_6/output/LayerNorm/beta
Mul_545/x
bert/encoder/layer_7/intermediate/dense/bias
bert/encoder/layer_8/attention/self/query/bias
bert/encoder/layer_8/attention/self/value/kernel
bert/encoder/layer_8/intermediate/dense/kernel
bert/encoder/layer_0/attention/self/transpose/perm
bert/encoder/layer_9/attention/output/dense/bias
bert/encoder/layer_9/output/dense/bias
bert/encoder/layer_10/attention/self/key/bias
bert/encoder/layer_7/attention/output/dense/bias/adam_m
bert/encoder/layer_10/output/LayerNorm/gamma
bert/encoder/layer_11/attention/self/value/kernel
bert/encoder/layer_11/attention/output/LayerNorm/beta
bert/encoder/layer_2/output/LayerNorm/gamma
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_4/attention/self/query/kernel
bert/encoder/layer_4/attention/output/dense/kernel
bert/encoder/layer_11/intermediate/dense/bias/adam_v
bert/encoder/layer_5/attention/self/key/kernel
bert/encoder/layer_5/attention/output/dense/bias
bert/encoder/layer_5/output/dense/bias
bert/encoder/layer_6/attention/self/key/bias
bert/encoder/layer_6/attention/output/LayerNorm/beta
bert/encoder/layer_6/output/LayerNorm/gamma
bert/encoder/layer_7/attention/self/value/bias
bert/encoder/layer_7/attention/self/key/kernel/adam_m
bert/encoder/layer_8/attention/self/value/bias
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_9/attention/self/query/kernel
bert/encoder/layer_0/attention/self/mul_1/y
bert/embeddings/word_embeddings/adam_m
bert/encoder/layer_7/attention/output/dense/kernel/adam_v
add_699/y
bert/encoder/layer_11/attention/self/query/kernel
bert/encoder/layer_11/attention/self/value/bias
bert/encoder/layer_11/attention/output/LayerNorm/gamma
bert/encoder/layer_11/output/LayerNorm/gamma
bert/pooler/dense/bias
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v
PolynomialDecay/sub
bert/encoder/layer_4/attention/self/query/bias
bert/encoder/layer_1/output/dense/bias/adam_v
cls/predictions/transform/dense/bias/adam_v
bert/encoder/layer_5/attention/self/key/bias
bert/encoder/layer_5/attention/output/LayerNorm/beta
bert/encoder/layer_6/output/dense/bias/adam_v
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_6/attention/output/LayerNorm/gamma
bert/encoder/layer_7/attention/self/query/kernel
bert/encoder/layer_7/attention/output/dense/kernel
bert/encoder/layer_9/intermediate/dense/kernel/adam_m
cls/predictions/output_bias/adam_v
bert/embeddings/word_embeddings
bert/encoder/layer_7/attention/self/key/bias/adam_m
bert/encoder/Reshape/shape
bert/encoder/layer_0/attention/self/key/bias
bert/encoder/layer_0/attention/self/Mul/y
cls/predictions/transform/dense/kernel/adam_v
bert/encoder/layer_0/attention/self/key/bias/adam_v
bert/encoder/layer_0/attention/output/dense/bias/adam_v
bert/encoder/layer_11/attention/output/dense/kernel
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_7/intermediate/dense/kernel/adam_m
Reshape
bert/encoder/layer_6/attention/output/dense/bias/adam_v
PolynomialDecay/Minimum/y
gradients/cls/predictions/Sum_grad/Tile/multiples
cls/seq_relationship/output_bias/adam_v
bert/encoder/layer_2/attention/self/key/bias/adam_v
bert/encoder/layer_5/attention/output/LayerNorm/gamma
bert/encoder/layer_5/output/LayerNorm/beta
bert/encoder/layer_6/output/LayerNorm/beta/adam_m
bert/encoder/layer_6/intermediate/dense/kernel
bert/encoder/layer_7/attention/self/query/bias
bert/encoder/layer_9/intermediate/dense/kernel/adam_v
bert/encoder/layer_9/output/dense/bias/adam_v
bert/encoder/layer_10/attention/self/query/bias/adam_m
bert/encoder/layer_8/attention/output/dense/kernel
bert/encoder/layer_8/intermediate/dense/bias
bert/embeddings/one_hot/off_value
bert/embeddings/LayerNorm/batchnorm/add/y
bert/encoder/layer_0/attention/self/query/bias
bert/encoder/layer_0/attention/self/value/kernel/adam_m
bert/encoder/layer_10/attention/self/value/bias/adam_m
bert/encoder/layer_0/intermediate/dense/kernel/adam_v
bert/encoder/layer_10/attention/output/dense/kernel/adam_v
bert/encoder/layer_11/intermediate/dense/kernel
bert/encoder/layer_7/intermediate/dense/kernel/adam_v
cls/predictions/transform/dense/kernel
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_1/output/LayerNorm/beta/adam_m
bert/encoder/layer_2/attention/self/query/bias/adam_v
bert/encoder/layer_2/attention/self/value/kernel/adam_m
bert/encoder/layer_6/attention/self/value/kernel
bert/encoder/layer_6/output/LayerNorm/gamma/adam_m
bert/encoder/layer_9/intermediate/dense/bias/adam_m
bert/encoder/layer_9/output/LayerNorm/beta/adam_m
bert/encoder/layer_10/attention/self/query/bias/adam_v
clip_by_global_norm/Const_1
bert/encoder/layer_8/attention/output/dense/bias
bert/encoder/layer_7/attention/self/key/bias/adam_v
bert/encoder/layer_9/attention/self/query/bias
bert/embeddings/one_hot/depth
bert/encoder/layer_0/attention/self/value/kernel/adam_v
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_0/intermediate/dense/bias/adam_m
bert/encoder/layer_0/output/LayerNorm/beta/adam_m
bert/encoder/layer_1/attention/self/query/bias/adam_m
bert/encoder/layer_1/attention/self/value/kernel/adam_m
bert/encoder/layer_1/attention/output/dense/bias/adam_v
bert/encoder/layer_1/intermediate/dense/bias/adam_m
cls/predictions/output_bias
bert/encoder/layer_10/intermediate/dense/kernel/adam_m
bert/encoder/layer_2/attention/self/value/kernel/adam_v
bert/encoder/layer_9/intermediate/dense/bias/adam_v
bert/pooler/dense/bias/adam_v
gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Sum/reduction_indices
bert/encoder/layer_11/attention/self/value/kernel/adam_v
bert/encoder/layer_5/output/dense/kernel/adam_v
bert/embeddings/word_embeddings/adam_v
bert/embeddings/LayerNorm/beta/adam_m
bert/encoder/layer_7/attention/self/value/kernel/adam_m
bert/embeddings/GatherV2/axis
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_0/intermediate/dense/bias/adam_v
bert/encoder/layer_0/output/LayerNorm/beta/adam_v
bert/encoder/layer_1/attention/self/query/bias/adam_v
bert/encoder/layer_1/attention/self/value/kernel/adam_v
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_1/intermediate/dense/bias/adam_v
bert/encoder/layer_1/output/LayerNorm/beta/adam_v
bert/encoder/layer_2/attention/self/key/kernel/adam_m
bert/encoder/layer_2/attention/self/value/bias/adam_m
bert/encoder/layer_9/output/dense/kernel/adam_m
bert/encoder/layer_9/output/LayerNorm/beta/adam_v
bert/encoder/layer_10/attention/self/key/kernel/adam_m
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_5/output/dense/bias/adam_m
bert/embeddings/token_type_embeddings/adam_m
bert/embeddings/LayerNorm/beta/adam_v
cls/seq_relationship/output_bias/adam_m
bert/encoder/layer_0/attention/self/value/bias/adam_m
bert/encoder/layer_10/attention/self/value/bias/adam_v
bert/encoder/layer_0/output/dense/kernel/adam_m
bert/encoder/layer_0/output/LayerNorm/gamma/adam_m
bert/encoder/layer_1/attention/self/key/kernel/adam_m
bert/encoder/layer_1/attention/self/value/bias/adam_m
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v
cls/predictions/transform/dense/bias/adam_m
bert/encoder/layer_1/output/LayerNorm/gamma/adam_m
bert/encoder/layer_10/intermediate/dense/kernel/adam_v
bert/encoder/layer_2/attention/self/value/bias/adam_v
bert/encoder/layer_2/attention/output/dense/bias/adam_v
bert/encoder/layer_10/intermediate/dense/bias/adam_m
bert/encoder/layer_2/intermediate/dense/kernel/adam_m
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_2/intermediate/dense/kernel/adam_v
bert/encoder/layer_2/output/dense/bias/adam_m
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_2/intermediate/dense/bias/adam_m
bert/encoder/layer_9/output/dense/kernel/adam_v
bert/encoder/layer_9/output/LayerNorm/gamma/adam_m
gradients/bert/embeddings/LayerNorm/moments/variance_grad/Tile/multiples
bert/encoder/layer_5/intermediate/dense/kernel/adam_m
bert/encoder/layer_11/attention/self/value/bias/adam_m
bert/embeddings/token_type_embeddings/adam_v
bert/embeddings/LayerNorm/gamma/adam_m
bert/encoder/layer_10/attention/self/value/kernel/adam_m
bert/encoder/layer_0/attention/self/value/bias/adam_v
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_10/attention/output/dense/kernel/adam_m
bert/encoder/layer_0/output/LayerNorm/gamma/adam_v
bert/encoder/layer_10/attention/output/dense/bias/adam_m
bert/encoder/layer_1/attention/self/value/bias/adam_v
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_1/output/LayerNorm/gamma/adam_v
bert/encoder/layer_2/attention/self/key/kernel/adam_v
bert/encoder/layer_2/attention/output/dense/kernel/adam_m
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_2/intermediate/dense/bias/adam_v
bert/encoder/layer_2/output/dense/bias/adam_v
bert/encoder/layer_3/attention/self/query/kernel/adam_v
bert/encoder/layer_3/attention/self/key/bias/adam_v
bert/encoder/layer_3/attention/output/dense/bias/adam_m
bert/encoder/layer_3/intermediate/dense/kernel/adam_m
bert/encoder/layer_10/output/LayerNorm/gamma/adam_v
bert/encoder/layer_10/intermediate/dense/bias/adam_v
bert/encoder/layer_2/output/dense/kernel/adam_m
bert/encoder/layer_2/output/LayerNorm/beta/adam_m
bert/encoder/layer_9/output/dense/bias/adam_m
bert/encoder/layer_9/output/LayerNorm/gamma/adam_v
bert/encoder/layer_10/attention/self/key/kernel/adam_v
bert/encoder/layer_5/intermediate/dense/kernel/adam_v
bert/encoder/layer_5/output/dense/bias/adam_v
bert/encoder/layer_10/attention/self/key/bias/adam_m
bert/embeddings/LayerNorm/gamma/adam_v
bert/encoder/layer_0/attention/self/key/kernel/adam_m
bert/encoder/layer_0/attention/output/dense/kernel/adam_m
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_0/output/dense/kernel/adam_v
bert/encoder/layer_1/attention/self/query/kernel/adam_m
bert/encoder/layer_1/attention/self/key/kernel/adam_v
bert/encoder/layer_1/attention/output/dense/kernel/adam_m
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_1/output/dense/kernel/adam_m
bert/encoder/layer_2/attention/self/query/kernel/adam_m
bert/encoder/layer_2/attention/self/key/bias/adam_m
bert/encoder/layer_2/attention/output/dense/kernel/adam_v
bert/encoder/layer_3/attention/self/query/bias/adam_m
bert/encoder/layer_10/output/dense/bias/adam_v
bert/encoder/layer_3/attention/output/dense/bias/adam_v
bert/encoder/layer_3/intermediate/dense/kernel/adam_v
bert/encoder/layer_3/output/dense/bias/adam_m
bert/encoder/layer_4/attention/self/query/kernel/adam_v
bert/encoder/layer_4/attention/self/key/bias/adam_v
bert/encoder/layer_4/attention/output/dense/bias/adam_m
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_2/output/dense/kernel/adam_v
bert/encoder/layer_10/output/dense/kernel/adam_v
bert/encoder/layer_3/attention/self/query/bias/adam_v
bert/encoder/layer_3/attention/self/value/kernel/adam_m
bert/pooler/dense/bias/adam_m
bert/encoder/layer_10/attention/self/query/kernel/adam_m
cls/predictions/transform/dense/kernel/adam_m
bert/encoder/layer_5/intermediate/dense/bias/adam_m
bert/encoder/layer_5/output/LayerNorm/beta/adam_m
bert/embeddings/position_embeddings/adam_m
bert/encoder/layer_0/attention/self/query/kernel/adam_m
bert/encoder/layer_0/attention/self/key/kernel/adam_v
bert/encoder/layer_0/attention/output/dense/kernel/adam_v
bert/encoder/layer_0/intermediate/dense/kernel/adam_m
bert/encoder/layer_0/output/dense/bias/adam_m
bert/encoder/layer_1/attention/self/query/kernel/adam_v
bert/encoder/layer_1/attention/self/key/bias/adam_m
bert/encoder/layer_1/attention/output/dense/kernel/adam_v
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_2/attention/self/query/kernel/adam_v
bert/encoder/layer_4/attention/output/dense/bias/adam_v
bert/encoder/layer_2/attention/output/dense/bias/adam_m
bert/encoder/layer_4/output/dense/bias/adam_m
bert/encoder/layer_5/attention/self/query/kernel/adam_v
bert/encoder/layer_10/output/dense/kernel/adam_m
bert/encoder/layer_2/output/LayerNorm/beta/adam_v
bert/encoder/layer_3/attention/self/key/kernel/adam_m
bert/encoder/layer_3/attention/self/value/kernel/adam_v
cls/predictions/transform/LayerNorm/beta/adam_v
bert/encoder/layer_3/intermediate/dense/bias/adam_m
bert/encoder/layer_3/output/dense/bias/adam_v
bert/encoder/layer_4/attention/self/query/bias/adam_m
bert/encoder/layer_4/attention/self/value/kernel/adam_m
bert/encoder/layer_11/attention/self/query/bias/adam_m
bert/encoder/layer_4/intermediate/dense/kernel/adam_m
bert/encoder/layer_4/output/dense/bias/adam_v
bert/encoder/layer_5/attention/self/query/bias/adam_m
bert/encoder/layer_10/attention/self/query/kernel/adam_v
bert/encoder/layer_5/attention/output/dense/bias/adam_m
bert/encoder/layer_5/intermediate/dense/bias/adam_v
bert/encoder/layer_11/attention/self/value/bias/adam_v
bert/encoder/layer_10/attention/self/key/bias/adam_v
bert/encoder/layer_0/attention/self/query/kernel/adam_v
bert/encoder/layer_10/attention/self/value/kernel/adam_v
bert/encoder/layer_0/attention/output/dense/bias/adam_m
bert/encoder/layer_4/attention/self/query/bias/adam_v
bert/encoder/layer_0/output/dense/bias/adam_v
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_10/attention/output/dense/bias/adam_v
bert/encoder/layer_1/attention/output/dense/bias/adam_m
bert/encoder/layer_1/intermediate/dense/kernel/adam_m
bert/encoder/layer_1/output/dense/kernel/adam_v
bert/encoder/layer_2/attention/self/query/bias/adam_m
bert/encoder/layer_2/output/LayerNorm/gamma/adam_m
bert/encoder/layer_3/attention/self/key/kernel/adam_v
bert/encoder/layer_3/attention/self/value/bias/adam_m
bert/encoder/layer_10/output/LayerNorm/beta/adam_m
bert/encoder/layer_3/intermediate/dense/bias/adam_v
bert/encoder/layer_3/output/LayerNorm/beta/adam_m
bert/encoder/layer_11/attention/self/query/kernel/adam_m
bert/encoder/layer_4/attention/self/value/kernel/adam_v
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v
bert/encoder/layer_4/intermediate/dense/kernel/adam_v
bert/encoder/layer_11/attention/self/key/kernel/adam_v
bert/encoder/layer_5/attention/self/query/bias/adam_v
bert/encoder/layer_11/attention/self/key/bias/adam_v
bert/encoder/layer_5/attention/output/dense/bias/adam_v
bert/encoder/layer_2/output/LayerNorm/gamma/adam_v
cls/predictions/transform/LayerNorm/beta/adam_m
bert/encoder/layer_3/attention/self/value/bias/adam_v
bert/encoder/layer_5/output/dense/kernel/adam_m
add
bert/encoder/layer_5/output/LayerNorm/beta/adam_v
bert/embeddings/position_embeddings/adam_v
bert/encoder/layer_0/attention/self/query/bias/adam_m
bert/encoder/layer_0/attention/self/key/bias/adam_m
bert/encoder/layer_11/attention/self/query/bias/adam_v
Reshape_1
bert/encoder/layer_4/intermediate/dense/bias/adam_m
bert/encoder/layer_4/output/LayerNorm/beta/adam_m
bert/encoder/layer_5/attention/self/key/kernel/adam_m
bert/encoder/layer_1/attention/self/key/bias/adam_v
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_1/output/dense/bias/adam_m
bert/encoder/layer_3/attention/self/query/kernel/adam_m
bert/encoder/layer_10/output/dense/bias/adam_m
bert/encoder/layer_3/attention/output/dense/kernel/adam_m
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m
bert/encoder/layer_3/output/dense/kernel/adam_m
bert/encoder/layer_3/output/LayerNorm/beta/adam_v
bert/encoder/layer_4/attention/self/key/kernel/adam_m
bert/encoder/layer_4/attention/self/value/bias/adam_m
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m
bert/encoder/layer_4/intermediate/dense/bias/adam_v
bert/encoder/layer_4/output/LayerNorm/beta/adam_v
cls/predictions/transform/LayerNorm/gamma/adam_v
bert/encoder/layer_5/attention/self/key/bias/adam_v
bert/encoder/layer_11/attention/self/value/kernel/adam_m
bert/encoder/layer_3/attention/self/key/bias/adam_m
bert/encoder/layer_3/attention/output/dense/kernel/adam_v
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v
bert/embeddings/ExpandDims
bert/encoder/layer_5/output/LayerNorm/gamma/adam_m
bert/encoder/layer_3/output/LayerNorm/gamma/adam_m
bert/encoder/layer_0/attention/self/query/bias/adam_v
bert/encoder/layer_4/attention/self/value/bias/adam_v
bert/embeddings/Reshape
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v
bert/encoder/layer_4/output/dense/kernel/adam_m
bert/encoder/layer_4/output/LayerNorm/gamma/adam_m
bert/encoder/layer_11/attention/self/key/bias/adam_m
bert/encoder/layer_5/attention/self/value/kernel/adam_m
ConstantFolding/PolynomialDecay/truediv_recip
bert/encoder/layer_1/intermediate/dense/kernel/adam_v
bert/encoder/layer_10/output/LayerNorm/beta/adam_v
bert/encoder/layer_10/output/LayerNorm/gamma/adam_m
bert/encoder/layer_3/output/LayerNorm/gamma/adam_v
bert/encoder/layer_4/attention/self/key/kernel/adam_v
bert/encoder/layer_4/attention/output/dense/kernel/adam_m
bert/encoder/layer_11/attention/self/key/kernel/adam_m
bert/encoder/layer_4/output/LayerNorm/gamma/adam_v
bert/encoder/layer_5/attention/self/key/kernel/adam_v
bert/encoder/layer_5/attention/self/value/kernel/adam_v
ConstantFolding/gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/truediv_recip
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m
bert/embeddings/Reshape_2
bert/encoder/layer_3/output/dense/kernel/adam_v
bert/encoder/layer_4/attention/self/query/kernel/adam_m
bert/encoder/layer_11/attention/self/query/kernel/adam_v
bert/encoder/layer_4/attention/output/dense/kernel/adam_v
bert/encoder/layer_4/output/dense/kernel/adam_v
bert/encoder/layer_5/attention/self/query/kernel/adam_m
bert/encoder/layer_5/attention/self/key/bias/adam_m
bert/encoder/layer_5/attention/self/value/bias/adam_m
gradients/cls/seq_relationship/Sum_grad/Tile
bert/encoder/layer_4/attention/self/key/bias/adam_m
cls/predictions/transform/LayerNorm/gamma/adam_m
cls/predictions/Reshape
bert/encoder/layer_5/attention/self/value/bias/adam_v
bert/embeddings/one_hot
1608802723825833	32768
1608802724252309	-32768
bert/encoder/ones
cls/predictions/one_hot
1608802723825970	93769728
1608802723972483	-93769728
bert/encoder/layer_5/attention/output/dense/kernel/adam_m
bert/encoder/layer_5/attention/output/dense/kernel/adam_v/read
cls/seq_relationship/Reshape
cls/predictions/Reshape_1
Cast/ReadVariableOp
ReadVariableOp
global_step/VarIsInitializedOp
cls/predictions/Sum_2
1608802723826292	256
1608802723977131	-256
PolynomialDecay/Cast_2/ReadVariableOp
bert/embeddings/dropout/random_uniform/RandomUniform
1608802723826344	12582912
1608802723860124	-12582912
cls/predictions/add
bert/encoder/layer_0/attention/self/key/kernel/read
bert/encoder/layer_6/attention/self/key/kernel/adam_m/read
gradients/cls/predictions/truediv_grad/RealDiv
1608802723826557	256
1608802723826689	-256
bert/embeddings/position_embeddings/read
bert/encoder/layer_0/attention/output/LayerNorm/gamma/read
bert/encoder/layer_6/attention/self/value/kernel/adam_v/read
bert/encoder/layer_1/attention/output/LayerNorm/beta/read
bert/encoder/layer_1/output/LayerNorm/beta/read
gradients/cls/predictions/Sum_1_grad/Reshape
gradients/cls/predictions/Sum_1_grad/Tile
1608802723826645	2560
1608802723826906	-2560
bert/encoder/layer_2/attention/self/value/kernel/read
bert/encoder/layer_0/attention/output/dense/kernel/read
bert/encoder/layer_2/attention/self/query/bias/read
bert/encoder/layer_2/attention/self/value/bias/read
bert/encoder/layer_1/attention/self/key/kernel/read
gradients/cls/predictions/mul_1_grad/Mul_1
bert/encoder/layer_1/output/dense/bias/read
bert/encoder/layer_0/attention/output/dense/bias/read
bert/encoder/layer_0/output/LayerNorm/beta/read
bert/encoder/layer_1/attention/self/key/bias/read
gradients/cls/predictions/Neg_grad/Neg
bert/encoder/layer_7/attention/output/dense/bias/read
bert/encoder/layer_7/attention/self/query/bias/adam_v/read
bert/encoder/layer_7/output/dense/kernel/read
bert/encoder/layer_8/attention/output/LayerNorm/beta/read
gradients/cls/predictions/Sum_grad/Reshape
bert/encoder/layer_8/output/dense/kernel/read
gradients/cls/predictions/Sum_grad/Tile
1608802723826862	78136320
1608802723976016	-78136320
bert/encoder/layer_9/attention/self/key/kernel/read
bert/encoder/layer_9/attention/output/LayerNorm/beta/read
bert/encoder/layer_9/output/LayerNorm/beta/read
bert/encoder/layer_10/attention/self/value/kernel/read
bert/encoder/layer_10/intermediate/dense/kernel/read
gradients/cls/predictions/mul_grad/Mul
bert/encoder/layer_11/attention/self/query/bias/read
bert/encoder/layer_11/attention/output/dense/bias/read
cls/seq_relationship/one_hot
1608802723826963	256
1608802723965807	-256
bert/encoder/layer_11/intermediate/dense/bias/read
bert/encoder/layer_7/intermediate/dense/bias/adam_m/read
gradients/cls/predictions/LogSoftmax_grad/Sum
1608802723826997	2560
1608802723827028	256
1608802723827085	-256
1608802723972683	-2560
bert/encoder/layer_7/output/dense/bias/adam_m/read
bert/encoder/layer_8/attention/self/query/kernel/adam_v/read
cls/seq_relationship/output_weights/adam_m/read
gradients/cls/seq_relationship/mul_grad/Mul_1
1608802723827054	256
1608802723966733	-256
bert/encoder/layer_8/attention/output/dense/bias/adam_m/read
bert/encoder/layer_8/attention/self/query/bias/adam_m/read
bert/encoder/layer_7/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_11/output/dense/bias/adam_v/read
bert/encoder/layer_8/attention/self/value/kernel/adam_m/read
bert/encoder/layer_8/attention/output/dense/bias/adam_v/read
bert/encoder/layer_9/output/LayerNorm/gamma/read
gradients/cls/seq_relationship/LogSoftmax_grad/Sum
1608802723827147	256
1608802723965883	-256
bert/encoder/layer_9/attention/output/dense/bias/adam_m/read
bert/encoder/layer_8/attention/self/key/kernel/read
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_8/output/dense/bias/read
bert/encoder/layer_9/attention/self/key/bias/read
bert/encoder/layer_9/attention/output/LayerNorm/gamma/read
cls/predictions/transform/dense/bias/read
bert/encoder/layer_10/attention/self/value/bias/read
bert/encoder/layer_11/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_11/intermediate/dense/bias/adam_m/read
bert/encoder/layer_11/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_11/output/dense/kernel/read
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_9/attention/self/query/kernel/adam_m/read
bert/encoder/layer_7/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_8/attention/self/query/bias/adam_v/read
bert/encoder/layer_11/output/dense/kernel/adam_v/read
bert/encoder/layer_8/intermediate/dense/kernel/adam_m/read
cls/seq_relationship/output_weights/adam_v/read
bert/encoder/layer_10/attention/self/query/kernel/read
bert/encoder/layer_11/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_9/attention/output/dense/bias/adam_v/read
bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_11/output/dense/bias/read
bert/encoder/layer_8/intermediate/dense/kernel/adam_v/read
cls/predictions/transform/LayerNorm/beta/read
bert/encoder/layer_7/attention/self/value/kernel/adam_v/read
bert/encoder/layer_7/attention/self/value/bias/adam_m/read
bert/encoder/layer_7/output/dense/bias/adam_v/read
bert/encoder/layer_10/attention/output/dense/kernel/read
bert/encoder/layer_7/attention/output/dense/bias/adam_v/read
bert/encoder/layer_8/attention/self/value/kernel/adam_v/read
bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_11/output/LayerNorm/beta/read
bert/encoder/layer_11/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_9/attention/self/query/kernel/adam_v/read
bert/encoder/layer_9/intermediate/dense/kernel/read
bert/encoder/layer_8/attention/self/key/kernel/adam_m/read
bert/encoder/layer_10/intermediate/dense/bias/read
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_8/intermediate/dense/bias/adam_m/read
bert/encoder/layer_8/output/dense/bias/adam_v/read
bert/pooler/dense/kernel/read
bert/encoder/layer_9/attention/self/key/bias/adam_v/read
bert/pooler/dense/kernel/adam_m/read
bert/encoder/layer_7/output/dense/kernel/adam_m/read
bert/encoder/layer_7/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_10/attention/self/query/bias/read
bert/encoder/layer_10/attention/output/dense/bias/read
bert/encoder/layer_11/output/dense/bias/adam_m/read
bert/encoder/layer_8/intermediate/dense/bias/adam_v/read
bert/encoder/layer_8/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_9/attention/self/query/bias/adam_m/read
bert/encoder/layer_9/attention/self/value/kernel/adam_m/read
cls/predictions/transform/LayerNorm/gamma/read
cls/seq_relationship/output_weights/read
bert/encoder/layer_7/output/dense/kernel/adam_v/read
bert/encoder/layer_8/attention/self/query/kernel/adam_m/read
bert/encoder/layer_8/attention/self/key/kernel/adam_v/read
bert/encoder/layer_8/attention/self/value/bias/adam_m/read
bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_8/output/dense/kernel/adam_m/read
bert/encoder/layer_11/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_9/attention/self/query/bias/adam_v/read
bert/encoder/layer_9/attention/self/value/kernel/adam_v/read
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_7/intermediate/dense/bias/adam_v/read
cls/seq_relationship/output_bias/read
bert/encoder/layer_7/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_8/attention/self/key/bias/adam_m/read
bert/encoder/layer_8/attention/self/value/bias/adam_v/read
bert/encoder/layer_8/output/dense/kernel/adam_v/read
bert/encoder/layer_8/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_9/attention/self/key/kernel/adam_m/read
bert/encoder/layer_9/attention/self/value/bias/adam_m/read
bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_0/intermediate/dense/kernel/read
bert/encoder/layer_0/output/LayerNorm/gamma/read
bert/encoder/layer_1/attention/output/LayerNorm/gamma/read
bert/encoder/layer_1/output/LayerNorm/gamma/read
bert/encoder/layer_2/attention/output/LayerNorm/beta/read
bert/encoder/layer_11/output/dense/kernel/adam_m/read
bert/encoder/layer_8/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_3/attention/output/dense/kernel/read
bert/encoder/layer_8/output/dense/bias/adam_m/read
bert/encoder/layer_8/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_9/attention/self/key/bias/adam_m/read
bert/encoder/layer_9/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_0/attention/self/value/kernel/read
bert/encoder/layer_0/attention/output/LayerNorm/beta/read
bert/encoder/layer_0/intermediate/dense/bias/read
bert/encoder/layer_1/attention/self/query/bias/read
bert/encoder/layer_1/attention/output/dense/kernel/read
bert/encoder/layer_1/output/dense/kernel/read
bert/encoder/layer_8/attention/self/key/bias/adam_v/read
bert/encoder/layer_8/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_2/intermediate/dense/kernel/read
bert/encoder/layer_3/attention/self/query/kernel/read
bert/encoder/layer_8/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_11/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_9/attention/self/value/bias/adam_v/read
bert/pooler/dense/kernel/adam_v/read
bert/encoder/layer_6/attention/self/query/kernel/adam_m/read
bert/embeddings/LayerNorm/beta/read
bert/encoder/layer_6/attention/self/query/bias/adam_m/read
bert/encoder/layer_0/attention/self/value/bias/read
bert/encoder/layer_6/attention/self/key/kernel/adam_v/read
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_1/attention/self/value/kernel/read
bert/encoder/layer_1/intermediate/dense/kernel/read
bert/encoder/layer_2/attention/self/query/kernel/read
bert/encoder/layer_6/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_2/intermediate/dense/bias/read
bert/encoder/layer_9/attention/self/key/kernel/adam_v/read
bert/encoder/layer_9/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_4/attention/self/key/kernel/read
bert/encoder/layer_4/attention/output/dense/bias/read
bert/embeddings/token_type_embeddings/read
bert/embeddings/LayerNorm/gamma/read
bert/encoder/layer_6/attention/self/query/bias/adam_v/read
bert/encoder/layer_11/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_6/attention/self/key/bias/adam_m/read
bert/encoder/layer_1/attention/self/value/bias/read
bert/encoder/layer_1/intermediate/dense/bias/read
bert/encoder/layer_6/attention/self/value/bias/adam_m/read
bert/encoder/layer_6/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_2/output/dense/kernel/read
bert/encoder/layer_3/attention/self/query/bias/read
bert/encoder/layer_3/attention/output/dense/bias/read
bert/encoder/layer_3/output/dense/kernel/read
bert/encoder/layer_4/attention/self/key/bias/read
bert/encoder/layer_4/attention/output/LayerNorm/beta/read
bert/encoder/layer_4/output/dense/bias/read
bert/encoder/layer_5/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_5/intermediate/dense/kernel/read
bert/encoder/layer_0/attention/self/query/kernel/read
bert/encoder/layer_7/attention/self/query/kernel/adam_m/read
bert/encoder/layer_11/attention/self/dropout/random_uniform/RandomUniform
1608802723829133	25165824
1608802723860200	-25165824
bert/encoder/layer_4/attention/self/dropout/random_uniform/RandomUniform
1608802723829220	25165824
1608802723860253	-25165824
bert/encoder/layer_7/attention/self/dropout/random_uniform/RandomUniform
1608802723829289	25165824
1608802723860306	-25165824
bert/encoder/layer_9/attention/self/dropout/random_uniform/RandomUniform
1608802723829357	25165824
1608802723860357	-25165824
bert/encoder/layer_0/attention/self/dropout/random_uniform/RandomUniform
1608802723829420	25165824
1608802723860415	-25165824
bert/encoder/layer_1/attention/self/dropout/random_uniform/RandomUniform
1608802723829480	25165824
1608802723860469	-25165824
bert/encoder/layer_2/attention/self/dropout/random_uniform/RandomUniform
1608802723829538	25165824
1608802723860521	-25165824
bert/encoder/layer_10/attention/self/dropout/random_uniform/RandomUniform
1608802723829596	25165824
1608802723860569	-25165824
bert/encoder/layer_5/attention/self/dropout/random_uniform/RandomUniform
1608802723829653	25165824
1608802723860621	-25165824
bert/encoder/layer_8/attention/self/dropout/random_uniform/RandomUniform
1608802723829712	25165824
1608802723860675	-25165824
bert/encoder/layer_6/attention/self/dropout/random_uniform/RandomUniform
1608802723829766	25165824
1608802723860724	-25165824
bert/encoder/layer_3/attention/self/dropout/random_uniform/RandomUniform
1608802723829826	25165824
1608802723860769	-25165824
bert/encoder/layer_6/attention/self/key/bias/adam_v/read
bert/encoder/layer_1/attention/output/dense/bias/read
bert/encoder/layer_8/output/LayerNorm/beta/read
bert/encoder/layer_2/attention/self/key/kernel/read
bert/encoder/layer_2/attention/output/dense/kernel/read
bert/encoder/layer_2/output/dense/bias/read
bert/encoder/layer_3/attention/self/key/kernel/read
bert/encoder/layer_3/attention/output/LayerNorm/beta/read
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_11/attention/output/dense/bias/adam_v/read
bert/encoder/layer_4/attention/output/LayerNorm/gamma/read
bert/encoder/layer_4/output/LayerNorm/beta/read
bert/encoder/layer_6/output/dense/kernel/adam_m/read
bert/encoder/layer_5/output/LayerNorm/gamma/read
bert/encoder/layer_6/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_6/intermediate/dense/bias/read
bert/encoder/layer_7/attention/self/key/kernel/read
bert/encoder/layer_7/attention/output/LayerNorm/beta/read
bert/encoder/layer_7/output/dense/bias/read
bert/encoder/layer_1/attention/self/query/kernel/read
bert/encoder/layer_11/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_9/attention/self/value/kernel/read
bert/encoder/layer_6/attention/self/value/bias/adam_v/read
bert/encoder/layer_2/attention/output/LayerNorm/gamma/read
bert/encoder/layer_2/output/LayerNorm/beta/read
bert/encoder/layer_3/attention/output/LayerNorm/gamma/read
bert/encoder/layer_3/output/dense/bias/read
bert/encoder/layer_6/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_4/intermediate/dense/kernel/read
bert/encoder/layer_6/intermediate/dense/bias/adam_v/read
bert/encoder/layer_5/attention/self/value/kernel/read
bert/encoder/layer_5/intermediate/dense/bias/read
bert/encoder/layer_6/attention/self/query/kernel/adam_v/read
bert/encoder/layer_6/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_7/attention/self/key/bias/read
bert/encoder/layer_7/attention/output/LayerNorm/gamma/read
bert/encoder/layer_7/output/LayerNorm/beta/read
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_0/output/dense/kernel/read
bert/encoder/layer_0/output/dense/bias/read
bert/encoder/layer_6/attention/self/value/kernel/adam_m/read
bert/encoder/layer_7/attention/self/value/bias/adam_v/read
bert/encoder/layer_2/attention/self/key/bias/read
bert/encoder/layer_2/attention/output/dense/bias/read
bert/encoder/layer_10/output/dense/kernel/read
bert/encoder/layer_3/attention/self/key/bias/read
bert/encoder/layer_3/intermediate/dense/kernel/read
cls/predictions/output_bias/adam_m/read
bert/encoder/layer_4/attention/self/value/kernel/read
bert/encoder/layer_4/intermediate/dense/bias/read
bert/encoder/layer_4/output/LayerNorm/gamma/read
bert/encoder/layer_5/attention/self/value/bias/read
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_6/attention/self/query/kernel/read
bert/encoder/layer_6/attention/self/value/bias/read
bert/encoder/layer_6/output/dense/kernel/read
bert/encoder/layer_7/attention/self/query/kernel/adam_v/read
bert/encoder/layer_7/intermediate/dense/kernel/read
bert/encoder/layer_7/output/LayerNorm/gamma/read
bert/encoder/layer_8/attention/self/key/bias/read
bert/encoder/layer_8/attention/output/LayerNorm/gamma/read
bert/encoder/layer_8/output/LayerNorm/gamma/read
bert/encoder/layer_9/attention/self/value/bias/read
bert/encoder/layer_9/intermediate/dense/bias/read
bert/encoder/layer_7/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_10/attention/output/LayerNorm/beta/read
bert/encoder/layer_10/output/dense/bias/read
bert/encoder/layer_11/attention/self/key/kernel/read
bert/encoder/layer_3/attention/self/value/kernel/read
bert/encoder/layer_3/intermediate/dense/bias/read
bert/encoder/layer_3/output/LayerNorm/beta/read
bert/encoder/layer_4/attention/self/value/bias/read
bert/encoder/layer_6/intermediate/dense/bias/adam_m/read
bert/encoder/layer_5/attention/self/query/kernel/read
bert/encoder/layer_6/output/dense/kernel/adam_v/read
bert/encoder/layer_6/output/dense/bias/adam_m/read
bert/encoder/layer_6/attention/self/query/bias/read
bert/encoder/layer_6/attention/output/dense/kernel/read
bert/encoder/layer_6/output/dense/bias/read
bert/encoder/layer_7/attention/self/value/kernel/read
bert/encoder/layer_7/attention/self/query/bias/adam_m/read
bert/encoder/layer_8/attention/self/query/kernel/read
bert/encoder/layer_7/attention/self/key/kernel/adam_v/read
bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_0/output/dropout/random_uniform/RandomUniform
1608802723831038	12582912
1608802723860821	-12582912
bert/encoder/layer_6/attention/output/dropout/random_uniform/RandomUniform
1608802723831103	12582912
1608802723860870	-12582912
bert/encoder/layer_8/output/dropout/random_uniform/RandomUniform
1608802723831167	12582912
1608802723860919	-12582912
bert/encoder/layer_4/attention/output/dropout/random_uniform/RandomUniform
1608802723831231	12582912
1608802723860971	-12582912
bert/encoder/layer_9/output/dropout/random_uniform/RandomUniform
1608802723831286	12582912
1608802723861024	-12582912
bert/encoder/layer_2/attention/output/dropout/random_uniform/RandomUniform
1608802723831365	12582912
1608802723861073	-12582912
bert/encoder/layer_4/output/dropout/random_uniform/RandomUniform
1608802723831429	12582912
1608802723861125	-12582912
bert/encoder/layer_10/attention/output/dropout/random_uniform/RandomUniform
1608802723831489	12582912
1608802723861176	-12582912
bert/encoder/layer_10/output/dropout/random_uniform/RandomUniform
1608802723831544	12582912
1608802723861225	-12582912
bert/encoder/layer_1/attention/output/dropout/random_uniform/RandomUniform
1608802723831596	12582912
1608802723861276	-12582912
bert/encoder/layer_5/attention/output/dropout/random_uniform/RandomUniform
1608802723831648	12582912
1608802723861327	-12582912
bert/encoder/layer_9/attention/output/dropout/random_uniform/RandomUniform
1608802723831712	12582912
1608802723861371	-12582912
bert/encoder/layer_11/attention/output/dropout/random_uniform/RandomUniform
1608802723831763	12582912
1608802723861421	-12582912
bert/encoder/layer_1/output/dropout/random_uniform/RandomUniform
1608802723831818	12582912
1608802723861472	-12582912
bert/encoder/layer_3/attention/output/dropout/random_uniform/RandomUniform
1608802723831871	12582912
1608802723861521	-12582912
bert/encoder/layer_6/output/dropout/random_uniform/RandomUniform
1608802723831932	12582912
1608802723861569	-12582912
bert/encoder/layer_11/output/dropout/random_uniform/RandomUniform
1608802723831986	12582912
1608802723861622	-12582912
bert/encoder/layer_2/output/dropout/random_uniform/RandomUniform
1608802723832039	12582912
1608802723861674	-12582912
bert/encoder/layer_3/output/dropout/random_uniform/RandomUniform
1608802723832091	12582912
1608802723861720	-12582912
bert/encoder/layer_7/attention/output/dropout/random_uniform/RandomUniform
1608802723832141	12582912
1608802723861770	-12582912
bert/encoder/layer_0/attention/output/dropout/random_uniform/RandomUniform
1608802723832196	12582912
1608802723861821	-12582912
bert/encoder/layer_5/output/dropout/random_uniform/RandomUniform
1608802723832256	12582912
1608802723861868	-12582912
bert/encoder/layer_7/output/dropout/random_uniform/RandomUniform
1608802723832309	12582912
1608802723861920	-12582912
bert/encoder/layer_8/attention/output/dropout/random_uniform/RandomUniform
1608802723832375	12582912
1608802723861969	-12582912
bert/encoder/layer_9/attention/output/dense/kernel/read
bert/encoder/layer_9/output/dense/kernel/read
bert/encoder/layer_10/attention/self/key/kernel/read
bert/encoder/layer_10/attention/output/LayerNorm/gamma/read
bert/encoder/layer_10/output/LayerNorm/beta/read
bert/encoder/layer_11/attention/self/key/bias/read
bert/encoder/layer_6/attention/output/dense/bias/adam_m/read
bert/encoder/layer_3/attention/self/value/bias/read
bert/encoder/layer_11/attention/output/dense/bias/adam_m/read
bert/encoder/layer_3/output/LayerNorm/gamma/read
bert/encoder/layer_6/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_4/output/dense/kernel/read
bert/encoder/layer_5/attention/self/query/bias/read
bert/encoder/layer_5/attention/output/dense/kernel/read
bert/encoder/layer_5/output/dense/kernel/read
bert/encoder/layer_6/attention/self/key/kernel/read
bert/encoder/layer_6/attention/output/dense/bias/read
bert/encoder/layer_6/output/LayerNorm/beta/read
bert/encoder/layer_7/intermediate/dense/bias/read
bert/encoder/layer_8/attention/self/query/bias/read
bert/encoder/layer_8/attention/self/value/kernel/read
bert/encoder/layer_8/intermediate/dense/kernel/read
bert/encoder/layer_9/attention/output/dense/bias/read
bert/encoder/layer_9/output/dense/bias/read
bert/encoder/layer_10/attention/self/key/bias/read
bert/encoder/layer_7/attention/output/dense/bias/adam_m/read
bert/encoder/layer_10/output/LayerNorm/gamma/read
bert/encoder/layer_11/attention/self/value/kernel/read
bert/encoder/layer_11/attention/output/LayerNorm/beta/read
bert/encoder/layer_2/output/LayerNorm/gamma/read
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_4/attention/self/query/kernel/read
bert/encoder/layer_4/attention/output/dense/kernel/read
bert/encoder/layer_11/intermediate/dense/bias/adam_v/read
bert/encoder/layer_5/attention/self/key/kernel/read
bert/encoder/layer_5/attention/output/dense/bias/read
bert/encoder/layer_5/output/dense/bias/read
bert/encoder/layer_6/attention/self/key/bias/read
bert/encoder/layer_6/attention/output/LayerNorm/beta/read
bert/encoder/layer_6/output/LayerNorm/gamma/read
bert/encoder/layer_7/attention/self/value/bias/read
bert/encoder/layer_7/attention/self/key/kernel/adam_m/read
bert/encoder/layer_8/attention/self/value/bias/read
bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_9/attention/self/query/kernel/read
bert/embeddings/word_embeddings/adam_m/read
bert/encoder/layer_7/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_11/attention/self/query/kernel/read
bert/encoder/layer_11/attention/self/value/bias/read
bert/encoder/layer_11/attention/output/LayerNorm/gamma/read
bert/encoder/layer_11/output/LayerNorm/gamma/read
bert/pooler/dense/bias/read
bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_4/attention/self/query/bias/read
bert/encoder/layer_1/output/dense/bias/adam_v/read
cls/predictions/transform/dense/bias/adam_v/read
bert/encoder/layer_5/attention/self/key/bias/read
bert/encoder/layer_5/attention/output/LayerNorm/beta/read
bert/encoder/layer_6/output/dense/bias/adam_v/read
bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_6/attention/output/LayerNorm/gamma/read
bert/encoder/layer_7/attention/self/query/kernel/read
bert/encoder/layer_7/attention/output/dense/kernel/read
bert/encoder/layer_9/intermediate/dense/kernel/adam_m/read
cls/predictions/output_bias/adam_v/read
bert/embeddings/word_embeddings/read
bert/encoder/layer_7/attention/self/key/bias/adam_m/read
bert/encoder/Reshape
bert/encoder/layer_0/attention/self/key/bias/read
cls/predictions/transform/dense/kernel/adam_v/read
bert/encoder/layer_0/attention/self/key/bias/adam_v/read
bert/encoder/layer_0/attention/output/dense/bias/adam_v/read
bert/encoder/layer_11/attention/output/dense/kernel/read
bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_7/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_6/attention/output/dense/bias/adam_v/read
cls/seq_relationship/output_bias/adam_v/read
bert/encoder/layer_2/attention/self/key/bias/adam_v/read
bert/encoder/layer_5/attention/output/LayerNorm/gamma/read
bert/encoder/layer_5/output/LayerNorm/beta/read
bert/encoder/layer_6/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_6/intermediate/dense/kernel/read
bert/encoder/layer_7/attention/self/query/bias/read
bert/encoder/layer_9/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_9/output/dense/bias/adam_v/read
bert/encoder/layer_10/attention/self/query/bias/adam_m/read
bert/encoder/layer_8/attention/output/dense/kernel/read
bert/encoder/layer_8/intermediate/dense/bias/read
bert/encoder/layer_0/attention/self/query/bias/read
bert/encoder/layer_0/attention/self/value/kernel/adam_m/read
bert/encoder/layer_10/attention/self/value/bias/adam_m/read
bert/encoder/layer_0/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_10/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_11/intermediate/dense/kernel/read
bert/encoder/layer_7/intermediate/dense/kernel/adam_v/read
cls/predictions/transform/dense/kernel/read
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_1/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_2/attention/self/query/bias/adam_v/read
bert/encoder/layer_2/attention/self/value/kernel/adam_m/read
bert/encoder/layer_6/attention/self/value/kernel/read
bert/encoder/layer_6/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_9/intermediate/dense/bias/adam_m/read
bert/encoder/layer_9/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_10/attention/self/query/bias/adam_v/read
bert/encoder/layer_8/attention/output/dense/bias/read
bert/encoder/layer_7/attention/self/key/bias/adam_v/read
bert/encoder/layer_9/attention/self/query/bias/read
bert/encoder/layer_0/attention/self/value/kernel/adam_v/read
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_0/intermediate/dense/bias/adam_m/read
bert/encoder/layer_0/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_1/attention/self/query/bias/adam_m/read
bert/encoder/layer_1/attention/self/value/kernel/adam_m/read
bert/encoder/layer_1/attention/output/dense/bias/adam_v/read
bert/encoder/layer_1/intermediate/dense/bias/adam_m/read
cls/predictions/output_bias/read
bert/encoder/layer_10/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_2/attention/self/value/kernel/adam_v/read
bert/encoder/layer_9/intermediate/dense/bias/adam_v/read
bert/pooler/dense/bias/adam_v/read
bert/encoder/layer_11/attention/self/value/kernel/adam_v/read
bert/encoder/layer_5/output/dense/kernel/adam_v/read
bert/embeddings/word_embeddings/adam_v/read
bert/embeddings/LayerNorm/beta/adam_m/read
bert/encoder/layer_7/attention/self/value/kernel/adam_m/read
bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_0/intermediate/dense/bias/adam_v/read
bert/encoder/layer_0/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_1/attention/self/query/bias/adam_v/read
bert/encoder/layer_1/attention/self/value/kernel/adam_v/read
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_1/intermediate/dense/bias/adam_v/read
bert/encoder/layer_1/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_2/attention/self/key/kernel/adam_m/read
bert/encoder/layer_2/attention/self/value/bias/adam_m/read
bert/encoder/layer_9/output/dense/kernel/adam_m/read
bert/encoder/layer_9/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_10/attention/self/key/kernel/adam_m/read
bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_5/output/dense/bias/adam_m/read
bert/embeddings/token_type_embeddings/adam_m/read
bert/embeddings/LayerNorm/beta/adam_v/read
cls/seq_relationship/output_bias/adam_m/read
bert/encoder/layer_0/attention/self/value/bias/adam_m/read
bert/encoder/layer_10/attention/self/value/bias/adam_v/read
bert/encoder/layer_0/output/dense/kernel/adam_m/read
bert/encoder/layer_0/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_1/attention/self/key/kernel/adam_m/read
bert/encoder/layer_1/attention/self/value/bias/adam_m/read
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v/read
cls/predictions/transform/dense/bias/adam_m/read
bert/encoder/layer_1/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_10/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_2/attention/self/value/bias/adam_v/read
bert/encoder/layer_2/attention/output/dense/bias/adam_v/read
bert/encoder/layer_10/intermediate/dense/bias/adam_m/read
bert/encoder/layer_2/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_2/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_2/output/dense/bias/adam_m/read
bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_2/intermediate/dense/bias/adam_m/read
bert/encoder/layer_9/output/dense/kernel/adam_v/read
bert/encoder/layer_9/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_5/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_11/attention/self/value/bias/adam_m/read
bert/embeddings/token_type_embeddings/adam_v/read
bert/embeddings/LayerNorm/gamma/adam_m/read
bert/encoder/layer_10/attention/self/value/kernel/adam_m/read
bert/encoder/layer_0/attention/self/value/bias/adam_v/read
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_10/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_0/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_10/attention/output/dense/bias/adam_m/read
bert/encoder/layer_1/attention/self/value/bias/adam_v/read
bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_1/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_2/attention/self/key/kernel/adam_v/read
bert/encoder/layer_2/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_2/intermediate/dense/bias/adam_v/read
bert/encoder/layer_2/output/dense/bias/adam_v/read
bert/encoder/layer_3/attention/self/query/kernel/adam_v/read
bert/encoder/layer_3/attention/self/key/bias/adam_v/read
bert/encoder/layer_3/attention/output/dense/bias/adam_m/read
bert/encoder/layer_3/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_10/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_10/intermediate/dense/bias/adam_v/read
bert/encoder/layer_2/output/dense/kernel/adam_m/read
bert/encoder/layer_2/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_9/output/dense/bias/adam_m/read
bert/encoder/layer_9/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_10/attention/self/key/kernel/adam_v/read
bert/encoder/layer_5/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_5/output/dense/bias/adam_v/read
bert/encoder/layer_10/attention/self/key/bias/adam_m/read
bert/embeddings/LayerNorm/gamma/adam_v/read
bert/encoder/layer_0/attention/self/key/kernel/adam_m/read
bert/encoder/layer_0/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_0/output/dense/kernel/adam_v/read
bert/encoder/layer_1/attention/self/query/kernel/adam_m/read
bert/encoder/layer_1/attention/self/key/kernel/adam_v/read
bert/encoder/layer_1/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_1/output/dense/kernel/adam_m/read
bert/encoder/layer_2/attention/self/query/kernel/adam_m/read
bert/encoder/layer_2/attention/self/key/bias/adam_m/read
bert/encoder/layer_2/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_3/attention/self/query/bias/adam_m/read
bert/encoder/layer_10/output/dense/bias/adam_v/read
bert/encoder/layer_3/attention/output/dense/bias/adam_v/read
bert/encoder/layer_3/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_3/output/dense/bias/adam_m/read
bert/encoder/layer_4/attention/self/query/kernel/adam_v/read
bert/encoder/layer_4/attention/self/key/bias/adam_v/read
bert/encoder/layer_4/attention/output/dense/bias/adam_m/read
bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_2/output/dense/kernel/adam_v/read
bert/encoder/layer_10/output/dense/kernel/adam_v/read
bert/encoder/layer_3/attention/self/query/bias/adam_v/read
bert/encoder/layer_3/attention/self/value/kernel/adam_m/read
bert/pooler/dense/bias/adam_m/read
bert/encoder/layer_10/attention/self/query/kernel/adam_m/read
cls/predictions/transform/dense/kernel/adam_m/read
bert/encoder/layer_5/intermediate/dense/bias/adam_m/read
bert/encoder/layer_5/output/LayerNorm/beta/adam_m/read
bert/embeddings/position_embeddings/adam_m/read
bert/encoder/layer_0/attention/self/query/kernel/adam_m/read
bert/encoder/layer_0/attention/self/key/kernel/adam_v/read
bert/encoder/layer_0/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_0/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_0/output/dense/bias/adam_m/read
bert/encoder/layer_1/attention/self/query/kernel/adam_v/read
bert/encoder/layer_1/attention/self/key/bias/adam_m/read
bert/encoder/layer_1/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_2/attention/self/query/kernel/adam_v/read
bert/encoder/layer_4/attention/output/dense/bias/adam_v/read
bert/encoder/layer_2/attention/output/dense/bias/adam_m/read
bert/encoder/layer_4/output/dense/bias/adam_m/read
bert/encoder/layer_5/attention/self/query/kernel/adam_v/read
bert/encoder/layer_10/output/dense/kernel/adam_m/read
bert/encoder/layer_2/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_3/attention/self/key/kernel/adam_m/read
bert/encoder/layer_3/attention/self/value/kernel/adam_v/read
cls/predictions/transform/LayerNorm/beta/adam_v/read
bert/encoder/layer_3/intermediate/dense/bias/adam_m/read
bert/encoder/layer_3/output/dense/bias/adam_v/read
bert/encoder/layer_4/attention/self/query/bias/adam_m/read
bert/encoder/layer_4/attention/self/value/kernel/adam_m/read
bert/encoder/layer_11/attention/self/query/bias/adam_m/read
bert/encoder/layer_4/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_4/output/dense/bias/adam_v/read
bert/encoder/layer_5/attention/self/query/bias/adam_m/read
bert/encoder/layer_10/attention/self/query/kernel/adam_v/read
bert/encoder/layer_5/attention/output/dense/bias/adam_m/read
bert/encoder/layer_5/intermediate/dense/bias/adam_v/read
bert/encoder/layer_11/attention/self/value/bias/adam_v/read
bert/encoder/layer_10/attention/self/key/bias/adam_v/read
bert/encoder/layer_0/attention/self/query/kernel/adam_v/read
bert/encoder/layer_10/attention/self/value/kernel/adam_v/read
bert/encoder/layer_0/attention/output/dense/bias/adam_m/read
bert/encoder/layer_4/attention/self/query/bias/adam_v/read
bert/encoder/layer_0/output/dense/bias/adam_v/read
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_10/attention/output/dense/bias/adam_v/read
bert/encoder/layer_1/attention/output/dense/bias/adam_m/read
bert/encoder/layer_1/intermediate/dense/kernel/adam_m/read
bert/encoder/layer_1/output/dense/kernel/adam_v/read
bert/encoder/layer_2/attention/self/query/bias/adam_m/read
bert/encoder/layer_2/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_3/attention/self/key/kernel/adam_v/read
bert/encoder/layer_3/attention/self/value/bias/adam_m/read
bert/encoder/layer_10/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_3/intermediate/dense/bias/adam_v/read
bert/encoder/layer_3/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_11/attention/self/query/kernel/adam_m/read
bert/encoder/layer_4/attention/self/value/kernel/adam_v/read
bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_4/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_11/attention/self/key/kernel/adam_v/read
bert/encoder/layer_5/attention/self/query/bias/adam_v/read
bert/encoder/layer_11/attention/self/key/bias/adam_v/read
bert/encoder/layer_5/attention/output/dense/bias/adam_v/read
bert/encoder/layer_2/output/LayerNorm/gamma/adam_v/read
cls/predictions/transform/LayerNorm/beta/adam_m/read
bert/encoder/layer_3/attention/self/value/bias/adam_v/read
bert/encoder/layer_5/output/dense/kernel/adam_m/read
bert/encoder/layer_5/output/LayerNorm/beta/adam_v/read
bert/embeddings/position_embeddings/adam_v/read
bert/encoder/layer_0/attention/self/query/bias/adam_m/read
bert/encoder/layer_0/attention/self/key/bias/adam_m/read
bert/encoder/layer_11/attention/self/query/bias/adam_v/read
bert/encoder/layer_4/intermediate/dense/bias/adam_m/read
bert/encoder/layer_4/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_5/attention/self/key/kernel/adam_m/read
bert/encoder/layer_1/attention/self/key/bias/adam_v/read
bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_1/output/dense/bias/adam_m/read
bert/encoder/layer_3/attention/self/query/kernel/adam_m/read
bert/encoder/layer_10/output/dense/bias/adam_m/read
bert/encoder/layer_3/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m/read
bert/encoder/layer_3/output/dense/kernel/adam_m/read
bert/encoder/layer_3/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_4/attention/self/key/kernel/adam_m/read
bert/encoder/layer_4/attention/self/value/bias/adam_m/read
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_4/intermediate/dense/bias/adam_v/read
bert/encoder/layer_4/output/LayerNorm/beta/adam_v/read
cls/predictions/transform/LayerNorm/gamma/adam_v/read
bert/encoder/layer_5/attention/self/key/bias/adam_v/read
bert/encoder/layer_11/attention/self/value/kernel/adam_m/read
bert/encoder/layer_3/attention/self/key/bias/adam_m/read
bert/encoder/layer_3/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_5/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_3/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_0/attention/self/query/bias/adam_v/read
bert/encoder/layer_4/attention/self/value/bias/adam_v/read
bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_4/output/dense/kernel/adam_m/read
bert/encoder/layer_4/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_11/attention/self/key/bias/adam_m/read
bert/encoder/layer_5/attention/self/value/kernel/adam_m/read
bert/encoder/layer_1/intermediate/dense/kernel/adam_v/read
bert/encoder/layer_10/output/LayerNorm/beta/adam_v/read
bert/encoder/layer_10/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_3/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_4/attention/self/key/kernel/adam_v/read
bert/encoder/layer_4/attention/output/dense/kernel/adam_m/read
bert/encoder/layer_11/attention/self/key/kernel/adam_m/read
bert/encoder/layer_4/output/LayerNorm/gamma/adam_v/read
bert/encoder/layer_5/attention/self/key/kernel/adam_v/read
bert/encoder/layer_5/attention/self/value/kernel/adam_v/read
bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m/read
bert/encoder/layer_3/output/dense/kernel/adam_v/read
bert/encoder/layer_4/attention/self/query/kernel/adam_m/read
bert/encoder/layer_11/attention/self/query/kernel/adam_v/read
bert/encoder/layer_4/attention/output/dense/kernel/adam_v/read
bert/encoder/layer_4/output/dense/kernel/adam_v/read
bert/encoder/layer_5/attention/self/query/kernel/adam_m/read
bert/encoder/layer_5/attention/self/key/bias/adam_m/read
bert/encoder/layer_5/attention/self/value/bias/adam_m/read
bert/encoder/layer_4/attention/self/key/bias/adam_m/read
cls/predictions/transform/LayerNorm/gamma/adam_m/read
bert/encoder/layer_5/attention/self/value/bias/adam_v/read
bert/encoder/layer_5/attention/output/dense/kernel/adam_m/read
Mul_497
1608802723836678	2359296
1608802724348622	-2359296
Cast
1608802723836751	256
1608802723862232	-256
add_699
1608802723836811	256
global_step/cond/Switch_1
global_step/cond/Read/ReadVariableOp/Switch
PolynomialDecay/Cast_2
1608802723836909	256
1608802724391432	-256
bert/embeddings/dropout/random_uniform/mul
mul_47
1608802723837091	2359296
1608802724405975	-2359296
Mul_559
1608802723837158	2359296
1608802724373737	-2359296
bert/embeddings/assert_less_equal/Assert/Assert
mul_20
1608802723837220	1572864
1608802724405913	-1572864
bert/embeddings/Slice/begin
Mul_572
1608802723837277	3145728
1608802724356270	-3145728
bert/embeddings/Slice/size
bert/embeddings/Reshape_4/shape
gradients/bert/embeddings/Slice_grad/concat
mul_230
1608802723837329	2359296
1608802724408716	-2359296
bert/embeddings/Slice
bert/embeddings/Reshape_4
mul_69
1608802723837384	2359296
1608802724406098	-2359296
mul_133
1608802723837435	2359296
1608802724406350	-2359296
Mul_642
1608802723837493	3072
1608802724350216	-3072
mul_703
1608802723837544	9437184
1608802724407689	-9437184
mul_789
1608802723837591	9437184
1608802724408895	-9437184
mul_821
1608802723837649	2359296
1608802724407117	-2359296
mul_918
1608802723837695	2359296
1608802724407625	-2359296
mul_950
1608802723837750	9437184
1608802724408255	-9437184
Mul_694
1608802723837802	12288
1608802724397802	-12288
Mul_705
1608802723837846	3072
1608802724396404	-3072
Mul_722
1608802723837892	2359296
1608802724355705	-2359296
Mul_1101
1608802723837943	8448
1608802724377767	-8448
Mul_759
1608802723837989	3072
1608802724398783	-3072
Mul_726
1608802723838036	3072
1608802724397932	-3072
Mul_715
1608802723838075	3072
1608802724397863	-3072
Mul_1051
1608802723838120	3072
1608802724356718	-3072
Mul_742
1608802723838164	2359296
1608802724376660	-2359296
Mul_761
1608802723838208	3072
1608802724354015	-3072
Mul_845
1608802723838258	3072
1608802724401797	-3072
mul_735
1608802723838304	2359296
1608802724407814	-2359296
Mul_678
1608802723838347	3072
1608802724397738	-3072
Mul_1032
1608802723838393	9437184
1608802724377427	-9437184
Mul_1038
1608802723838443	12288
1608802724402946	-12288
Mul_1034
1608802723838489	9437184
1608802724357532	-9437184
mul_1047
1608802723838542	9437184
1608802724409306	-9437184
Mul_764
1608802723838592	3072
1608802724399694	-3072
Mul_806
1608802723838635	2359296
1608802724377189	-2359296
Mul_717
1608802723838685	3072
1608802724353047	-3072
Mul_728
1608802723838726	3072
1608802724353107	-3072
Mul_1045
1608802723838774	9437184
1608802724358643	-9437184
Mul_774
1608802723838822	9437184
1608802724376295	-9437184
Mul_1103
1608802723838884	6144
1608802724358336	-6144
mul_896
1608802723838927	2359296
1608802724407557	-2359296
Mul_1061
1608802723838973	3072
1608802724358221	-3072
Mul_847
1608802723839014	3072
1608802724357034	-3072
Mul_680
1608802723839060	3072
1608802724352926	-3072
Mul_683
1608802723839104	3072
1608802724395625	-3072
Mul_776
1608802723839146	9437184
1608802724354953	-9437184
Mul_658
1608802723839197	2359296
1608802724352868	-2359296
Mul_662
1608802723839246	3072
1608802724394934	-3072
Mul_707
1608802723839288	3072
1608802724351603	-3072
mul_929
1608802723839348	2359296
1608802724408188	-2359296
Mul_675
1608802723839401	3072
1608802724352176	-3072
Mul_744
1608802723839445	2359296
1608802724355768	-2359296
Mul_766
1608802723839497	3072
1608802724354889	-3072
Mul_1054
1608802723839541	3072
1608802724402379	-3072
Mul_808
1608802723839589	2359296
1608802724356975	-2359296
mul_864
1608802723839639	9437184
1608802724409011	-9437184
Mul_731
1608802723839687	2359296
1608802724375863	-2359296
Mul_769
1608802723839738	3072
1608802724398846	-3072
Mul_780
1608802723839787	12288
1608802724400685	-12288
Mul_793
1608802723839830	3072
1608802724354134	-3072
mul_1068
1608802723839874	2359296
1608802724409369	-2359296
Mul_825
1608802723839924	3072
1608802724354191	-3072
Mul_1064
1608802723839966	2359296
1608802724377931	-2359296
Mul_699
1608802723840014	9437184
1608802724375809	-9437184
Mul_710
1608802723840058	3072
1608802724397120	-3072
Mul_1049
1608802723840106	3072
1608802724401487	-3072
Mul_782
1608802723840152	12288
1608802724355888	-12288
Mul_796
1608802723840195	3072
1608802724399819	-3072
Mul_812
1608802723840239	3072
1608802724396746	-3072
Mul_828
1608802723840285	2359296
1608802724375994	-2359296
mul_1105
1608802723840332	6144
1608802724409190	-6144
Mul_701
1608802723840395	9437184
1608802724353824	-9437184
Mul_720
1608802723840444	2359296
1608802724376631	-2359296
Mul_733
1608802723840493	2359296
1608802724353951	-2359296
Mul_748
1608802723840544	3072
1608802724397241	-3072
Mul_771
1608802723840586	3072
1608802724354074	-3072
Mul_785
1608802723840635	9437184
1608802724377160	-9437184
Mul_1056
1608802723840701	3072
1608802724357591	-3072
Mul_814
1608802723840742	3072
1608802724351920	-3072
Mul_830
1608802723840782	2359296
1608802724354259	-2359296
Mul_850
1608802723840844	3072
1608802724400018	-3072
Mul_696
1608802723840887	12288
1608802724352984	-12288
Mul_712
1608802723840925	3072
1608802724352290	-3072
Mul_737
1608802723840968	3072
1608802724399623	-3072
Mul_750
1608802723841014	3072
1608802724352419	-3072
Mul_787
1608802723841053	9437184
1608802724356911	-9437184
Mul_803
1608802723841099	3072
1608802724355953	-3072
Mul_817
1608802723841142	2359296
1608802724375285	-2359296
Mul_834
1608802723841182	3072
1608802724399948	-3072
Mul_852
1608802723841226	3072
1608802724355195	-3072
mul_90
1608802723841265	9437184
1608802724406223	-9437184
Mul_1043
1608802723841311	9437184
1608802724377903	-9437184
Mul_753
1608802723841359	2359296
1608802724375527	-2359296
mul_327
1608802723841403	2359296
1608802724409250	-2359296
Mul_791
1608802723841452	3072
1608802724398914	-3072
Mul_798
1608802723841505	3072
1608802724355013	-3072
Mul_823
1608802723841545	3072
1608802724398976	-3072
Mul_841
1608802723841587	2359296
1608802724356076	-2359296
Mul_855
1608802723841634	3072
1608802724400937	-3072
mul_58
1608802723841677	2359296
1608802724406160	-2359296
mul_155
1608802723841725	2359296
1608802724409746	-2359296
mul_187
1608802723841769	9437184
1608802724409813	-9437184
Mul_739
1608802723841815	3072
1608802724354821	-3072
Mul_755
1608802723841858	2359296
1608802724353174	-2359296
mul_262
1608802723841902	9437184
1608802724407427	-9437184
mul_294
1608802723841951	2359296
1608802724406602	-2359296
Mul_801
1608802723841999	3072
1608802724400748	-3072
Mul_1059
1608802723842038	3072
1608802724403015	-3072
Mul_836
1608802723842081	3072
1608802724355137	-3072
Mul_1066
1608802723842123	2359296
1608802724358706	-2359296
Mul_548
1608802723842167	2359296
1608802724375614	-2359296
Mul_554
1608802723842214	3072
1608802724395141	-3072
Mul_561
1608802723842254	2359296
1608802724349126	-2359296
Mul_508
1608802723842304	3072
1608802724350977	-3072
mul_144
1608802723842350	2359296
1608802724409690	-2359296
mul_176
1608802723842393	9437184
1608802724406409	-9437184
mul_208
1608802723842445	2359296
1608802724406989	-2359296
Mul_581
1608802723842491	2359296
1608802724374695	-2359296
Mul_819
1608802723842533	2359296
1608802724352618	-2359296
Mul_839
1608802723842579	2359296
1608802724376795	-2359296
Mul_857
1608802723842625	3072
1608802724356137	-3072
mul_391
1608802723842666	2359296
1608802724405014	-2359296
mul_14
1608802723842718	6144
1608802724409126	-6144
bert/embeddings/MatMul
1608802723842765	12582912
1608802723865287	-12582912
Mul_556
1608802723842851	3072
1608802724350467	-3072
Mul_1011
1608802723842900	2359296
1608802724376601	-2359296
Mul_565
1608802723842945	3072
1608802724395912	-3072
Mul_576
1608802723842990	3072
1608802724395285	-3072
Mul_583
1608802723843031	2359296
1608802724351230	-2359296
mul_273
1608802723843072	9437184
1608802724409548	-9437184
mul_359
1608802723843123	9437184
1608802724405210	-9437184
Mul_545
1608802723843171	3072
1608802724352547	-3072
mul_520
1608802723843212	9437184
1608802724405583	-9437184
mul_36
1608802723843259	2359296
1608802724406038	-2359296
Mul_634
1608802723843306	2359296
1608802724375364	-2359296
bert/encoder/layer_11/attention/self/dropout/random_uniform/mul
bert/encoder/layer_4/attention/self/dropout/random_uniform/mul
bert/encoder/layer_7/attention/self/dropout/random_uniform/mul
bert/encoder/layer_9/attention/self/dropout/random_uniform/mul
bert/encoder/layer_0/attention/self/dropout/random_uniform/mul
bert/encoder/layer_1/attention/self/dropout/random_uniform/mul
bert/encoder/layer_2/attention/self/dropout/random_uniform/mul
bert/encoder/layer_10/attention/self/dropout/random_uniform/mul
bert/encoder/layer_5/attention/self/dropout/random_uniform/mul
bert/encoder/layer_8/attention/self/dropout/random_uniform/mul
bert/encoder/layer_6/attention/self/dropout/random_uniform/mul
bert/encoder/layer_3/attention/self/dropout/random_uniform/mul
Mul_567
1608802723843523	3072
1608802724351166	-3072
mul_219
1608802723843570	2359296
1608802724406286	-2359296
mul_241
1608802723843617	2359296
1608802724407942	-2359296
mul_305
1608802723843660	2359296
1608802724409423	-2359296
Mul_599
1608802723843706	3072
1608802724351291	-3072
Mul_1019
1608802723843752	3072
1608802724356599	-3072
Mul_613
1608802723843811	9437184
1608802724375700	-9437184
Mul_626
1608802723843856	3072
1608802724351350	-3072
mul_649
1608802723843902	2359296
1608802724406728	-2359296
mul_122
1608802723843956	2359296
1608802724409627	-2359296
Mul_1013
1608802723844004	2359296
1608802724355637	-2359296
mul_832
1608802723844074	2359296
1608802724407874	-2359296
Mul_578
1608802723844118	3072
1608802724350599	-3072
Mul_602
1608802723844164	9437184
1608802724375016	-9437184
mul_434
1608802723844212	9437184
1608802724405335	-9437184
Mul_610
1608802723844258	12288
1608802724352737	-12288
mul_488
1608802723844304	2359296
1608802724405399	-2359296
Mul_550
1608802723844351	2359296
1608802724353366	-2359296
Mul_631
1608802723844411	3072
1608802724352068	-3072
Mul_1027
1608802723844459	3072
1608802724401427	-3072
mul_101
1608802723844503	9437184
1608802724406474	-9437184
Mul_570
1608802723844548	2359296
1608802724376878	-2359296
Mul_664
1608802723844595	3072
1608802724350289	-3072
mul_961
1608802723844634	9437184
1608802724408005	-9437184
mul_348
1608802723844683	9437184
1608802724409875	-9437184
Mul_1096
1608802723844732	122112
1608802724402506	-122112
mul_402
1608802723844777	2359296
1608802724405458	-2359296
Mul_1022
1608802723844828	3072
1608802724402254	-3072
mul_552
1608802723844874	2359296
1608802724407365	-2359296
mul_617
1608802723844914	9437184
1608802724407494	-9437184
Mul_636
1608802723844962	2359296
1608802724352802	-2359296
mul_692
1608802723845013	9437184
1608802724406922	-9437184
Mul_667
1608802723845056	2359296
1608802724374531	-2359296
mul_993
1608802723845103	2359296
1608802724409487	-2359296
mul_316
1608802723845153	2359296
1608802724406795	-2359296
Mul_608
1608802723845195	12288
1608802724397557	-12288
mul_466
1608802723845241	2359296
1608802724405707	-2359296
Mul_615
1608802723845287	9437184
1608802724353561	-9437184
Mul_619
1608802723845337	3072
1608802724395420	-3072
mul_585
1608802723845384	2359296
1608802724406667	-2359296
mul_660
1608802723845427	2359296
1608802724407236	-2359296
Mul_640
1608802723845474	3072
1608802724394861	-3072
mul_724
1608802723845516	2359296
1608802724408394	-2359296
Mul_647
1608802723845560	2359296
1608802724351420	-2359296
Mul_1029
1608802723845609	3072
1608802724356660	-3072
bert/encoder/layer_0/output/dropout/random_uniform/mul
bert/encoder/layer_6/attention/output/dropout/random_uniform/mul
bert/encoder/layer_8/output/dropout/random_uniform/mul
bert/encoder/layer_4/attention/output/dropout/random_uniform/mul
bert/encoder/layer_9/output/dropout/random_uniform/mul
bert/encoder/layer_2/attention/output/dropout/random_uniform/mul
bert/encoder/layer_4/output/dropout/random_uniform/mul
bert/encoder/layer_10/attention/output/dropout/random_uniform/mul
bert/encoder/layer_10/output/dropout/random_uniform/mul
bert/encoder/layer_1/attention/output/dropout/random_uniform/mul
bert/encoder/layer_5/attention/output/dropout/random_uniform/mul
bert/encoder/layer_9/attention/output/dropout/random_uniform/mul
bert/encoder/layer_11/attention/output/dropout/random_uniform/mul
bert/encoder/layer_1/output/dropout/random_uniform/mul
bert/encoder/layer_3/attention/output/dropout/random_uniform/mul
bert/encoder/layer_6/output/dropout/random_uniform/mul
bert/encoder/layer_11/output/dropout/random_uniform/mul
bert/encoder/layer_2/output/dropout/random_uniform/mul
bert/encoder/layer_3/output/dropout/random_uniform/mul
bert/encoder/layer_7/attention/output/dropout/random_uniform/mul
bert/encoder/layer_0/attention/output/dropout/random_uniform/mul
bert/encoder/layer_5/output/dropout/random_uniform/mul
bert/encoder/layer_7/output/dropout/random_uniform/mul
bert/encoder/layer_8/attention/output/dropout/random_uniform/mul
mul_843
1608802723845957	2359296
1608802724408534	-2359296
mul_875
1608802723846010	9437184
1608802724408596	-9437184
mul_907
1608802723846060	2359296
1608802724408129	-2359296
Mul_587
1608802723846103	3072
1608802724398306	-3072
Mul_1017
1608802723846148	3072
1608802724401358	-3072
Mul_604
1608802723846188	9437184
1608802724351986	-9437184
mul_445
1608802723846238	9437184
1608802724405644	-9437184
mul_499
1608802723846285	2359296
1608802724405522	-2359296
mul_531
1608802723846328	9437184
1608802724407049	-9437184
mul_563
1608802723846375	2359296
1608802724405768	-2359296
mul_746
1608802723846424	2359296
1608802724408477	-2359296
mul_778
1608802723846470	9437184
1608802724408065	-9437184
Mul_673
1608802723846519	3072
1608802724397000	-3072
mul_1004
1608802723846563	2359296
1608802724408779	-2359296
Mul_592
1608802723846604	3072
1608802724399153	-3072
Mul_597
1608802723846647	3072
1608802724396048	-3072
mul_380
1608802723846691	2359296
1608802724405270	-2359296
mul_413
1608802723846735	2359296
1608802724405081	-2359296
Mul_1040
1608802723846785	12288
1608802724358157	-12288
mul_477
1608802723846827	2359296
1608802724405143	-2359296
Mul_645
1608802723846875	2359296
1608802724374772	-2359296
Mul_506
1608802723846924	3072
1608802724395695	-3072
mul_810
1608802723846966	2359296
1608802724408954	-2359296
Mul_4
1608802723847019	93763584
1608802724373814	-93763584
Mul_669
1608802723847070	2359296
1608802724350856	-2359296
mul_982
1608802723847129	2359296
1608802724407749	-2359296
Mul_594
1608802723847172	3072
1608802724354383	-3072
Mul_191
1608802723847213	3072
1608802724351482	-3072
Mul_1083
1608802723847250	3072
1608802724357657	-3072
Mul_621
1608802723847285	3072
1608802724350723	-3072
Mul_1024
1608802723847321	3072
1608802724357467	-3072
mul_638
1608802723847365	2359296
1608802724407178	-2359296
mul_671
1608802723847445	2359296
1608802724406538	-2359296
Mul_860
1608802723847492	9437184
1608802724377244	-9437184
Mul_1098
1608802723847557	122112
1608802724357721	-122112
mul_8
1608802723847610	93763584
1608802724405845	-93763584
bert/embeddings/GatherV2
1608802723847657	12582912
1608802724251522	-12582912
Mul_651
1608802723847722	3072
1608802724396935	-3072
Mul_1077
1608802723847779	2359296
1608802724356785	-2359296
Mul_51
1608802723847826	3072
1608802724349735	-3072
Mul_73
1608802723847874	3072
1608802724350026	-3072
mul_1015
1608802723847916	2359296
1608802724408320	-2359296
Mul_685
1608802723847957	3072
1608802724350916	-3072
Mul_688
1608802723848001	9437184
1608802724375121	-9437184
Mul_589
1608802723848049	3072
1608802724353499	-3072
Mul_1109
1608802723848089	256
1608802724358834	-256
Mul_223
1608802723848134	3072
1608802724355255	-3072
Mul_624
1608802723848178	3072
1608802724396121	-3072
mul_606
1608802723848218	9437184
1608802724406861	-9437184
Mul_862
1608802723848266	9437184
1608802724357103	-9437184
Mul_879
1608802723848312	3072
1608802724357163	-3072
Mul_898
1608802723848358	3072
1608802724399220	-3072
mul_757
1608802723848412	2359296
1608802724407303	-2359296
Mul_54
1608802723848453	2359296
1608802724374150	-2359296
Mul_920
1608802723848501	3072
1608802724399280	-3072
Mul_88
1608802723848547	9437184
1608802724350091	-9437184
Mul_927
1608802723848591	2359296
1608802724355389	-2359296
mul_1036
1608802723848637	9437184
1608802724409071	-9437184
Mul_690
1608802723848685	9437184
1608802724352236	-9437184
mul_1079
1608802723848727	2359296
1608802724408834	-2359296
Mul_511
1608802723848774	3072
1608802724396489	-3072
Mul_194
1608802723848814	3072
1608802724402820	-3072
Mul_212
1608802723848862	3072
1608802724353234	-3072
Mul_226
1608802723848905	2359296
1608802724376917	-2359296
mul_574
1608802723848948	2359296
1608802724408659	-2359296
Mul_629
1608802723848995	3072
1608802724396872	-3072
Mul_866
1608802723849039	12288
1608802724402629	-12288
Mul_882
1608802723849081	3072
1608802724402695	-3072
Mul_900
1608802723849124	3072
1608802724354445	-3072
Mul_653
1608802723849168	3072
1608802724352124	-3072
Mul_56
1608802723849207	2359296
1608802724349959	-2359296
Mul_76
1608802723849255	3072
1608802724395003	-3072
Mul_92
1608802723849298	12288
1608802724395076	-12288
Mul_108
1608802723849342	3072
1608802724403319	-3072
Mul_124
1608802723849385	3072
1608802724404306	-3072
Mul_140
1608802723849425	2359296
1608802724378226	-2359296
Mul_159
1608802723849474	3072
1608802724355074	-3072
Mul_178
1608802723849524	12288
1608802724403379	-12288
Mul_946
1608802723849563	9437184
1608802724376524	-9437184
Mul_228
1608802723849609	2359296
1608802724356334	-2359296
Mul_868
1608802723849658	12288
1608802724357842	-12288
Mul_1072
1608802723849699	3072
1608802724355829	-3072
Mul_1002
1608802723849757	2359296
1608802724356539	-2359296
Mul_529
1608802723849802	9437184
1608802724352484	-9437184
Mul_6
1608802723849853	93763584
1608802724349301	-93763584
Mul_22
1608802723849905	3072
1608802724393902	-3072
Mul_656
1608802723849945	2359296
1608802724375390	-2359296
Mul_78
1608802723849994	3072
1608802724350347	-3072
Mul_94
1608802723850040	12288
1608802724350407	-12288
Mul_110
1608802723850080	3072
1608802724358522	-3072
Mul_126
1608802723850124	3072
1608802724359574	-3072
Mul_142
1608802723850162	2359296
1608802724359405	-2359296
Mul_162
1608802723850211	3072
1608802724404246	-3072
Mul_180
1608802723850256	12288
1608802724358581	-12288
Mul_196
1608802723850295	3072
1608802724358036	-3072
Mul_215
1608802723850339	2359296
1608802724374231	-2359296
Mul_232
1608802723850388	3072
1608802724402884	-3072
Mul_871
1608802723850429	9437184
1608802724376852	-9437184
Mul_884
1608802723850478	3072
1608802724357906	-3072
Mul_903
1608802723850522	2359296
1608802724376462	-2359296
Mul_513
1608802723850564	3072
1608802724351665	-3072
Mul_533
1608802723850610	3072
1608802724398123	-3072
Mul_10
1608802723850652	6144
1608802724377535	-6144
Mul_24
1608802723850695	3072
1608802724349410	-3072
Mul_1107
1608802723850738	256
1608802724403623	-256
Mul_60
1608802723850777	3072
1608802724394177	-3072
Mul_922
1608802723850819	3072
1608802724354507	-3072
Mul_97
1608802723850865	9437184
1608802724374503	-9437184
Mul_113
1608802723850909	3072
1608802724400814	-3072
Mul_129
1608802723850964	2359296
1608802724374394	-2359296
Mul_146
1608802723851016	3072
1608802724404384	-3072
Mul_938
1608802723851058	3072
1608802724353760	-3072
Mul_1081
1608802723851103	3072
1608802724402439	-3072
Mul_199
1608802723851143	3072
1608802724396324	-3072
Mul_948
1608802723851189	9437184
1608802724355455	-9437184
Mul_234
1608802723851236	3072
1608802724358096	-3072
Mul_245
1608802723851277	3072
1608802724357407	-3072
Mul_952
1608802723851320	12288
1608802724401163	-12288
Mul_258
1608802723851366	9437184
1608802724375643	-9437184
Mul_248
1608802723851410	3072
1608802724403556	-3072
Mul_260
1608802723851457	9437184
1608802724353435	-9437184
Mul_275
1608802723851500	3072
1608802724403997	-3072
Mul_250
1608802723851547	3072
1608802724358773	-3072
Mul_264
1608802723851592	12288
1608802724403875	-12288
Mul_873
1608802723851636	9437184
1608802724356207	-9437184
Mul_887
1608802723851682	3072
1608802724403251	-3072
Mul_516
1608802723851729	9437184
1608802724373522	-9437184
Mul_1006
1608802723851771	3072
1608802724399539	-3072
Mul_12
1608802723851816	6144
1608802724357783	-6144
Mul_27
1608802723851868	3072
1608802724394042	-3072
Mul_914
1608802723851909	2359296
1608802724375754	-2359296
Mul_62
1608802723851956	3072
1608802724349628	-3072
Mul_81
1608802723851995	3072
1608802724394449	-3072
Mul_925
1608802723852040	2359296
1608802724376493	-2359296
Mul_115
1608802723852086	3072
1608802724356014	-3072
Mul_931
1608802723852128	3072
1608802724402058	-3072
Mul_148
1608802723852172	3072
1608802724359627	-3072
Mul_164
1608802723852216	3072
1608802724359517	-3072
Mul_941
1608802723852258	3072
1608802724399349	-3072
Mul_201
1608802723852303	3072
1608802724351542	-3072
Mul_217
1608802723852342	2359296
1608802724350156	-2359296
Mul_237
1608802723852404	2359296
1608802724376025	-2359296
Mul_341
1608802723852455	3072
1608802724348498	-3072
Mul_253
1608802723852496	3072
1608802724404514	-3072
Mul_266
1608802723852543	12288
1608802724359086	-12288
Mul_277
1608802723852599	3072
1608802724359211	-3072
Mul_292
1608802723852637	2359296
1608802724351042	-2359296
Mul_309
1608802723852682	3072
1608802724351104	-3072
Mul_329
1608802723852727	3072
1608802724404763	-3072
Mul_344
1608802723852765	9437184
1608802724378495	-9437184
Mul_975
1608802723852810	3072
1608802724357345	-3072
Mul_954
1608802723852853	12288
1608802724356402	-12288
Mul_269
1608802723852895	9437184
1608802724378120	-9437184
Mul_280
1608802723852940	3072
1608802724403683	-3072
Mul_877
1608802723852978	3072
1608802724401925	-3072
Mul_889
1608802723853021	3072
1608802724358462	-3072
Mul_905
1608802723853061	2359296
1608802724355318	-2359296
Mul_518
1608802723853102	9437184
1608802724348680	-9437184
Mul_535
1608802723853149	3072
1608802724353293	-3072
Mul_909
1608802723853192	3072
1608802724401987	-3072
Mul_29
1608802723853230	3072
1608802724349517	-3072
Mul_43
1608802723853271	2359296
1608802724373949	-2359296
Mul_65
1608802723853311	2359296
1608802724374060	-2359296
Mul_83
1608802723853358	3072
1608802724349846	-3072
Mul_99
1608802723853401	9437184
1608802724337912	-9437184
Mul_118
1608802723853442	2359296
1608802724378199	-2359296
Mul_131
1608802723853488	2359296
1608802724350533	-2359296
Mul_151
1608802723853538	2359296
1608802724378390	-2359296
Mul_167
1608802723853581	3072
1608802724404451	-3072
Mul_183
1608802723853625	9437184
1608802724378442	-9437184
Mul_204
1608802723853672	2359296
1608802724375175	-2359296
Mul_221
1608802723853763	3072
1608802724400079	-3072
Mul_239
1608802723853821	2359296
1608802724354324	-2359296
Mul_296
1608802723853864	3072
1608802724396616	-3072
Mul_965
1608802723853911	3072
1608802724355514	-3072
Mul_331
1608802723853968	3072
1608802724359961	-3072
Mul_346
1608802723854007	9437184
1608802724360018	-9437184
Mul_361
1608802723854054	3072
1608802724392883	-3072
Mul_378
1608802723854101	2359296
1608802724347994	-2359296
Mul_395
1608802723854145	3072
1608802724347452	-3072
Mul_415
1608802723854190	3072
1608802724392327	-3072
Mul_255
1608802723854229	3072
1608802724359733	-3072
Mul_271
1608802723854272	9437184
1608802724359151	-9437184
Mul_959
1608802723854320	9437184
1608802724354634	-9437184
Mul_298
1608802723854365	3072
1608802724351789	-3072
Mul_312
1608802723854410	2359296
1608802724374960	-2359296
Mul_1070
1608802723854455	3072
1608802724400619	-3072
Mul_892
1608802723854493	2359296
1608802724375725	-2359296
Mul_1075
1608802723854538	2359296
1608802724377107	-2359296
Mul_522
1608802723854587	12288
1608802724396548	-12288
Mul_538
1608802723854627	3072
1608802724394527	-3072
Mul_16
1608802723854672	1572864
1608802724373846	-1572864
Mul_32
1608802723854713	2359296
1608802724374008	-2359296
Mul_45
1608802723854759	2359296
1608802724349573	-2359296
Mul_67
1608802723854805	2359296
1608802724349793	-2359296
Mul_86
1608802723854847	9437184
1608802724374204	-9437184
Mul_103
1608802723854896	3072
1608802724404066	-3072
Mul_120
1608802723854940	2359296
1608802724359341	-2359296
Mul_135
1608802723854982	3072
1608802724402755	-3072
Mul_153
1608802723855026	2359296
1608802724359789	-2359296
Mul_169
1608802723855070	3072
1608802724359679	-3072
Mul_943
1608802723855110	3072
1608802724354572	-3072
Mul_206
1608802723855155	2359296
1608802724352348	-2359296
Mul_417
1608802723855198	3072
1608802724348103	-3072
Mul_243
1608802723855243	3072
1608802724402188	-3072
Mul_447
1608802723855285	3072
1608802724393624	-3072
Mul_464
1608802723855324	2359296
1608802724348851	-2359296
Mul_957
1608802723855370	9437184
1608802724376163	-9437184
Mul_282
1608802723855415	3072
1608802724358897	-3072
Mul_301
1608802723855455	2359296
1608802724378035	-2359296
Mul_314
1608802723855502	2359296
1608802724351859	-2359296
Mul_1088
1608802723855551	3072
1608802724358279	-3072
Mul_350
1608802723855589	12288
1608802724391834	-12288
Mul_363
1608802723855629	3072
1608802724348558	-3072
Mul_382
1608802723855667	3072
1608802724393413	-3072
Mul_398
1608802723855710	2359296
1608802724373359	-2359296
Mul_984
1608802723855754	3072
1608802724399472	-3072
Mul_430
1608802723855795	9437184
1608802724373276	-9437184
Mul_449
1608802723855840	3072
1608802724349187	-3072
Mul_468
1608802723855885	3072
1608802724393695	-3072
Mul_894
1608802723855922	2359296
1608802724353623	-2359296
Mul_501
1608802723855965	3072
1608802724393347	-3072
Mul_524
1608802723856010	12288
1608802724351727	-12288
Mul_1008
1608802723856046	3072
1608802724354758	-3072
Mul_911
1608802723856093	3072
1608802724357222	-3072
Mul_34
1608802723856129	2359296
1608802724349683	-2359296
Mul_916
1608802723856174	2359296
1608802724353696	-2359296
Mul_71
1608802723856218	3072
1608802724394658	-3072
Mul_384
1608802723856259	3072
1608802724349018	-3072
Mul_105
1608802723856302	3072
1608802724359273	-3072
Mul_420
1608802723856348	3072
1608802724392673	-3072
Mul_933
1608802723856400	3072
1608802724357281	-3072
Mul_157
1608802723856443	3072
1608802724399886	-3072
Mul_172
1608802723856484	9437184
1608802724374449	-9437184
Mul_185
1608802723856534	9437184
1608802724359908	-9437184
Mul_210
1608802723856582	3072
1608802724398058	-3072
Mul_285
1608802723856622	3072
1608802724404635	-3072
Mul_303
1608802723856665	2359296
1608802724358961	-2359296
Mul_318
1608802723856714	3072
1608802724397496	-3072
Mul_968
1608802723856755	3072
1608802724401236	-3072
Mul_352
1608802723856800	12288
1608802724347512	-12288
Mul_366
1608802723856840	3072
1608802724404885	-3072
Mul_978
1608802723856882	2359296
1608802724375837	-2359296
Mul_400
1608802723856927	2359296
1608802724348323	-2359296
Mul_422
1608802723856970	3072
1608802724348386	-3072
Mul_432
1608802723857013	9437184
1608802724348158	-9437184
Mul_991
1608802723857060	2359296
1608802724359022	-2359296
Mul_470
1608802723857100	3072
1608802724349239	-3072
Mul_997
1608802723857144	3072
1608802724355573	-3072
Mul_503
1608802723857185	3072
1608802724348963	-3072
Mul_287
1608802723857222	3072
1608802724359848	-3072
Mul_1086
1608802723857262	3072
1608802724403075	-3072
Mul_320
1608802723857305	3072
1608802724352678	-3072
Mul_527
1608802723857344	9437184
1608802724375231	-9437184
Mul_540
1608802723857388	3072
1608802724349903	-3072
Mul_18
1608802723857426	1572864
1608802724349356	-1572864
Mul_38
1608802723857474	3072
1608802724393972	-3072
Mul_49
1608802723857517	3072
1608802724394312	-3072
Mul_986
1608802723857558	3072
1608802724354696	-3072
Mul_436
1608802723857601	12288
1608802724392743	-12288
Mul_452
1608802723857643	3072
1608802724391986	-3072
Mul_473
1608802723857680	2359296
1608802724373074	-2359296
Mul_137
1608802723857725	3072
1608802724357975	-3072
Mul_936
1608802723857765	3072
1608802724398543	-3072
Mul_189
1608802723857807	3072
1608802724396254	-3072
Mul_290
1608802723857849	2359296
1608802724374614	-2359296
Mul_963
1608802723857892	3072
1608802724400311	-3072
Mul_323
1608802723857935	2359296
1608802724377796	-2359296
Mul_334
1608802723857981	3072
1608802724392057	-3072
Mul_355
1608802723858018	9437184
1608802724373163	-9437184
Mul_368
1608802723858062	3072
1608802724360071	-3072
Mul_387
1608802723858105	2359296
1608802724372956	-2359296
Mul_404
1608802723858148	3072
1608802724393487	-3072
Mul_425
1608802723858199	3072
1608802724393082	-3072
Mul_438
1608802723858239	12288
1608802724348444	-12288
Mul_454
1608802723858282	3072
1608802724347823	-3072
Mul_1093
1608802723858324	3072
1608802724356847	-3072
Mul_481
1608802723858361	3072
1608802724348049	-3072
Mul_1000
1608802723858402	2359296
1608802724376997	-2359296
Mul_307
1608802723858447	3072
1608802724395834	-3072
Mul_325
1608802723858488	2359296
1608802724358398	-2359296
Mul_336
1608802723858555	3072
1608802724347879	-3072
Mul_543
1608802723858595	3072
1608802724397365	-3072
Mul_371
1608802723858632	3072
1608802724404945	-3072
Mul_40
1608802723858670	3072
1608802724349464	-3072
Mul_406
1608802723858706	3072
1608802724349070	-3072
Mul_427
1608802723858742	3072
1608802724348737	-3072
Mul_441
1608802723858780	9437184
1608802724373575	-9437184
Mul_457
1608802723858820	3072
1608802724392478	-3072
Mul_995
1608802723858859	3072
1608802724400388	-3072
Mul_484
1608802723858900	2359296
1608802724373330	-2359296
Mul_174
1608802723858946	9437184
1608802724350663	-9437184
Mul_970
1608802723858994	3072
1608802724356470	-3072
Mul_973
1608802723859035	3072
1608802724402121	-3072
Mul_373
1608802723859078	3072
1608802724360123	-3072
Mul_389
1608802723859120	2359296
1608802724347320	-2359296
Mul_409
1608802723859161	2359296
1608802724372988	-2359296
Mul_989
1608802723859211	2359296
1608802724378064	-2359296
Mul_459
1608802723859257	3072
1608802724348211	-3072
Mul_475
1608802723859294	2359296
1608802724347768	-2359296
Mul_486
1608802723859339	2359296
1608802724348267	-2359296
Mul_339
1608802723859384	3072
1608802724392818	-3072
Mul_357
1608802723859426	9437184
1608802724347938	-9437184
Mul_376
1608802723859470	2359296
1608802724373195	-2359296
Mul_980
1608802723859519	2359296
1608802724353889	-2359296
Mul_411
1608802723859561	2359296
1608802724347386	-2359296
Mul_443
1608802723859608	9437184
1608802724348795	-9437184
Mul_462
1608802723859649	2359296
1608802724373604	-2359296
Mul_479
1608802723859694	3072
1608802724392257	-3072
Mul_490
1608802723859736	3072
1608802724393275	-3072
Mul_393
1608802723859773	3072
1608802724391763	-3072
Mul_1091
1608802723859814	3072
1608802724401623	-3072
Mul_492
1608802723859856	3072
1608802724348906	-3072
Mul_495
1608802723859892	2359296
1608802724373492	-2359296
Cast_1
1608802723859935	256
1608802723863208	-256
AssignVariableOp
global_step/cond/Read/ReadVariableOp
PolynomialDecay/Minimum
bert/embeddings/dropout/GreaterEqual
1608802723860085	3145728
1608802723862472	-3145728
bert/embeddings/Reshape_3
bert/encoder/layer_11/attention/self/dropout/GreaterEqual
1608802723860167	6291456
1608802723862835	-6291456
bert/encoder/layer_4/attention/self/dropout/GreaterEqual
1608802723860222	6291456
1608802723863043	-6291456
bert/encoder/layer_7/attention/self/dropout/GreaterEqual
1608802723860270	6291456
1608802723863165	-6291456
bert/encoder/layer_9/attention/self/dropout/GreaterEqual
1608802723860323	6291456
1608802723863243	-6291456
bert/encoder/layer_0/attention/self/dropout/GreaterEqual
1608802723860382	6291456
1608802723863308	-6291456
bert/encoder/layer_1/attention/self/dropout/GreaterEqual
1608802723860437	6291456
1608802723863372	-6291456
bert/encoder/layer_2/attention/self/dropout/GreaterEqual
1608802723860485	6291456
1608802723863428	-6291456
bert/encoder/layer_10/attention/self/dropout/GreaterEqual
1608802723860538	6291456
1608802723863492	-6291456
bert/encoder/layer_5/attention/self/dropout/GreaterEqual
1608802723860590	6291456
1608802723863552	-6291456
bert/encoder/layer_8/attention/self/dropout/GreaterEqual
1608802723860640	6291456
1608802723863607	-6291456
bert/encoder/layer_6/attention/self/dropout/GreaterEqual
1608802723860689	6291456
1608802723863671	-6291456
bert/encoder/layer_3/attention/self/dropout/GreaterEqual
1608802723860739	6291456
1608802723863733	-6291456
bert/encoder/layer_0/output/dropout/GreaterEqual
1608802723860790	3145728
1608802723863784	-3145728
bert/encoder/layer_6/attention/output/dropout/GreaterEqual
1608802723860836	3145728
1608802723863869	-3145728
bert/encoder/layer_8/output/dropout/GreaterEqual
1608802723860889	3145728
1608802723863933	-3145728
bert/encoder/layer_4/attention/output/dropout/GreaterEqual
1608802723860940	3145728
1608802723863991	-3145728
bert/encoder/layer_9/output/dropout/GreaterEqual
1608802723860990	3145728
1608802723864055	-3145728
bert/encoder/layer_2/attention/output/dropout/GreaterEqual
1608802723861043	3145728
1608802723864115	-3145728
bert/encoder/layer_4/output/dropout/GreaterEqual
1608802723861095	3145728
1608802723864169	-3145728
bert/encoder/layer_10/attention/output/dropout/GreaterEqual
1608802723861140	3145728
1608802723864238	-3145728
bert/encoder/layer_10/output/dropout/GreaterEqual
1608802723861195	3145728
1608802723864298	-3145728
bert/encoder/layer_1/attention/output/dropout/GreaterEqual
1608802723861246	3145728
1608802723864350	-3145728
bert/encoder/layer_5/attention/output/dropout/GreaterEqual
1608802723861292	3145728
1608802723864452	-3145728
bert/encoder/layer_9/attention/output/dropout/GreaterEqual
1608802723861342	3145728
1608802723864516	-3145728
bert/encoder/layer_11/attention/output/dropout/GreaterEqual
1608802723861391	3145728
1608802723864567	-3145728
bert/encoder/layer_1/output/dropout/GreaterEqual
1608802723861437	3145728
1608802723864628	-3145728
bert/encoder/layer_3/attention/output/dropout/GreaterEqual
1608802723861487	3145728
1608802723864688	-3145728
bert/encoder/layer_6/output/dropout/GreaterEqual
1608802723861539	3145728
1608802723864746	-3145728
bert/encoder/layer_11/output/dropout/GreaterEqual
1608802723861586	3145728
1608802723864808	-3145728
bert/encoder/layer_2/output/dropout/GreaterEqual
1608802723861640	3145728
1608802723864868	-3145728
bert/encoder/layer_3/output/dropout/GreaterEqual
1608802723861690	3145728
1608802723864922	-3145728
bert/encoder/layer_7/attention/output/dropout/GreaterEqual
1608802723861739	3145728
1608802723864983	-3145728
bert/encoder/layer_0/attention/output/dropout/GreaterEqual
1608802723861787	3145728
1608802723865043	-3145728
bert/encoder/layer_5/output/dropout/GreaterEqual
1608802723861839	3145728
1608802723865094	-3145728
bert/encoder/layer_7/output/dropout/GreaterEqual
1608802723861889	3145728
1608802723865160	-3145728
bert/encoder/layer_8/attention/output/dropout/GreaterEqual
1608802723861935	3145728
1608802723865221	-3145728
bert/embeddings/Reshape_1
mul_1
bert/encoder/Cast
1608802723862227	16384
1608802723862440	-16384
global_step/cond/Identity
PolynomialDecay/truediv
Less
bert/encoder/mul
1608802723862353	2097152
1608802723951596	-2097152
bert/embeddings/dropout/Cast
1608802723862358	12582912
1608802724247550	-12582912
bert/encoder/layer_0/attention/self/ExpandDims
bert/encoder/layer_11/attention/self/dropout/Cast
1608802723862516	25165824
1608802724003131	-25165824
bert/encoder/layer_0/attention/self/sub
Cast_3
1608802723862745	256
1608802723868106	-256
bert/encoder/layer_0/attention/self/mul_1
bert/encoder/layer_4/attention/self/dropout/Cast
1608802723862929	25165824
1608802724153095	-25165824
sub
1608802723862931	256
1608802723867992	-256
bert/encoder/layer_7/attention/self/dropout/Cast
1608802723863077	25165824
1608802724086615	-25165824
mul_3
bert/encoder/layer_9/attention/self/dropout/Cast
1608802723863197	25165824
1608802724046507	-25165824
bert/encoder/layer_0/attention/self/dropout/Cast
1608802723863268	25165824
1608802724241833	-25165824
bert/encoder/layer_1/attention/self/dropout/Cast
1608802723863332	25165824
1608802724219630	-25165824
bert/encoder/layer_2/attention/self/dropout/Cast
1608802723863393	25165824
1608802724197488	-25165824
bert/encoder/layer_10/attention/self/dropout/Cast
1608802723863455	25165824
1608802724025931	-25165824
bert/encoder/layer_5/attention/self/dropout/Cast
1608802723863512	25165824
1608802724130917	-25165824
bert/encoder/layer_8/attention/self/dropout/Cast
1608802723863573	25165824
1608802724064478	-25165824
bert/encoder/layer_6/attention/self/dropout/Cast
1608802723863635	25165824
1608802724108776	-25165824
bert/encoder/layer_3/attention/self/dropout/Cast
1608802723863694	25165824
1608802724175310	-25165824
bert/encoder/layer_0/output/dropout/Cast
1608802723863752	12582912
1608802724230545	-12582912
bert/encoder/layer_6/attention/output/dropout/Cast
1608802723863832	15728640
1608802724105845	-15728640
bert/encoder/layer_8/output/dropout/Cast
1608802723863893	12582912
1608802724053172	-12582912
bert/encoder/layer_4/attention/output/dropout/Cast
1608802723863956	12582912
1608802724150155	-12582912
bert/encoder/layer_9/output/dropout/Cast
1608802723864020	12582912
1608802724031283	-12582912
bert/encoder/layer_2/attention/output/dropout/Cast
1608802723864075	12582912
1608802724194445	-12582912
bert/encoder/layer_4/output/dropout/Cast
1608802723864135	12582912
1608802724141865	-12582912
bert/encoder/layer_10/attention/output/dropout/Cast
1608802723864197	12582912
1608802724020514	-12582912
bert/encoder/layer_10/output/dropout/Cast
1608802723864259	12582912
1608802724012236	-12582912
bert/encoder/layer_1/attention/output/dropout/Cast
1608802723864317	12582912
1608802724216587	-12582912
bert/encoder/layer_5/attention/output/dropout/Cast
1608802723864412	12582912
1608802724127985	-12582912
bert/encoder/layer_9/attention/output/dropout/Cast
1608802723864476	12582912
1608802724044892	-12582912
bert/encoder/layer_11/attention/output/dropout/Cast
1608802723864535	12582912
1608802723995218	-12582912
bert/encoder/layer_1/output/dropout/Cast
1608802723864594	12582912
1608802724208390	-12582912
bert/encoder/layer_3/attention/output/dropout/Cast
1608802723864647	12582912
1608802724172244	-12582912
bert/encoder/layer_6/output/dropout/Cast
1608802723864711	12582912
1608802724097526	-12582912
bert/encoder/layer_11/output/dropout/Cast
1608802723864773	12582912
1608802723989703	-12582912
bert/encoder/layer_2/output/dropout/Cast
1608802723864828	12582912
1608802724186224	-12582912
bert/encoder/layer_3/output/dropout/Cast
1608802723864889	12582912
1608802724164003	-12582912
bert/encoder/layer_7/attention/output/dropout/Cast
1608802723864949	12582912
1608802724083639	-12582912
bert/encoder/layer_0/attention/output/dropout/Cast
1608802723865003	23068672
1608802724238780	-23068672
bert/encoder/layer_5/output/dropout/Cast
1608802723865061	12582912
1608802724119690	-12582912
bert/encoder/layer_7/output/dropout/Cast
1608802723865126	12582912
1608802724075371	-12582912
bert/encoder/layer_8/attention/output/dropout/Cast
1608802723865180	12582912
1608802724061498	-12582912
bert/embeddings/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add_1
global_step/cond/Merge
PolynomialDecay/sub_1
gradients/bert/embeddings/dropout/mul_1_grad/Mul
bert/encoder/layer_11/attention/self/dropout/mul
1608802723865456	25165824
1608802723999719	-25165824
bert/encoder/layer_4/attention/self/dropout/mul
1608802723865514	25165824
1608802724152217	-25165824
bert/encoder/layer_7/attention/self/dropout/mul
1608802723865572	25165824
1608802724085725	-25165824
bert/encoder/layer_9/attention/self/dropout/mul
1608802723865634	25165824
1608802724045710	-25165824
bert/encoder/layer_0/attention/self/dropout/mul
1608802723865686	25165824
1608802724240920	-25165824
bert/encoder/layer_1/attention/self/dropout/mul
1608802723865743	25165824
1608802724218733	-25165824
bert/encoder/layer_2/attention/self/dropout/mul
1608802723865801	25165824
1608802724196579	-25165824
bert/encoder/layer_10/attention/self/dropout/mul
1608802723865858	25165824
1608802724021862	-25165824
bert/encoder/layer_5/attention/self/dropout/mul
1608802723865913	25165824
1608802724130028	-25165824
bert/encoder/layer_8/attention/self/dropout/mul
1608802723865970	25165824
1608802724063570	-25165824
bert/encoder/layer_6/attention/self/dropout/mul
1608802723866028	25165824
1608802724107889	-25165824
bert/encoder/layer_3/attention/self/dropout/mul
1608802723866080	25165824
1608802724174405	-25165824
bert/encoder/layer_0/output/dropout/mul
1608802723866136	12582912
1608802723872125	-12582912
bert/encoder/layer_6/attention/output/dropout/mul
1608802723866192	12582912
1608802723896141	-12582912
bert/encoder/layer_8/output/dropout/mul
1608802723866246	12582912
1608802723917860	-12582912
bert/encoder/layer_4/attention/output/dropout/mul
1608802723866295	12582912
1608802723888996	-12582912
bert/encoder/layer_9/output/dropout/mul
1608802723866353	12582912
1608802723931310	-12582912
bert/encoder/layer_2/attention/output/dropout/mul
1608802723866411	12582912
1608802723878343	-12582912
bert/encoder/layer_4/output/dropout/mul
1608802723866463	12582912
1608802723890466	-12582912
bert/encoder/layer_10/attention/output/dropout/mul
1608802723866520	12582912
1608802723939429	-12582912
bert/encoder/layer_10/output/dropout/mul
1608802723866576	12582912
1608802723944738	-12582912
bert/encoder/layer_1/attention/output/dropout/mul
1608802723866625	12582912
1608802723874457	-12582912
bert/encoder/layer_5/attention/output/dropout/mul
1608802723866683	12582912
1608802723892639	-12582912
bert/encoder/layer_9/attention/output/dropout/mul
1608802723866738	12582912
1608802723925990	-12582912
bert/encoder/layer_11/attention/output/dropout/mul
1608802723866788	12582912
1608802723952855	-12582912
bert/encoder/layer_1/output/dropout/mul
1608802723866844	12582912
1608802723875925	-12582912
bert/encoder/layer_3/attention/output/dropout/mul
1608802723866899	12582912
1608802723882046	-12582912
bert/encoder/layer_6/output/dropout/mul
1608802723866956	12582912
1608802723897540	-12582912
bert/encoder/layer_11/output/dropout/mul
1608802723867006	12582912
1608802723958170	-12582912
bert/encoder/layer_2/output/dropout/mul
1608802723867062	12582912
1608802723879775	-12582912
bert/encoder/layer_3/output/dropout/mul
1608802723867117	12582912
1608802723883481	-12582912
bert/encoder/layer_7/attention/output/dropout/mul
1608802723867166	12582912
1608802723899588	-12582912
bert/encoder/layer_0/attention/output/dropout/mul
1608802723867219	12582912
1608802723870675	-12582912
bert/encoder/layer_5/output/dropout/mul
1608802723867275	12582912
1608802723894000	-12582912
bert/encoder/layer_7/output/dropout/mul
1608802723867327	12582912
1608802723904438	-12582912
bert/encoder/layer_8/attention/output/dropout/mul
1608802723867380	12582912
1608802723912557	-12582912
bert/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_1
global_step/add
1608802723867505	256
1608802723868167	-256
PolynomialDecay/Mul
bert/embeddings/LayerNorm/moments/mean
1608802723867671	16384
1608802724251458	-16384
PolynomialDecay
bert/embeddings/LayerNorm/moments/SquaredDifference
1608802723867876	12582912
1608802723868056	-12582912
mul_2
bert/embeddings/LayerNorm/moments/variance
1608802723868010	16384
1608802724250927	-16384
add_2
bert/embeddings/LayerNorm/batchnorm/add
bert/embeddings/LayerNorm/batchnorm/Rsqrt
bert/embeddings/LayerNorm/batchnorm/mul
1608802723868351	12582912
1608802724247675	-12582912
bert/embeddings/LayerNorm/batchnorm/mul_1
1608802723868542	12582912
1608802724245068	-12582912
bert/embeddings/LayerNorm/batchnorm/mul_2
1608802723868687	12582912
1608802723868866	-12582912
bert/embeddings/LayerNorm/batchnorm/sub
bert/embeddings/LayerNorm/batchnorm/add_1
bert/embeddings/dropout/mul_1
bert/encoder/Reshape_1
bert/encoder/layer_0/attention/self/query/MatMul
1608802723868951	12582912
1608802723869375	-12582912
bert/encoder/layer_0/attention/self/key/MatMul
1608802723869033	12582912
1608802723869449	-12582912
bert/encoder/layer_0/attention/self/value/MatMul
1608802723869090	12582912
1608802723869512	-12582912
bert/encoder/layer_0/attention/self/query/BiasAdd
bert/encoder/layer_0/attention/self/key/BiasAdd
bert/encoder/layer_0/attention/self/value/BiasAdd
bert/encoder/layer_0/attention/self/Reshape
bert/encoder/layer_0/attention/self/Reshape_1
bert/encoder/layer_0/attention/self/Reshape_2
bert/encoder/layer_0/attention/self/transpose
1608802723869325	12582912
1608802724244715	-12582912
bert/encoder/layer_0/attention/self/transpose_1
1608802723869396	12582912
1608802724243414	-12582912
bert/encoder/layer_0/attention/self/transpose_2
1608802723869469	12582912
1608802724240205	-12582912
bert/encoder/layer_0/attention/self/MatMul
1608802723869545	25165824
1608802723869651	3072
1608802723869655	3072
1608802723869657	3072
1608802723869818	-3072
1608802723869820	-3072
1608802723869822	-3072
1608802724241402	-25165824
bert/encoder/layer_0/attention/self/Mul
bert/encoder/layer_0/attention/self/add
bert/encoder/layer_0/attention/self/Softmax
1608802723869970	25165824
1608802723869972	25165824
1608802723870062	-25165824
1608802723870064	-25165824
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul_1
1608802723870093	25165824
1608802724244713	-25165824
bert/encoder/layer_0/attention/self/dropout/mul_1
bert/encoder/layer_0/attention/self/MatMul_1
1608802723870194	12582912
1608802723870280	3072
1608802723870283	3072
1608802723870285	3072
1608802723870415	-3072
1608802723870417	-3072
1608802723870419	-3072
1608802723870499	-12582912
bert/encoder/layer_0/attention/self/transpose_3
1608802723870456	12582912
1608802724238778	-12582912
bert/encoder/layer_0/attention/self/Reshape_3
bert/encoder/layer_0/attention/output/dense/MatMul
1608802723870532	12582912
1608802724238526	-12582912
bert/encoder/layer_0/attention/output/dense/BiasAdd
bert/encoder/layer_0/attention/output/dropout/mul_1
bert/encoder/layer_0/attention/output/add
bert/encoder/layer_0/attention/output/LayerNorm/moments/mean
1608802723870733	16384
1608802724238395	-16384
bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference
1608802723870789	12582912
1608802723870882	-12582912
bert/encoder/layer_0/attention/output/LayerNorm/moments/variance
1608802723870845	16384
1608802724238320	-16384
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul
1608802723870993	12582912
1608802724236511	-12582912
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2
1608802723871049	12582912
1608802723871239	-12582912
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1
1608802723871100	12582912
1608802724235992	-12582912
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_0/intermediate/dense/MatMul
1608802723871256	50331648
1608802724230544	-50331648
bert/encoder/layer_0/intermediate/dense/BiasAdd
bert/encoder/layer_0/intermediate/dense/Pow
1608802723871383	50331648
1608802724235831	-50331648
gradients/bert/encoder/layer_0/intermediate/dense/mul_3_grad/Mul_1
1608802723871450	50331648
1608802724235662	-50331648
gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/Pow
1608802723871504	50331648
1608802724235994	-50331648
bert/encoder/layer_0/intermediate/dense/mul
gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/mul
bert/encoder/layer_0/intermediate/dense/add
bert/encoder/layer_0/intermediate/dense/mul_1
bert/encoder/layer_0/intermediate/dense/Tanh
bert/encoder/layer_0/intermediate/dense/add_1
1608802723871825	50331648
1608802724235833	-50331648
bert/encoder/layer_0/intermediate/dense/mul_2
bert/encoder/layer_0/intermediate/dense/mul_3
bert/encoder/layer_0/output/dense/MatMul
1608802723871977	12582912
1608802724229292	-12582912
bert/encoder/layer_0/output/dense/BiasAdd
bert/encoder/layer_0/output/dropout/mul_1
bert/encoder/layer_0/output/add
bert/encoder/layer_0/output/LayerNorm/moments/mean
1608802723872192	16384
1608802724228975	-16384
bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference
1608802723872249	12582912
1608802723872349	-12582912
bert/encoder/layer_0/output/LayerNorm/moments/variance
1608802723872304	16384
1608802724228689	-16384
bert/encoder/layer_0/output/LayerNorm/batchnorm/add
bert/encoder/layer_0/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_0/output/LayerNorm/batchnorm/mul
1608802723872486	12582912
1608802724223514	-12582912
bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2
1608802723872546	12582912
1608802723872757	-12582912
bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1
1608802723872608	12582912
1608802724222903	-12582912
bert/encoder/layer_0/output/LayerNorm/batchnorm/sub
bert/encoder/layer_0/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_1/attention/self/query/MatMul
1608802723872773	12582912
1608802723873203	-12582912
bert/encoder/layer_1/attention/self/key/MatMul
1608802723872845	12582912
1608802723873260	-12582912
bert/encoder/layer_1/attention/self/value/MatMul
1608802723872903	12582912
1608802723873321	-12582912
bert/encoder/layer_1/attention/self/query/BiasAdd
bert/encoder/layer_1/attention/self/key/BiasAdd
bert/encoder/layer_1/attention/self/value/BiasAdd
bert/encoder/layer_1/attention/self/Reshape
bert/encoder/layer_1/attention/self/Reshape_1
bert/encoder/layer_1/attention/self/Reshape_2
bert/encoder/layer_1/attention/self/transpose
1608802723873157	12582912
1608802724222520	-12582912
bert/encoder/layer_1/attention/self/transpose_1
1608802723873223	12582912
1608802724221222	-12582912
bert/encoder/layer_1/attention/self/transpose_2
1608802723873285	12582912
1608802724218016	-12582912
bert/encoder/layer_1/attention/self/MatMul
1608802723873350	25165824
1608802723873456	3072
1608802723873460	3072
1608802723873462	3072
1608802723873591	-3072
1608802723873593	-3072
1608802723873595	-3072
1608802724219211	-25165824
bert/encoder/layer_1/attention/self/Mul
bert/encoder/layer_1/attention/self/add
bert/encoder/layer_1/attention/self/Softmax
1608802723873750	25165824
1608802723873752	25165824
1608802723873824	-25165824
1608802723873827	-25165824
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul_1
1608802723873856	25165824
1608802724222519	-25165824
bert/encoder/layer_1/attention/self/dropout/mul_1
bert/encoder/layer_1/attention/self/MatMul_1
1608802723873982	12582912
1608802723874069	3072
1608802723874072	3072
1608802723874073	3072
1608802723874193	-3072
1608802723874194	-3072
1608802723874196	-3072
1608802723874277	-12582912
bert/encoder/layer_1/attention/self/transpose_3
1608802723874232	12582912
1608802724216585	-12582912
bert/encoder/layer_1/attention/self/Reshape_3
bert/encoder/layer_1/attention/output/dense/MatMul
1608802723874316	12582912
1608802724216334	-12582912
bert/encoder/layer_1/attention/output/dense/BiasAdd
bert/encoder/layer_1/attention/output/dropout/mul_1
bert/encoder/layer_1/attention/output/add
bert/encoder/layer_1/attention/output/LayerNorm/moments/mean
1608802723874519	16384
1608802724216266	-16384
bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference
1608802723874576	12582912
1608802723874670	-12582912
bert/encoder/layer_1/attention/output/LayerNorm/moments/variance
1608802723874632	16384
1608802724216121	-16384
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul
1608802723874772	12582912
1608802724214366	-12582912
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2
1608802723874830	12582912
1608802723875027	-12582912
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1
1608802723874888	12582912
1608802724213840	-12582912
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_1/intermediate/dense/MatMul
1608802723875046	50331648
1608802724208388	-50331648
bert/encoder/layer_1/intermediate/dense/BiasAdd
bert/encoder/layer_1/intermediate/dense/Pow
1608802723875178	50331648
1608802724213679	-50331648
gradients/bert/encoder/layer_1/intermediate/dense/mul_3_grad/Mul_1
1608802723875246	50331648
1608802724213512	-50331648
gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/Pow
1608802723875305	50331648
1608802724213843	-50331648
bert/encoder/layer_1/intermediate/dense/mul
gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/mul
bert/encoder/layer_1/intermediate/dense/add
bert/encoder/layer_1/intermediate/dense/mul_1
bert/encoder/layer_1/intermediate/dense/Tanh
bert/encoder/layer_1/intermediate/dense/add_1
1608802723875613	50331648
1608802724213682	-50331648
bert/encoder/layer_1/intermediate/dense/mul_2
bert/encoder/layer_1/intermediate/dense/mul_3
bert/encoder/layer_1/output/dense/MatMul
1608802723875771	12582912
1608802724207153	-12582912
bert/encoder/layer_1/output/dense/BiasAdd
bert/encoder/layer_1/output/dropout/mul_1
bert/encoder/layer_1/output/add
bert/encoder/layer_1/output/LayerNorm/moments/mean
1608802723875988	16384
1608802724206831	-16384
bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference
1608802723876048	12582912
1608802723876154	-12582912
bert/encoder/layer_1/output/LayerNorm/moments/variance
1608802723876116	18688
1608802724206548	-18688
bert/encoder/layer_1/output/LayerNorm/batchnorm/add
bert/encoder/layer_1/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_1/output/LayerNorm/batchnorm/mul
1608802723876270	12582912
1608802724201376	-12582912
bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2
1608802723876333	12582912
1608802723876558	-12582912
bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1
1608802723876406	12582912
1608802724200792	-12582912
bert/encoder/layer_1/output/LayerNorm/batchnorm/sub
bert/encoder/layer_1/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_2/attention/self/query/MatMul
1608802723876572	12582912
1608802723877006	-12582912
bert/encoder/layer_2/attention/self/key/MatMul
1608802723876641	12582912
1608802723877072	-12582912
bert/encoder/layer_2/attention/self/value/MatMul
1608802723876707	12582912
1608802723877136	-12582912
bert/encoder/layer_2/attention/self/query/BiasAdd
bert/encoder/layer_2/attention/self/key/BiasAdd
bert/encoder/layer_2/attention/self/value/BiasAdd
bert/encoder/layer_2/attention/self/Reshape
bert/encoder/layer_2/attention/self/Reshape_1
bert/encoder/layer_2/attention/self/Reshape_2
bert/encoder/layer_2/attention/self/transpose
1608802723876965	12582912
1608802724200389	-12582912
bert/encoder/layer_2/attention/self/transpose_1
1608802723877028	12582912
1608802724199077	-12582912
bert/encoder/layer_2/attention/self/transpose_2
1608802723877093	12582912
1608802724195855	-12582912
bert/encoder/layer_2/attention/self/MatMul
1608802723877162	25165824
1608802723877262	3072
1608802723877266	3072
1608802723877269	3072
1608802723877396	-3072
1608802723877398	-3072
1608802723877400	-3072
1608802724197057	-25165824
bert/encoder/layer_2/attention/self/Mul
bert/encoder/layer_2/attention/self/add
bert/encoder/layer_2/attention/self/Softmax
1608802723877650	25165824
1608802723877652	25165824
1608802723877728	-25165824
1608802723877730	-25165824
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul_1
1608802723877758	25165824
1608802724200387	-25165824
bert/encoder/layer_2/attention/self/dropout/mul_1
bert/encoder/layer_2/attention/self/MatMul_1
1608802723877865	12582912
1608802723877955	3072
1608802723877958	3072
1608802723877960	3072
1608802723878078	-3072
1608802723878081	-3072
1608802723878082	-3072
1608802723878160	-12582912
bert/encoder/layer_2/attention/self/transpose_3
1608802723878117	12582912
1608802724194443	-12582912
bert/encoder/layer_2/attention/self/Reshape_3
bert/encoder/layer_2/attention/output/dense/MatMul
1608802723878196	12582912
1608802724194162	-12582912
bert/encoder/layer_2/attention/output/dense/BiasAdd
bert/encoder/layer_2/attention/output/dropout/mul_1
bert/encoder/layer_2/attention/output/add
bert/encoder/layer_2/attention/output/LayerNorm/moments/mean
1608802723878405	16384
1608802724194044	-16384
bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference
1608802723878456	12582912
1608802723878545	-12582912
bert/encoder/layer_2/attention/output/LayerNorm/moments/variance
1608802723878509	16384
1608802724193954	-16384
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul
1608802723878647	12582912
1608802724192202	-12582912
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2
1608802723878701	12582912
1608802723878893	-12582912
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1
1608802723878752	12582912
1608802724191675	-12582912
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_2/intermediate/dense/MatMul
1608802723878914	50331648
1608802724186223	-50331648
bert/encoder/layer_2/intermediate/dense/BiasAdd
bert/encoder/layer_2/intermediate/dense/Pow
1608802723879033	50331648
1608802724191512	-50331648
gradients/bert/encoder/layer_2/intermediate/dense/mul_3_grad/Mul_1
1608802723879095	50331648
1608802724191350	-50331648
gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/Pow
1608802723879155	50331648
1608802724191678	-50331648
bert/encoder/layer_2/intermediate/dense/mul
gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/mul
bert/encoder/layer_2/intermediate/dense/add
bert/encoder/layer_2/intermediate/dense/mul_1
bert/encoder/layer_2/intermediate/dense/Tanh
bert/encoder/layer_2/intermediate/dense/add_1
1608802723879460	50331648
1608802724191515	-50331648
bert/encoder/layer_2/intermediate/dense/mul_2
bert/encoder/layer_2/intermediate/dense/mul_3
bert/encoder/layer_2/output/dense/MatMul
1608802723879627	12582912
1608802724184984	-12582912
bert/encoder/layer_2/output/dense/BiasAdd
bert/encoder/layer_2/output/dropout/mul_1
bert/encoder/layer_2/output/add
bert/encoder/layer_2/output/LayerNorm/moments/mean
1608802723879841	16384
1608802724184663	-16384
bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference
1608802723879895	12582912
1608802723879996	-12582912
bert/encoder/layer_2/output/LayerNorm/moments/variance
1608802723879959	16384
1608802724184377	-16384
bert/encoder/layer_2/output/LayerNorm/batchnorm/add
bert/encoder/layer_2/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_2/output/LayerNorm/batchnorm/mul
1608802723880114	12582912
1608802724179226	-12582912
bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2
1608802723880173	12582912
1608802723880388	-12582912
bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1
1608802723880234	12582912
1608802724178634	-12582912
bert/encoder/layer_2/output/LayerNorm/batchnorm/sub
bert/encoder/layer_2/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_3/attention/self/query/MatMul
1608802723880411	12582912
1608802723880816	-12582912
bert/encoder/layer_3/attention/self/key/MatMul
1608802723880478	12582912
1608802723880880	-12582912
bert/encoder/layer_3/attention/self/value/MatMul
1608802723880541	12582912
1608802723880942	-12582912
bert/encoder/layer_3/attention/self/query/BiasAdd
bert/encoder/layer_3/attention/self/key/BiasAdd
bert/encoder/layer_3/attention/self/value/BiasAdd
bert/encoder/layer_3/attention/self/Reshape
bert/encoder/layer_3/attention/self/Reshape_1
bert/encoder/layer_3/attention/self/Reshape_2
bert/encoder/layer_3/attention/self/transpose
1608802723880777	12582912
1608802724178204	-12582912
bert/encoder/layer_3/attention/self/transpose_1
1608802723880842	12582912
1608802724176899	-12582912
bert/encoder/layer_3/attention/self/transpose_2
1608802723880898	12582912
1608802724173639	-12582912
bert/encoder/layer_3/attention/self/MatMul
1608802723880967	25165824
1608802723881064	3072
1608802723881068	3072
1608802723881070	3072
1608802723881206	-3072
1608802723881208	-3072
1608802723881210	-3072
1608802724174880	-25165824
bert/encoder/layer_3/attention/self/Mul
bert/encoder/layer_3/attention/self/add
bert/encoder/layer_3/attention/self/Softmax
1608802723881352	25165824
1608802723881354	25165824
1608802723881426	-25165824
1608802723881429	-25165824
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul_1
1608802723881456	25165824
1608802724178203	-25165824
bert/encoder/layer_3/attention/self/dropout/mul_1
bert/encoder/layer_3/attention/self/MatMul_1
1608802723881567	12582912
1608802723881657	3072
1608802723881660	3072
1608802723881661	3072
1608802723881780	-3072
1608802723881782	-3072
1608802723881784	-3072
1608802723881864	-12582912
bert/encoder/layer_3/attention/self/transpose_3
1608802723881822	12582912
1608802724172242	-12582912
bert/encoder/layer_3/attention/self/Reshape_3
bert/encoder/layer_3/attention/output/dense/MatMul
1608802723881902	12582912
1608802724171941	-12582912
bert/encoder/layer_3/attention/output/dense/BiasAdd
bert/encoder/layer_3/attention/output/dropout/mul_1
bert/encoder/layer_3/attention/output/add
bert/encoder/layer_3/attention/output/LayerNorm/moments/mean
1608802723882107	16384
1608802724171846	-16384
bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference
1608802723882160	12582912
1608802723882249	-12582912
bert/encoder/layer_3/attention/output/LayerNorm/moments/variance
1608802723882213	16384
1608802724171739	-16384
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul
1608802723882356	12582912
1608802724169988	-12582912
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2
1608802723882410	12582912
1608802723882601	-12582912
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1
1608802723882461	12582912
1608802724169450	-12582912
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_3/intermediate/dense/MatMul
1608802723882620	50331648
1608802724164001	-50331648
bert/encoder/layer_3/intermediate/dense/BiasAdd
bert/encoder/layer_3/intermediate/dense/Pow
1608802723882743	50331648
1608802724169283	-50331648
gradients/bert/encoder/layer_3/intermediate/dense/mul_3_grad/Mul_1
1608802723882806	50331648
1608802724169135	-50331648
gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/Pow
1608802723882866	50331648
1608802724169453	-50331648
bert/encoder/layer_3/intermediate/dense/mul
gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/mul
bert/encoder/layer_3/intermediate/dense/add
bert/encoder/layer_3/intermediate/dense/mul_1
bert/encoder/layer_3/intermediate/dense/Tanh
bert/encoder/layer_3/intermediate/dense/add_1
1608802723883173	50331648
1608802724169287	-50331648
bert/encoder/layer_3/intermediate/dense/mul_2
bert/encoder/layer_3/intermediate/dense/mul_3
bert/encoder/layer_3/output/dense/MatMul
1608802723883331	12582912
1608802724162761	-12582912
bert/encoder/layer_3/output/dense/BiasAdd
bert/encoder/layer_3/output/dropout/mul_1
bert/encoder/layer_3/output/add
bert/encoder/layer_3/output/LayerNorm/moments/mean
1608802723883554	16384
1608802724162438	-16384
bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference
1608802723883608	12582912
1608802723883706	-12582912
bert/encoder/layer_3/output/LayerNorm/moments/variance
1608802723883669	16384
1608802724162151	-16384
bert/encoder/layer_3/output/LayerNorm/batchnorm/add
bert/encoder/layer_3/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_3/output/LayerNorm/batchnorm/mul
1608802723883821	12582912
1608802724157125	-12582912
bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2
1608802723883880	12582912
1608802723884089	-12582912
bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1
1608802723883944	12582912
1608802724156472	-12582912
bert/encoder/layer_3/output/LayerNorm/batchnorm/sub
bert/encoder/layer_3/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_4/attention/self/query/MatMul
1608802723884105	12582912
1608802723884566	-12582912
bert/encoder/layer_4/attention/self/key/MatMul
1608802723884177	12582912
1608802723884634	-12582912
bert/encoder/layer_4/attention/self/value/MatMul
1608802723884259	12582912
1608802723884709	-12582912
bert/encoder/layer_4/attention/self/query/BiasAdd
bert/encoder/layer_4/attention/self/key/BiasAdd
bert/encoder/layer_4/attention/self/value/BiasAdd
bert/encoder/layer_4/attention/self/Reshape
bert/encoder/layer_4/attention/self/Reshape_1
bert/encoder/layer_4/attention/self/Reshape_2
bert/encoder/layer_4/attention/self/transpose
1608802723884526	12582912
1608802724155988	-12582912
bert/encoder/layer_4/attention/self/transpose_1
1608802723884595	12582912
1608802724154681	-12582912
bert/encoder/layer_4/attention/self/transpose_2
1608802723884656	12582912
1608802724151490	-12582912
bert/encoder/layer_4/attention/self/MatMul
1608802723884735	25165824
1608802723884838	3072
1608802723884841	3072
1608802723884843	3072
1608802723884964	-3072
1608802723884965	-3072
1608802723884967	-3072
1608802724152740	-25165824
bert/encoder/layer_4/attention/self/Mul
bert/encoder/layer_4/attention/self/add
bert/encoder/layer_4/attention/self/Softmax
1608802723885104	25165824
1608802723885106	25165824
1608802723885176	-25165824
1608802723885179	-25165824
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul_1
1608802723885204	25165824
1608802724155986	-25165824
bert/encoder/layer_4/attention/self/dropout/mul_1
bert/encoder/layer_4/attention/self/MatMul_1
1608802723885298	12582912
1608802723885377	3072
1608802723885379	3072
1608802723885381	3072
1608802723885494	-3072
1608802723885496	-3072
1608802723885498	-3072
1608802723885572	-12582912
bert/encoder/layer_4/attention/self/transpose_3
1608802723885532	12582912
1608802724150153	-12582912
bert/encoder/layer_4/attention/self/Reshape_3
bert/encoder/layer_4/attention/output/dense/MatMul
1608802723885604	12582912
1608802724149805	-12582912
bert/encoder/layer_4/attention/output/dense/BiasAdd
bert/encoder/layer_4/attention/output/dropout/mul_1
bert/encoder/layer_4/attention/output/add
bert/encoder/layer_4/attention/output/LayerNorm/moments/mean
1608802723889082	16384
1608802724149708	-16384
bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference
1608802723889143	12582912
1608802723889238	-12582912
bert/encoder/layer_4/attention/output/LayerNorm/moments/variance
1608802723889202	16384
1608802724149600	-16384
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul
1608802723889349	12582912
1608802724147923	-12582912
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2
1608802723889405	12582912
1608802723889609	-12582912
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1
1608802723889465	12582912
1608802724147334	-12582912
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_4/intermediate/dense/MatMul
1608802723889628	50331648
1608802724141863	-50331648
bert/encoder/layer_4/intermediate/dense/BiasAdd
bert/encoder/layer_4/intermediate/dense/Pow
1608802723889768	50331648
1608802724147151	-50331648
gradients/bert/encoder/layer_4/intermediate/dense/mul_3_grad/Mul_1
1608802723889830	50331648
1608802724147012	-50331648
gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/Pow
1608802723889888	50331648
1608802724147336	-50331648
bert/encoder/layer_4/intermediate/dense/mul
gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/mul
bert/encoder/layer_4/intermediate/dense/add
bert/encoder/layer_4/intermediate/dense/mul_1
bert/encoder/layer_4/intermediate/dense/Tanh
bert/encoder/layer_4/intermediate/dense/add_1
1608802723890175	50331648
1608802724147154	-50331648
bert/encoder/layer_4/intermediate/dense/mul_2
bert/encoder/layer_4/intermediate/dense/mul_3
bert/encoder/layer_4/output/dense/MatMul
1608802723890322	12582912
1608802724140642	-12582912
bert/encoder/layer_4/output/dense/BiasAdd
bert/encoder/layer_4/output/dropout/mul_1
bert/encoder/layer_4/output/add
bert/encoder/layer_4/output/LayerNorm/moments/mean
1608802723890535	16384
1608802724140306	-16384
bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference
1608802723890585	12582912
1608802723890681	-12582912
bert/encoder/layer_4/output/LayerNorm/moments/variance
1608802723890646	16384
1608802724140018	-16384
bert/encoder/layer_4/output/LayerNorm/batchnorm/add
bert/encoder/layer_4/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_4/output/LayerNorm/batchnorm/mul
1608802723890794	12582912
1608802724135098	-12582912
bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2
1608802723890851	12582912
1608802723891041	-12582912
bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1
1608802723890909	12582912
1608802724134399	-12582912
bert/encoder/layer_4/output/LayerNorm/batchnorm/sub
bert/encoder/layer_4/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_5/attention/self/query/MatMul
1608802723891061	12582912
1608802723891450	-12582912
bert/encoder/layer_5/attention/self/key/MatMul
1608802723891121	12582912
1608802723891514	-12582912
bert/encoder/layer_5/attention/self/value/MatMul
1608802723891183	12582912
1608802723891573	-12582912
bert/encoder/layer_5/attention/self/query/BiasAdd
bert/encoder/layer_5/attention/self/key/BiasAdd
bert/encoder/layer_5/attention/self/value/BiasAdd
bert/encoder/layer_5/attention/self/Reshape
bert/encoder/layer_5/attention/self/Reshape_1
bert/encoder/layer_5/attention/self/Reshape_2
bert/encoder/layer_5/attention/self/transpose
1608802723891412	12582912
1608802724133866	-12582912
bert/encoder/layer_5/attention/self/transpose_1
1608802723891478	12582912
1608802724132507	-12582912
bert/encoder/layer_5/attention/self/transpose_2
1608802723891532	12582912
1608802724129299	-12582912
bert/encoder/layer_5/attention/self/MatMul
1608802723891597	25165824
1608802723891700	3072
1608802723891703	3072
1608802723891705	3072
1608802723891833	-3072
1608802723891835	-3072
1608802723891837	-3072
1608802724130528	-25165824
bert/encoder/layer_5/attention/self/Mul
bert/encoder/layer_5/attention/self/add
bert/encoder/layer_5/attention/self/Softmax
1608802723891975	25165824
1608802723891977	25165824
1608802723892047	-25165824
1608802723892049	-25165824
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul_1
1608802723892075	25165824
1608802724133864	-25165824
bert/encoder/layer_5/attention/self/dropout/mul_1
bert/encoder/layer_5/attention/self/MatMul_1
1608802723892169	12582912
1608802723892254	3072
1608802723892257	3072
1608802723892258	3072
1608802723892390	-3072
1608802723892392	-3072
1608802723892394	-3072
1608802723892474	-12582912
bert/encoder/layer_5/attention/self/transpose_3
1608802723892430	12582912
1608802724127982	-12582912
bert/encoder/layer_5/attention/self/Reshape_3
bert/encoder/layer_5/attention/output/dense/MatMul
1608802723892506	12582912
1608802724127619	-12582912
bert/encoder/layer_5/attention/output/dense/BiasAdd
bert/encoder/layer_5/attention/output/dropout/mul_1
bert/encoder/layer_5/attention/output/add
bert/encoder/layer_5/attention/output/LayerNorm/moments/mean
1608802723892696	16384
1608802724127523	-16384
bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference
1608802723892745	12582912
1608802723892830	-12582912
bert/encoder/layer_5/attention/output/LayerNorm/moments/variance
1608802723892795	16384
1608802724127409	-16384
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul
1608802723892929	12582912
1608802724125762	-12582912
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2
1608802723892981	12582912
1608802723893149	-12582912
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1
1608802723893030	12582912
1608802724125169	-12582912
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_5/intermediate/dense/MatMul
1608802723893173	50331648
1608802724119688	-50331648
bert/encoder/layer_5/intermediate/dense/BiasAdd
bert/encoder/layer_5/intermediate/dense/Pow
1608802723893290	50331648
1608802724124980	-50331648
gradients/bert/encoder/layer_5/intermediate/dense/mul_3_grad/Mul_1
1608802723893349	50331648
1608802724124846	-50331648
gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/Pow
1608802723893400	50331648
1608802724125172	-50331648
bert/encoder/layer_5/intermediate/dense/mul
gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/mul
bert/encoder/layer_5/intermediate/dense/add
bert/encoder/layer_5/intermediate/dense/mul_1
bert/encoder/layer_5/intermediate/dense/Tanh
bert/encoder/layer_5/intermediate/dense/add_1
1608802723893733	50331648
1608802724124983	-50331648
bert/encoder/layer_5/intermediate/dense/mul_2
bert/encoder/layer_5/intermediate/dense/mul_3
bert/encoder/layer_5/output/dense/MatMul
1608802723893874	12582912
1608802724118463	-12582912
bert/encoder/layer_5/output/dense/BiasAdd
bert/encoder/layer_5/output/dropout/mul_1
bert/encoder/layer_5/output/add
bert/encoder/layer_5/output/LayerNorm/moments/mean
1608802723894074	16384
1608802724118117	-16384
bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference
1608802723894126	12582912
1608802723894215	-12582912
bert/encoder/layer_5/output/LayerNorm/moments/variance
1608802723894179	16384
1608802724117831	-16384
bert/encoder/layer_5/output/LayerNorm/batchnorm/add
bert/encoder/layer_5/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_5/output/LayerNorm/batchnorm/mul
1608802723894309	12582912
1608802724112992	-12582912
bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2
1608802723894359	12582912
1608802723894571	-12582912
bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1
1608802723894414	12582912
1608802724112219	-12582912
bert/encoder/layer_5/output/LayerNorm/batchnorm/sub
bert/encoder/layer_5/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_6/attention/self/query/MatMul
1608802723894586	12582912
1608802723894987	-12582912
bert/encoder/layer_6/attention/self/key/MatMul
1608802723894653	12582912
1608802723895043	-12582912
bert/encoder/layer_6/attention/self/value/MatMul
1608802723894709	12582912
1608802723895103	-12582912
bert/encoder/layer_6/attention/self/query/BiasAdd
bert/encoder/layer_6/attention/self/key/BiasAdd
bert/encoder/layer_6/attention/self/value/BiasAdd
bert/encoder/layer_6/attention/self/Reshape
bert/encoder/layer_6/attention/self/Reshape_1
bert/encoder/layer_6/attention/self/Reshape_2
bert/encoder/layer_6/attention/self/transpose
1608802723894944	12582912
1608802724111686	-12582912
bert/encoder/layer_6/attention/self/transpose_1
1608802723895007	12582912
1608802724110370	-12582912
bert/encoder/layer_6/attention/self/transpose_2
1608802723895067	12582912
1608802724107161	-12582912
bert/encoder/layer_6/attention/self/MatMul
1608802723895128	25165824
1608802723895222	3072
1608802723895226	3072
1608802723895227	3072
1608802723895350	-3072
1608802723895352	-3072
1608802723895354	-3072
1608802724108441	-25165824
bert/encoder/layer_6/attention/self/Mul
bert/encoder/layer_6/attention/self/add
bert/encoder/layer_6/attention/self/Softmax
1608802723895492	25165824
1608802723895494	25165824
1608802723895564	-25165824
1608802723895566	-25165824
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul_1
1608802723895593	25165824
1608802724111683	-25165824
bert/encoder/layer_6/attention/self/dropout/mul_1
bert/encoder/layer_6/attention/self/MatMul_1
1608802723895692	12582912
1608802723895776	3072
1608802723895779	3072
1608802723895781	3072
1608802723895896	-3072
1608802723895897	-3072
1608802723895899	-3072
1608802723895975	-12582912
bert/encoder/layer_6/attention/self/transpose_3
1608802723895934	12582912
1608802724105842	-12582912
bert/encoder/layer_6/attention/self/Reshape_3
bert/encoder/layer_6/attention/output/dense/MatMul
1608802723896006	12582912
1608802724105474	-12582912
bert/encoder/layer_6/attention/output/dense/BiasAdd
bert/encoder/layer_6/attention/output/dropout/mul_1
bert/encoder/layer_6/attention/output/add
bert/encoder/layer_6/attention/output/LayerNorm/moments/mean
1608802723896204	16384
1608802724105377	-16384
bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference
1608802723896255	12582912
1608802723896341	-12582912
bert/encoder/layer_6/attention/output/LayerNorm/moments/variance
1608802723896306	16384
1608802724105252	-16384
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul
1608802723896469	12582912
1608802724103747	-12582912
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2
1608802723896523	12582912
1608802723896720	-12582912
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1
1608802723896576	12582912
1608802724103086	-12582912
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_6/intermediate/dense/MatMul
1608802723896735	50331648
1608802724097524	-50331648
bert/encoder/layer_6/intermediate/dense/BiasAdd
bert/encoder/layer_6/intermediate/dense/Pow
1608802723896858	50331648
1608802724102868	-50331648
gradients/bert/encoder/layer_6/intermediate/dense/mul_3_grad/Mul_1
1608802723896916	50331648
1608802724102717	-50331648
gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/Pow
1608802723896973	50331648
1608802724103089	-50331648
bert/encoder/layer_6/intermediate/dense/mul
gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/mul
bert/encoder/layer_6/intermediate/dense/add
bert/encoder/layer_6/intermediate/dense/mul_1
bert/encoder/layer_6/intermediate/dense/Tanh
bert/encoder/layer_6/intermediate/dense/add_1
1608802723897261	50331648
1608802724102873	-50331648
bert/encoder/layer_6/intermediate/dense/mul_2
bert/encoder/layer_6/intermediate/dense/mul_3
bert/encoder/layer_6/output/dense/MatMul
1608802723897401	12582912
1608802724096285	-12582912
bert/encoder/layer_6/output/dense/BiasAdd
bert/encoder/layer_6/output/dropout/mul_1
bert/encoder/layer_6/output/add
bert/encoder/layer_6/output/LayerNorm/moments/mean
1608802723897596	16384
1608802724095962	-16384
bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference
1608802723897654	12582912
1608802723897745	-12582912
bert/encoder/layer_6/output/LayerNorm/moments/variance
1608802723897711	16384
1608802724095675	-16384
bert/encoder/layer_6/output/LayerNorm/batchnorm/add
bert/encoder/layer_6/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_6/output/LayerNorm/batchnorm/mul
1608802723897853	12582912
1608802724090630	-12582912
bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2
1608802723897909	12582912
1608802723898091	-12582912
bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1
1608802723897961	12582912
1608802724090152	-12582912
bert/encoder/layer_6/output/LayerNorm/batchnorm/sub
bert/encoder/layer_6/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_7/attention/self/query/MatMul
1608802723898105	12582912
1608802723898484	-12582912
bert/encoder/layer_7/attention/self/key/MatMul
1608802723898170	12582912
1608802723898543	-12582912
bert/encoder/layer_7/attention/self/value/MatMul
1608802723898228	12582912
1608802723898595	-12582912
bert/encoder/layer_7/attention/self/query/BiasAdd
bert/encoder/layer_7/attention/self/key/BiasAdd
bert/encoder/layer_7/attention/self/value/BiasAdd
bert/encoder/layer_7/attention/self/Reshape
bert/encoder/layer_7/attention/self/Reshape_1
bert/encoder/layer_7/attention/self/Reshape_2
bert/encoder/layer_7/attention/self/transpose
1608802723898448	12582912
1608802724089512	-12582912
bert/encoder/layer_7/attention/self/transpose_1
1608802723898503	12582912
1608802724088199	-12582912
bert/encoder/layer_7/attention/self/transpose_2
1608802723898562	12582912
1608802724085004	-12582912
bert/encoder/layer_7/attention/self/MatMul
1608802723898624	25165824
1608802723898723	3072
1608802723898726	3072
1608802723898728	3072
1608802723898845	-3072
1608802723898847	-3072
1608802723898849	-3072
1608802724086203	-25165824
bert/encoder/layer_7/attention/self/Mul
bert/encoder/layer_7/attention/self/add
bert/encoder/layer_7/attention/self/Softmax
1608802723898979	25165824
1608802723898981	25165824
1608802723899046	-25165824
1608802723899048	-25165824
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul_1
1608802723899073	25165824
1608802724089509	-25165824
bert/encoder/layer_7/attention/self/dropout/mul_1
bert/encoder/layer_7/attention/self/MatMul_1
1608802723899164	12582912
1608802723899241	3072
1608802723899243	3072
1608802723899245	3072
1608802723899354	-3072
1608802723899355	-3072
1608802723899357	-3072
1608802723899430	-12582912
bert/encoder/layer_7/attention/self/transpose_3
1608802723899390	12582912
1608802724083637	-12582912
bert/encoder/layer_7/attention/self/Reshape_3
bert/encoder/layer_7/attention/output/dense/MatMul
1608802723899460	12582912
1608802724083308	-12582912
bert/encoder/layer_7/attention/output/dense/BiasAdd
bert/encoder/layer_7/attention/output/dropout/mul_1
bert/encoder/layer_7/attention/output/add
bert/encoder/layer_7/attention/output/LayerNorm/moments/mean
1608802723899644	16384
1608802724083208	-16384
bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference
1608802723899694	12582912
1608802723899778	-12582912
bert/encoder/layer_7/attention/output/LayerNorm/moments/variance
1608802723899745	16384
1608802724083100	-16384
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul
1608802723899869	12582912
1608802724081399	-12582912
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2
1608802723899919	12582912
1608802723900730	-12582912
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1
1608802723899977	12582912
1608802724080842	-12582912
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_7/intermediate/dense/MatMul
1608802723900746	50331648
1608802724075369	-50331648
bert/encoder/layer_7/intermediate/dense/BiasAdd
bert/encoder/layer_7/intermediate/dense/Pow
1608802723900886	50331648
1608802724080669	-50331648
gradients/bert/encoder/layer_7/intermediate/dense/mul_3_grad/Mul_1
1608802723900942	50331648
1608802724080534	-50331648
gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/Pow
1608802723900997	50331648
1608802724080845	-50331648
bert/encoder/layer_7/intermediate/dense/mul
gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/mul
bert/encoder/layer_7/intermediate/dense/add
bert/encoder/layer_7/intermediate/dense/mul_1
bert/encoder/layer_7/intermediate/dense/Tanh
bert/encoder/layer_7/intermediate/dense/add_1
1608802723901276	50331648
1608802724080672	-50331648
bert/encoder/layer_7/intermediate/dense/mul_2
bert/encoder/layer_7/intermediate/dense/mul_3
bert/encoder/layer_7/output/dense/MatMul
1608802723903682	12582912
1608802724074156	-12582912
bert/encoder/layer_7/output/dense/BiasAdd
bert/encoder/layer_7/output/dropout/mul_1
bert/encoder/layer_7/output/add
bert/encoder/layer_7/output/LayerNorm/moments/mean
1608802723904654	16384
1608802724073813	-16384
bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference
1608802723904854	12582912
1608802723905317	-12582912
bert/encoder/layer_7/output/LayerNorm/moments/variance
1608802723905132	16384
1608802724073527	-16384
bert/encoder/layer_7/output/LayerNorm/batchnorm/add
bert/encoder/layer_7/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_7/output/LayerNorm/batchnorm/mul
1608802723905728	12582912
1608802724068604	-12582912
bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2
1608802723905938	12582912
1608802723908805	-12582912
bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1
1608802723906221	12582912
1608802724067891	-12582912
bert/encoder/layer_7/output/LayerNorm/batchnorm/sub
bert/encoder/layer_7/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_8/attention/self/query/MatMul
1608802723908826	12582912
1608802723909229	-12582912
bert/encoder/layer_8/attention/self/key/MatMul
1608802723908903	12582912
1608802723909281	-12582912
bert/encoder/layer_8/attention/self/value/MatMul
1608802723908960	12582912
1608802723909337	-12582912
bert/encoder/layer_8/attention/self/query/BiasAdd
bert/encoder/layer_8/attention/self/key/BiasAdd
bert/encoder/layer_8/attention/self/value/BiasAdd
bert/encoder/layer_8/attention/self/Reshape
bert/encoder/layer_8/attention/self/Reshape_1
bert/encoder/layer_8/attention/self/Reshape_2
bert/encoder/layer_8/attention/self/transpose
1608802723909188	12582912
1608802724067378	-12582912
bert/encoder/layer_8/attention/self/transpose_1
1608802723909247	12582912
1608802724066068	-12582912
bert/encoder/layer_8/attention/self/transpose_2
1608802723909303	12582912
1608802724062844	-12582912
bert/encoder/layer_8/attention/self/MatMul
1608802723909359	25165824
1608802723909441	3072
1608802723909444	3072
1608802723909446	3072
1608802723910599	-3072
1608802723910600	-3072
1608802723910602	-3072
1608802724064066	-25165824
bert/encoder/layer_8/attention/self/Mul
bert/encoder/layer_8/attention/self/add
bert/encoder/layer_8/attention/self/Softmax
1608802723911312	25165824
1608802723911313	25165824
1608802723911605	-25165824
1608802723911607	-25165824
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul_1
1608802723911631	25165824
1608802724067375	-25165824
bert/encoder/layer_8/attention/self/dropout/mul_1
bert/encoder/layer_8/attention/self/MatMul_1
1608802723912011	12582912
1608802723912088	3072
1608802723912090	3072
1608802723912092	3072
1608802723912212	-3072
1608802723912214	-3072
1608802723912216	-3072
1608802723912288	-12582912
bert/encoder/layer_8/attention/self/transpose_3
1608802723912249	12582912
1608802724061496	-12582912
bert/encoder/layer_8/attention/self/Reshape_3
bert/encoder/layer_8/attention/output/dense/MatMul
1608802723912322	12582912
1608802724061151	-12582912
bert/encoder/layer_8/attention/output/dense/BiasAdd
bert/encoder/layer_8/attention/output/dropout/mul_1
bert/encoder/layer_8/attention/output/add
bert/encoder/layer_8/attention/output/LayerNorm/moments/mean
1608802723912727	16384
1608802724061058	-16384
bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference
1608802723912831	12582912
1608802723913011	-12582912
bert/encoder/layer_8/attention/output/LayerNorm/moments/variance
1608802723912977	16384
1608802724060941	-16384
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul
1608802723913104	12582912
1608802724059345	-12582912
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2
1608802723913209	12582912
1608802723914161	-12582912
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1
1608802723913400	12582912
1608802724058702	-12582912
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_8/intermediate/dense/MatMul
1608802723914180	50331648
1608802724053170	-50331648
bert/encoder/layer_8/intermediate/dense/BiasAdd
bert/encoder/layer_8/intermediate/dense/Pow
1608802723914310	50331648
1608802724058502	-50331648
gradients/bert/encoder/layer_8/intermediate/dense/mul_3_grad/Mul_1
1608802723914373	50331648
1608802724058365	-50331648
gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/Pow
1608802723914423	50331648
1608802724058706	-50331648
bert/encoder/layer_8/intermediate/dense/mul
gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/mul
bert/encoder/layer_8/intermediate/dense/add
bert/encoder/layer_8/intermediate/dense/mul_1
bert/encoder/layer_8/intermediate/dense/Tanh
bert/encoder/layer_8/intermediate/dense/add_1
1608802723914706	50331648
1608802724058506	-50331648
bert/encoder/layer_8/intermediate/dense/mul_2
bert/encoder/layer_8/intermediate/dense/mul_3
bert/encoder/layer_8/output/dense/MatMul
1608802723917106	12582912
1608802724051943	-12582912
bert/encoder/layer_8/output/dense/BiasAdd
bert/encoder/layer_8/output/dropout/mul_1
bert/encoder/layer_8/output/add
bert/encoder/layer_8/output/LayerNorm/moments/mean
1608802723918073	16384
1608802724051616	-16384
bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference
1608802723918277	12582912
1608802723918736	-12582912
bert/encoder/layer_8/output/LayerNorm/moments/variance
1608802723918564	16384
1608802724051333	-16384
bert/encoder/layer_8/output/LayerNorm/batchnorm/add
bert/encoder/layer_8/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_8/output/LayerNorm/batchnorm/mul
1608802723919157	12582912
1608802724048249	-12582912
bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2
1608802723919356	12582912
1608802723922238	-12582912
bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1
1608802723919647	12582912
1608802724047485	-12582912
bert/encoder/layer_8/output/LayerNorm/batchnorm/sub
bert/encoder/layer_8/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_9/attention/self/query/MatMul
1608802723922254	12582912
1608802723922630	-12582912
bert/encoder/layer_9/attention/self/key/MatMul
1608802723922334	12582912
1608802723922678	-12582912
bert/encoder/layer_9/attention/self/value/MatMul
1608802723922384	12582912
1608802723922731	-12582912
bert/encoder/layer_9/attention/self/query/BiasAdd
bert/encoder/layer_9/attention/self/key/BiasAdd
bert/encoder/layer_9/attention/self/value/BiasAdd
bert/encoder/layer_9/attention/self/Reshape
bert/encoder/layer_9/attention/self/Reshape_1
bert/encoder/layer_9/attention/self/Reshape_2
bert/encoder/layer_9/attention/self/transpose
1608802723922592	12582912
1608802724046971	-12582912
bert/encoder/layer_9/attention/self/transpose_1
1608802723922647	12582912
1608802724046751	-12582912
bert/encoder/layer_9/attention/self/transpose_2
1608802723922699	12582912
1608802724045476	-12582912
bert/encoder/layer_9/attention/self/MatMul
1608802723922756	25165824
1608802723922835	3072
1608802723922837	3072
1608802723922839	3072
1608802723924030	-3072
1608802723924032	-3072
1608802723924034	-3072
1608802724046273	-25165824
bert/encoder/layer_9/attention/self/Mul
bert/encoder/layer_9/attention/self/add
bert/encoder/layer_9/attention/self/Softmax
1608802723924737	25165824
1608802723924738	25165824
1608802723925035	-25165824
1608802723925037	-25165824
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul_1
1608802723925066	25165824
1608802724046968	-25165824
bert/encoder/layer_9/attention/self/dropout/mul_1
bert/encoder/layer_9/attention/self/MatMul_1
1608802723925438	12582912
1608802723925518	3072
1608802723925520	3072
1608802723925521	3072
1608802723925625	-3072
1608802723925627	-3072
1608802723925628	-3072
1608802723925698	-12582912
bert/encoder/layer_9/attention/self/transpose_3
1608802723925660	12582912
1608802724044887	-12582912
bert/encoder/layer_9/attention/self/Reshape_3
bert/encoder/layer_9/attention/output/dense/MatMul
1608802723925727	12582912
1608802724044430	-12582912
bert/encoder/layer_9/attention/output/dense/BiasAdd
bert/encoder/layer_9/attention/output/dropout/mul_1
bert/encoder/layer_9/attention/output/add
bert/encoder/layer_9/attention/output/LayerNorm/moments/mean
1608802723926158	16384
1608802724044366	-16384
bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference
1608802723926263	12582912
1608802723926440	-12582912
bert/encoder/layer_9/attention/output/LayerNorm/moments/variance
1608802723926408	16384
1608802724037380	-16384
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul
1608802723926528	12582912
1608802724034739	-12582912
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2
1608802723926664	12582912
1608802723927610	-12582912
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1
1608802723926845	12582912
1608802724031689	-12582912
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_9/intermediate/dense/MatMul
1608802723927628	50331648
1608802724031282	-50331648
bert/encoder/layer_9/intermediate/dense/BiasAdd
bert/encoder/layer_9/intermediate/dense/Pow
1608802723927749	50331648
1608802724031560	-50331648
gradients/bert/encoder/layer_9/intermediate/dense/mul_3_grad/Mul_1
1608802723927803	50331648
1608802724031473	-50331648
gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/Pow
1608802723927857	50331648
1608802724031691	-50331648
bert/encoder/layer_9/intermediate/dense/mul
gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/mul
bert/encoder/layer_9/intermediate/dense/add
bert/encoder/layer_9/intermediate/dense/mul_1
bert/encoder/layer_9/intermediate/dense/Tanh
bert/encoder/layer_9/intermediate/dense/add_1
1608802723928115	50331648
1608802724031561	-50331648
bert/encoder/layer_9/intermediate/dense/mul_2
bert/encoder/layer_9/intermediate/dense/mul_3
bert/encoder/layer_9/output/dense/MatMul
1608802723930555	12582912
1608802724031067	-12582912
bert/encoder/layer_9/output/dense/BiasAdd
bert/encoder/layer_9/output/dropout/mul_1
bert/encoder/layer_9/output/add
bert/encoder/layer_9/output/LayerNorm/moments/mean
1608802723931523	16384
1608802724031011	-16384
bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference
1608802723931715	12582912
1608802723932182	-12582912
bert/encoder/layer_9/output/LayerNorm/moments/variance
1608802723932011	16384
1608802724030936	-16384
bert/encoder/layer_9/output/LayerNorm/batchnorm/add
bert/encoder/layer_9/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_9/output/LayerNorm/batchnorm/mul
1608802723932601	12582912
1608802724030591	-12582912
bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2
1608802723932819	12582912
1608802723935685	-12582912
bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1
1608802723933084	12582912
1608802724029999	-12582912
bert/encoder/layer_9/output/LayerNorm/batchnorm/sub
bert/encoder/layer_9/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_10/attention/self/query/MatMul
1608802723935698	12582912
1608802723936085	-12582912
bert/encoder/layer_10/attention/self/key/MatMul
1608802723935779	12582912
1608802723936137	-12582912
bert/encoder/layer_10/attention/self/value/MatMul
1608802723935837	12582912
1608802723936186	-12582912
bert/encoder/layer_10/attention/self/query/BiasAdd
bert/encoder/layer_10/attention/self/key/BiasAdd
bert/encoder/layer_10/attention/self/value/BiasAdd
bert/encoder/layer_10/attention/self/Reshape
bert/encoder/layer_10/attention/self/Reshape_1
bert/encoder/layer_10/attention/self/Reshape_2
bert/encoder/layer_10/attention/self/transpose
1608802723936053	12582912
1608802724029641	-12582912
bert/encoder/layer_10/attention/self/transpose_1
1608802723936107	12582912
1608802724026742	-12582912
bert/encoder/layer_10/attention/self/transpose_2
1608802723936151	12582912
1608802724021726	-12582912
bert/encoder/layer_10/attention/self/MatMul
1608802723936206	25165824
1608802723936295	3072
1608802723936300	3072
1608802723936301	3072
1608802723937476	-3072
1608802723937477	-3072
1608802723937479	-3072
1608802724024864	-25165824
bert/encoder/layer_10/attention/self/Mul
bert/encoder/layer_10/attention/self/add
bert/encoder/layer_10/attention/self/Softmax
1608802723938181	25165824
1608802723938182	25165824
1608802723938478	-25165824
1608802723938480	-25165824
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul_1
1608802723938502	25165824
1608802724029640	-25165824
bert/encoder/layer_10/attention/self/dropout/mul_1
bert/encoder/layer_10/attention/self/MatMul_1
1608802723938881	12582912
1608802723938947	3072
1608802723938949	3072
1608802723938950	3072
1608802723939046	-3072
1608802723939048	-3072
1608802723939049	-3072
1608802723939136	-12582912
bert/encoder/layer_10/attention/self/transpose_3
1608802723939079	12582912
1608802724020512	-12582912
bert/encoder/layer_10/attention/self/Reshape_3
bert/encoder/layer_10/attention/output/dense/MatMul
1608802723939164	12582912
1608802724019961	-12582912
bert/encoder/layer_10/attention/output/dense/BiasAdd
bert/encoder/layer_10/attention/output/dropout/mul_1
bert/encoder/layer_10/attention/output/add
bert/encoder/layer_10/attention/output/LayerNorm/moments/mean
1608802723939597	16384
1608802724019802	-16384
bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference
1608802723939701	12582912
1608802723939873	-12582912
bert/encoder/layer_10/attention/output/LayerNorm/moments/variance
1608802723939843	16384
1608802724019566	-16384
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul
1608802723939954	12582912
1608802724018642	-12582912
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2
1608802723940079	12582912
1608802723941031	-12582912
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1
1608802723940269	12582912
1608802724016337	-12582912
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_10/intermediate/dense/MatMul
1608802723941056	50331648
1608802724012235	-50331648
bert/encoder/layer_10/intermediate/dense/BiasAdd
bert/encoder/layer_10/intermediate/dense/Pow
1608802723941178	50331648
1608802724016211	-50331648
gradients/bert/encoder/layer_10/intermediate/dense/mul_3_grad/Mul_1
1608802723941232	50331648
1608802724013598	-50331648
gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/Pow
1608802723941276	50331648
1608802724016339	-50331648
bert/encoder/layer_10/intermediate/dense/mul
gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/mul
bert/encoder/layer_10/intermediate/dense/add
bert/encoder/layer_10/intermediate/dense/mul_1
bert/encoder/layer_10/intermediate/dense/Tanh
bert/encoder/layer_10/intermediate/dense/add_1
1608802723941518	50331648
1608802724016213	-50331648
bert/encoder/layer_10/intermediate/dense/mul_2
bert/encoder/layer_10/intermediate/dense/mul_3
bert/encoder/layer_10/output/dense/MatMul
1608802723943983	12582912
1608802724010858	-12582912
bert/encoder/layer_10/output/dense/BiasAdd
bert/encoder/layer_10/output/dropout/mul_1
bert/encoder/layer_10/output/add
bert/encoder/layer_10/output/LayerNorm/moments/mean
1608802723944947	16384
1608802724008590	-16384
bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference
1608802723945151	12582912
1608802723945611	-12582912
bert/encoder/layer_10/output/LayerNorm/moments/variance
1608802723945432	16384
1608802724008502	-16384
bert/encoder/layer_10/output/LayerNorm/batchnorm/add
bert/encoder/layer_10/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_10/output/LayerNorm/batchnorm/mul
1608802723946030	12582912
1608802724008010	-12582912
bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2
1608802723946228	12582912
1608802723949117	-12582912
bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1
1608802723946514	12582912
1608802724006113	-12582912
bert/encoder/layer_10/output/LayerNorm/batchnorm/sub
bert/encoder/layer_10/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_11/attention/self/query/MatMul
1608802723949132	12582912
1608802723949480	-12582912
bert/encoder/layer_11/attention/self/key/MatMul
1608802723949209	12582912
1608802723949523	-12582912
bert/encoder/layer_11/attention/self/value/MatMul
1608802723949257	12582912
1608802723949570	-12582912
bert/encoder/layer_11/attention/self/query/BiasAdd
bert/encoder/layer_11/attention/self/key/BiasAdd
bert/encoder/layer_11/attention/self/value/BiasAdd
bert/encoder/layer_11/attention/self/Reshape
bert/encoder/layer_11/attention/self/Reshape_1
bert/encoder/layer_11/attention/self/Reshape_2
bert/encoder/layer_11/attention/self/transpose
1608802723949445	12582912
1608802724005305	-12582912
bert/encoder/layer_11/attention/self/transpose_1
1608802723949495	12582912
1608802724003340	-12582912
bert/encoder/layer_11/attention/self/transpose_2
1608802723949541	12582912
1608802723998831	-12582912
bert/encoder/layer_11/attention/self/MatMul
1608802723949593	25165824
1608802723949652	3072
1608802723949655	3072
1608802723949657	3072
1608802723950901	-3072
1608802723950903	-3072
1608802723950904	-3072
1608802724002974	-25165824
bert/encoder/layer_11/attention/self/Mul
bert/encoder/layer_11/attention/self/add
bert/encoder/layer_11/attention/self/Softmax
1608802723951611	25165824
1608802723951613	25165824
1608802723951906	-25165824
1608802723951908	-25165824
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul_1
1608802723951930	25165824
1608802724005304	-25165824
bert/encoder/layer_11/attention/self/dropout/mul_1
bert/encoder/layer_11/attention/self/MatMul_1
1608802723952308	12582912
1608802723952376	3072
1608802723952378	3072
1608802723952380	3072
1608802723952474	-3072
1608802723952475	-3072
1608802723952476	-3072
1608802723952563	-12582912
bert/encoder/layer_11/attention/self/transpose_3
1608802723952505	12582912
1608802723995216	-12582912
bert/encoder/layer_11/attention/self/Reshape_3
bert/encoder/layer_11/attention/output/dense/MatMul
1608802723952590	12582912
1608802723994958	-12582912
bert/encoder/layer_11/attention/output/dense/BiasAdd
bert/encoder/layer_11/attention/output/dropout/mul_1
bert/encoder/layer_11/attention/output/add
bert/encoder/layer_11/attention/output/LayerNorm/moments/mean
1608802723953023	16384
1608802723994886	-16384
bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference
1608802723953128	12582912
1608802723953299	-12582912
bert/encoder/layer_11/attention/output/LayerNorm/moments/variance
1608802723953271	16384
1608802723994797	-16384
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/add
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul
1608802723953383	12582912
1608802723993379	-12582912
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2
1608802723953518	12582912
1608802723954472	-12582912
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1
1608802723953708	12582912
1608802723992184	-12582912
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub
bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/add_1
bert/encoder/layer_11/intermediate/dense/MatMul
1608802723954488	50331648
1608802723989702	-50331648
bert/encoder/layer_11/intermediate/dense/BiasAdd
bert/encoder/layer_11/intermediate/dense/Pow
1608802723954610	50331648
1608802723991869	-50331648
gradients/bert/encoder/layer_11/intermediate/dense/mul_3_grad/Mul_1
1608802723954660	50331648
1608802723990530	-50331648
gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/Pow
1608802723954705	50331648
1608802723992186	-50331648
bert/encoder/layer_11/intermediate/dense/mul
gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/mul
bert/encoder/layer_11/intermediate/dense/add
bert/encoder/layer_11/intermediate/dense/mul_1
bert/encoder/layer_11/intermediate/dense/Tanh
bert/encoder/layer_11/intermediate/dense/add_1
1608802723954931	50331648
1608802723991872	-50331648
bert/encoder/layer_11/intermediate/dense/mul_2
bert/encoder/layer_11/intermediate/dense/mul_3
bert/encoder/layer_11/output/dense/MatMul
1608802723957414	12582912
1608802723989394	-12582912
bert/encoder/layer_11/output/dense/BiasAdd
bert/encoder/layer_11/output/dropout/mul_1
bert/encoder/layer_11/output/add
bert/encoder/layer_11/output/LayerNorm/moments/mean
1608802723958379	16384
1608802723989255	-16384
bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference
1608802723958574	12582912
1608802723959043	-12582912
bert/encoder/layer_11/output/LayerNorm/moments/variance
1608802723958866	16384
1608802723986493	-16384
bert/encoder/layer_11/output/LayerNorm/batchnorm/add
bert/encoder/layer_11/output/LayerNorm/batchnorm/Rsqrt
bert/encoder/layer_11/output/LayerNorm/batchnorm/mul
1608802723959461	12582912
1608802723984261	-12582912
bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2
1608802723959665	12582912
1608802723962541	-12582912
bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1
1608802723959942	12582912
1608802723962684	-12582912
bert/encoder/layer_11/output/LayerNorm/batchnorm/sub
bert/encoder/layer_11/output/LayerNorm/batchnorm/add_1
bert/encoder/Reshape_13
GatherV2
1608802723962569	1966080
1608802723981299	-1966080
bert/pooler/strided_slice
1608802723962640	98304
1608802723967981	-98304
cls/predictions/transform/dense/MatMul
1608802723962699	1966080
1608802723980187	-1966080
bert/pooler/Squeeze
cls/predictions/transform/dense/BiasAdd
bert/pooler/dense/MatMul
1608802723962855	98304
1608802723967983	-98304
cls/predictions/transform/dense/Pow
1608802723962904	1966080
1608802723980546	-1966080
gradients/cls/predictions/transform/dense/Pow_grad/Pow
1608802723962947	1966080
1608802723981301	-1966080
bert/pooler/dense/BiasAdd
cls/predictions/transform/dense/mul
gradients/cls/predictions/transform/dense/Pow_grad/mul
bert/pooler/dense/Tanh
cls/predictions/transform/dense/add
cls/seq_relationship/MatMul
1608802723963690	256
1608802723965984	-256
cls/predictions/transform/dense/mul_1
cls/seq_relationship/BiasAdd
cls/predictions/transform/dense/Tanh
cls/seq_relationship/LogSoftmax
1608802723965092	256
1608802723965093	256
1608802723965523	-256
1608802723965524	-256
cls/predictions/transform/dense/add_1
1608802723965540	2818048
1608802723980548	-2818048
gradients/cls/seq_relationship/LogSoftmax_grad/Exp
1608802723965738	256
1608802723966176	-256
cls/seq_relationship/mul
cls/predictions/transform/dense/mul_2
gradients/cls/seq_relationship/LogSoftmax_grad/mul
cls/seq_relationship/Sum
1608802723965895	256
1608802723966777	-256
cls/predictions/transform/dense/mul_3
1608802723965997	1966080
1608802723979716	-1966080
gradients/cls/seq_relationship/LogSoftmax_grad/sub
cls/seq_relationship/Neg
cls/predictions/transform/LayerNorm/moments/mean
1608802723966291	2560
1608802723979559	-2560
gradients/cls/seq_relationship/BiasAdd_grad/BiasAddGrad
1608802723966447	256
1608802724333101	-256
gradients/cls/seq_relationship/MatMul_grad/MatMul
1608802723966548	98304
1608802723967111	-98304
gradients/cls/seq_relationship/MatMul_grad/MatMul_1
1608802723966685	6144
1608802724345448	-6144
cls/seq_relationship/Mean
1608802723966746	256
1608802723978461	-256
cls/predictions/transform/LayerNorm/moments/SquaredDifference
1608802723966793	1966080
1608802723967813	-1966080
global_norm/L2Loss_205
1608802723966833	256
1608802724258162	-256
gradients/bert/pooler/dense/Tanh_grad/TanhGrad
global_norm/L2Loss_204
1608802723967128	256
1608802724258161	-256
cls/predictions/transform/LayerNorm/moments/variance
1608802723967757	2560
1608802723979330	-2560
gradients/bert/pooler/dense/BiasAdd_grad/BiasAddGrad
1608802723967826	3072
1608802724342949	-3072
gradients/bert/pooler/dense/MatMul_grad/MatMul
1608802723967880	98304
1608802723968284	-98304
gradients/bert/pooler/dense/MatMul_grad/MatMul_1
1608802723967925	2359296
1608802724345823	-2359296
cls/predictions/transform/LayerNorm/batchnorm/add
global_norm/L2Loss_198
1608802723968041	256
1608802724258157	-256
gradients/bert/pooler/Squeeze_grad/Reshape
global_norm/L2Loss_197
1608802723968088	256
1608802723968106	9216
1608802723968158	-9216
1608802724258156	-256
cls/predictions/transform/LayerNorm/batchnorm/Rsqrt
gradients/bert/pooler/strided_slice_grad/StridedSliceGrad
1608802723968220	12582912
1608802723981575	-12582912
cls/predictions/transform/LayerNorm/batchnorm/mul
1608802723968302	1966080
1608802723977761	-1966080
cls/predictions/transform/LayerNorm/batchnorm/mul_2
1608802723968345	1966080
1608802723970843	-1966080
cls/predictions/transform/LayerNorm/batchnorm/mul_1
1608802723968407	1966080
1608802723976017	-1966080
cls/predictions/transform/LayerNorm/batchnorm/sub
cls/predictions/transform/LayerNorm/batchnorm/add_1
cls/predictions/MatMul
1608802723970856	78136320
1608802723972891	-78136320
cls/predictions/BiasAdd
cls/predictions/LogSoftmax
1608802723971414	78136320
1608802723971417	78136320
1608802723971424	256
1608802723971601	-256
1608802723971609	256
1608802723971794	-256
1608802723971986	-78136320
1608802723971988	-78136320
gradients/cls/predictions/LogSoftmax_grad/Exp
1608802723972013	78136320
1608802723973078	-78136320
cls/predictions/mul
gradients/cls/predictions/LogSoftmax_grad/mul
cls/predictions/Sum
1608802723972695	2560
1608802723972703	256
1608802723972882	-256
1608802723976349	-2560
gradients/cls/predictions/LogSoftmax_grad/sub
cls/predictions/Neg
gradients/cls/predictions/BiasAdd_grad/BiasAddGrad
1608802723973369	122112
1608802724344831	-122112
gradients/cls/predictions/MatMul_grad/MatMul
1608802723975908	1966080
1608802723979158	-1966080
gradients/cls/predictions/MatMul_grad/MatMul_1
1608802723975979	93763584
1608802724257816	-93763584
cls/predictions/mul_1
global_norm/L2Loss_203
1608802723976087	256
1608802723976088	256
1608802723976140	-256
1608802724258160	-256
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_1_grad/Mul
1608802723976159	3810048
1608802723980118	-3810048
gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Neg
1608802723976201	1966080
1608802723978396	-1966080
gradients/cls/predictions/transform/LayerNorm/batchnorm/sub_grad/Sum
1608802723976244	3072
1608802724331986	-3072
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_1_grad/Mul_1
cls/predictions/Sum_1
1608802723976323	256
1608802724044582	-256
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_201
1608802723976449	256
1608802724258159	-256
cls/predictions/truediv
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Sum
1608802723977142	2560
1608802723978787	-2560
gradients/AddN
add_1
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Mul
1608802723978481	1966080
1608802723978970	-1966080
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/cls/predictions/transform/LayerNorm/moments/mean_grad/Tile
1608802723978666	1966080
1608802723979882	-1966080
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum
1608802723978805	3072
1608802723979287	-3072
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Sum_1
1608802723978988	3072
1608802724330700	-3072
gradients/cls/predictions/transform/LayerNorm/moments/mean_grad/truediv
gradients/cls/predictions/transform/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_202
1608802723979229	256
1608802724258160	-256
gradients/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/Tile
1608802723979298	1966080
1608802723979882	-1966080
gradients/cls/predictions/transform/LayerNorm/moments/variance_grad/truediv
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/sub
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/cls/predictions/transform/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_1
gradients/cls/predictions/transform/dense/mul_3_grad/Mul_1
1608802723979894	1966080
1608802723980156	-1966080
gradients/cls/predictions/transform/dense/mul_3_grad/Mul
gradients/cls/predictions/transform/dense/mul_2_grad/Mul_1
gradients/cls/predictions/transform/dense/Tanh_grad/TanhGrad
gradients/cls/predictions/transform/dense/mul_1_grad/Mul_1
gradients/cls/predictions/transform/dense/Pow_grad/mul_1
gradients/AddN_2
gradients/cls/predictions/transform/dense/BiasAdd_grad/BiasAddGrad
1608802723980564	3072
1608802724331427	-3072
gradients/cls/predictions/transform/dense/MatMul_grad/MatMul
1608802723981184	1966080
1608802723981427	-1966080
gradients/cls/predictions/transform/dense/MatMul_grad/MatMul_1
1608802723981255	2818048
1608802724330648	-2818048
global_norm/L2Loss_200
1608802723981320	256
1608802724258158	-256
gradients/Reshape_2_grad/Reshape/tensor
1608802723981355	12582912
1608802723985695	-12582912
global_norm/L2Loss_199
1608802723981442	256
1608802723981454	9216
1608802723981512	-9216
1608802724258158	-256
gradients/Reshape_2_grad/Reshape
gradients/AddN_3
gradients/bert/encoder/Reshape_13_grad/Reshape
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802723981596	14548992
1608802723993947	-14548992
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Neg
1608802723981639	12582912
1608802723984625	-12582912
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/sub_grad/Sum
1608802723981680	3584
1608802724344709	-3584
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_195
1608802723981846	256
1608802724258155	-256
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802723984036	16384
1608802723985215	-16384
gradients/AddN_4
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Mul
1608802723984649	12582912
1608802723985419	-12582912
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_11/output/LayerNorm/moments/mean_grad/Tile
1608802723985029	12582912
1608802723989464	-12582912
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum
1608802723985231	16384
1608802723986303	-16384
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802723985431	3072
1608802724345326	-3072
gradients/bert/encoder/layer_11/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_196
1608802723985914	256
1608802724258155	-256
gradients/bert/encoder/layer_11/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_11/output/LayerNorm/moments/variance_grad/Tile
1608802723986312	12582912
1608802723989463	-12582912
gradients/bert/encoder/layer_11/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_11/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_5
gradients/bert/encoder/layer_11/output/dropout/mul_1_grad/Mul
1608802723989482	12582912
1608802723989558	-12582912
gradients/bert/encoder/layer_11/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_11/output/dense/BiasAdd_grad/BiasAddGrad
1608802723989569	3072
1608802724343820	-3072
gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul
1608802723989619	78136320
1608802723989810	-78136320
gradients/bert/encoder/layer_11/output/dense/MatMul_grad/MatMul_1
1608802723989664	12582912
1608802724345762	-12582912
global_norm/L2Loss_194
1608802723989715	256
1608802724258154	-256
gradients/bert/encoder/layer_11/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_11/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_193
1608802723989819	256
1608802723989830	13312
1608802723989891	-13312
1608802724258154	-256
gradients/bert/encoder/layer_11/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_11/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_11/intermediate/dense/Pow_grad/mul_1
gradients/AddN_6
gradients/bert/encoder/layer_11/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802723991885	13312
1608802724345261	-13312
gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul
1608802723991938	12582912
1608802723992569	-12582912
gradients/bert/encoder/layer_11/intermediate/dense/MatMul_grad/MatMul_1
1608802723992011	12582912
1608802724344643	-12582912
global_norm/L2Loss_192
1608802723992204	256
1608802724258153	-256
gradients/AddN_7
global_norm/L2Loss_191
1608802723992579	256
1608802723992590	9216
1608802723992634	-9216
1608802724258153	-256
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802723992661	12582912
1608802724007020	-12582912
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802723992698	12582912
1608802723993524	-12582912
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802723992739	3072
1608802724344580	-3072
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_189
1608802723993135	256
1608802724258152	-256
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802723993288	16384
1608802723993642	-16384
gradients/AddN_8
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802723993547	12582912
1608802723993757	-12582912
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/mean_grad/Tile
1608802723993619	12582912
1608802723994992	-12582912
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802723993659	16384
1608802723994721	-16384
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802723993767	3072
1608802724343757	-3072
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_190
1608802723994610	256
1608802724258152	-256
gradients/bert/encoder/layer_11/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/variance_grad/Tile
1608802723994730	12582912
1608802723994991	-12582912
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_11/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_9
gradients/bert/encoder/layer_11/attention/output/dropout/mul_1_grad/Mul
1608802723995003	12582912
1608802723995064	-12582912
gradients/bert/encoder/layer_11/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_11/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802723995075	3072
1608802724343690	-3072
gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul
1608802723995125	12582912
1608802723998031	-12582912
gradients/bert/encoder/layer_11/attention/output/dense/MatMul_grad/MatMul_1
1608802723995172	3810048
1608802724342761	-3810048
global_norm/L2Loss_188
1608802723995231	256
1608802724258151	-256
gradients/bert/encoder/layer_11/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_187
1608802723995275	256
1608802723995286	12800
1608802723997657	-12800
1608802724258151	-256
gradients/bert/encoder/layer_11/attention/self/transpose_3_grad/transpose
1608802723997673	12582912
1608802723999719	-12582912
gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul
1608802723998048	25165824
1608802723998122	3072
1608802723998124	3072
1608802723998125	3072
1608802723998820	-3072
1608802723998821	-3072
1608802723998822	-3072
1608802724002666	-25165824
gradients/bert/encoder/layer_11/attention/self/MatMul_1_grad/MatMul_1
1608802723998849	12582912
1608802723998891	3072
1608802723998893	3072
1608802723998894	3072
1608802723999708	-3072
1608802723999710	-3072
1608802723999711	-3072
1608802724000188	-12582912
gradients/bert/encoder/layer_11/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_11/attention/self/transpose_2_grad/transpose
1608802723999918	12582912
1608802724002930	-12582912
gradients/bert/encoder/layer_11/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_11/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_11/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724002733	3072
1608802724341895	-3072
gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul
1608802724002803	12582912
1608802724006441	-12582912
gradients/bert/encoder/layer_11/attention/self/value/MatMul_grad/MatMul_1
1608802724002883	2359296
1608802724343630	-2359296
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum
1608802724002945	196608
1608802724003101	-196608
global_norm/L2Loss_186
1608802724002986	256
1608802724258150	-256
global_norm/L2Loss_185
1608802724003015	256
1608802724003025	9216
1608802724003066	-9216
1608802724258150	-256
gradients/bert/encoder/layer_11/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_11/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul
1608802724003144	12582912
1608802724003209	3072
1608802724003211	3072
1608802724003211	3072
1608802724003331	-3072
1608802724003332	-3072
1608802724003333	-3072
1608802724005359	-12582912
gradients/bert/encoder/layer_11/attention/self/MatMul_grad/MatMul_1
1608802724003357	12582912
1608802724003395	3072
1608802724003397	3072
1608802724003398	3072
1608802724005294	-3072
1608802724005295	-3072
1608802724005296	-3072
1608802724005419	-12582912
gradients/bert/encoder/layer_11/attention/self/transpose_grad/transpose
1608802724005321	14548992
1608802724005985	-14548992
gradients/bert/encoder/layer_11/attention/self/transpose_1_grad/transpose
1608802724005374	12582912
1608802724006115	-12582912
gradients/bert/encoder/layer_11/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_11/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_11/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724005446	3072
1608802724328802	-3072
gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul
1608802724005613	12582912
1608802724006440	-12582912
gradients/bert/encoder/layer_11/attention/self/query/MatMul_grad/MatMul_1
1608802724005808	2359296
1608802724328044	-2359296
gradients/bert/encoder/layer_11/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724005997	3072
1608802724329574	-3072
gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul
1608802724006036	14548992
1608802724006439	-14548992
gradients/bert/encoder/layer_11/attention/self/key/MatMul_grad/MatMul_1
1608802724006074	3407872
1608802724333265	-3407872
global_norm/L2Loss_182
1608802724006128	256
1608802724258148	-256
global_norm/L2Loss_181
1608802724006160	256
1608802724006169	9216
1608802724006245	-9216
1608802724258147	-256
global_norm/L2Loss_184
1608802724006260	256
1608802724258149	-256
gradients/AddN_10
global_norm/L2Loss_183
1608802724006451	256
1608802724006459	9216
1608802724006698	-9216
1608802724258148	-256
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724006713	12582912
1608802724016663	-12582912
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724006821	12582912
1608802724008351	-12582912
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724006955	14548992
1608802724008076	-14548992
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724006994	3072
1608802724330371	-3072
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_179
1608802724007189	256
1608802724258146	-256
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724007382	16384
1608802724008258	-16384
gradients/AddN_11
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724008092	12582912
1608802724008313	-12582912
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_10/output/LayerNorm/moments/mean_grad/Tile
1608802724008234	12582912
1608802724011084	-12582912
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724008275	16384
1608802724008468	-16384
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724008326	3072
1608802724331156	-3072
gradients/bert/encoder/layer_10/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_180
1608802724008414	256
1608802724258147	-256
gradients/bert/encoder/layer_10/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_10/output/LayerNorm/moments/variance_grad/Tile
1608802724008476	12582912
1608802724011084	-12582912
gradients/bert/encoder/layer_10/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_12
gradients/bert/encoder/layer_10/output/dropout/mul_1_grad/Mul
1608802724011100	12582912
1608802724011648	-12582912
gradients/bert/encoder/layer_10/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_10/output/dense/BiasAdd_grad/BiasAddGrad
1608802724011659	3072
1608802724329522	-3072
gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul
1608802724011854	78136320
1608802724012924	-78136320
gradients/bert/encoder/layer_10/output/dense/MatMul_grad/MatMul_1
1608802724012055	12582912
1608802724328746	-12582912
global_norm/L2Loss_178
1608802724012248	256
1608802724258146	-256
gradients/bert/encoder/layer_10/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_10/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_177
1608802724012935	256
1608802724012945	13312
1608802724013314	-13312
1608802724258145	-256
gradients/bert/encoder/layer_10/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_10/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_10/intermediate/dense/Pow_grad/mul_1
gradients/AddN_13
gradients/bert/encoder/layer_10/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724016227	13312
1608802724330317	-13312
gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul
1608802724016268	12582912
1608802724016426	-12582912
gradients/bert/encoder/layer_10/intermediate/dense/MatMul_grad/MatMul_1
1608802724016313	12582912
1608802724329470	-12582912
global_norm/L2Loss_176
1608802724016357	256
1608802724258145	-256
gradients/AddN_14
global_norm/L2Loss_175
1608802724016435	256
1608802724016444	9216
1608802724016482	-9216
1608802724258144	-256
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724016499	12582912
1608802724030471	-12582912
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724016536	12582912
1608802724019404	-12582912
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724016567	14548992
1608802724018708	-14548992
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724016621	3072
1608802724327940	-3072
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_173
1608802724017380	256
1608802724258143	-256
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724018020	17920
1608802724019020	-17920
gradients/AddN_15
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724018727	12582912
1608802724019213	-12582912
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724018846	12582912
1608802724020123	-12582912
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724019037	17920
1608802724019500	-17920
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724019226	3072
1608802724328669	-3072
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_174
1608802724019451	256
1608802724258143	-256
gradients/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724019508	12582912
1608802724020122	-12582912
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_10/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_16
gradients/bert/encoder/layer_10/attention/output/dropout/mul_1_grad/Mul
1608802724020132	12582912
1608802724020372	-12582912
gradients/bert/encoder/layer_10/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_10/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724020381	3072
1608802724331101	-3072
gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul
1608802724020429	12582912
1608802724021480	-12582912
gradients/bert/encoder/layer_10/attention/output/dense/MatMul_grad/MatMul_1
1608802724020471	2359296
1608802724329413	-2359296
global_norm/L2Loss_172
1608802724020532	256
1608802724258142	-256
gradients/bert/encoder/layer_10/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_171
1608802724020604	256
1608802724020613	14336
1608802724021413	-14336
1608802724258142	-256
gradients/bert/encoder/layer_10/attention/self/transpose_3_grad/transpose
1608802724021428	12582912
1608802724021863	-12582912
gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul
1608802724021495	25165824
1608802724021552	3072
1608802724021554	3072
1608802724021555	3072
1608802724021716	-3072
1608802724021717	-3072
1608802724021718	-3072
1608802724021992	-25165824
gradients/bert/encoder/layer_10/attention/self/MatMul_1_grad/MatMul_1
1608802724021742	12582912
1608802724021780	3072
1608802724021782	3072
1608802724021783	3072
1608802724021853	-3072
1608802724021854	-3072
1608802724021855	-3072
1608802724021937	-12582912
gradients/bert/encoder/layer_10/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_10/attention/self/transpose_2_grad/transpose
1608802724021913	12582912
1608802724024501	-12582912
gradients/bert/encoder/layer_10/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_10/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_10/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724022074	3072
1608802724328615	-3072
gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul
1608802724022113	12582912
1608802724030169	-12582912
gradients/bert/encoder/layer_10/attention/self/value/MatMul_grad/MatMul_1
1608802724024287	2359296
1608802724327888	-2359296
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum
1608802724024515	196608
1608802724025649	-196608
global_norm/L2Loss_170
1608802724024876	256
1608802724258141	-256
global_norm/L2Loss_169
1608802724025065	256
1608802724025074	10752
1608802724025449	-10752
1608802724258141	-256
gradients/bert/encoder/layer_10/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_10/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul
1608802724025942	12582912
1608802724026003	3072
1608802724026005	3072
1608802724026006	4608
1608802724026733	-3072
1608802724026734	-3072
1608802724026735	-4608
1608802724029707	-12582912
gradients/bert/encoder/layer_10/attention/self/MatMul_grad/MatMul_1
1608802724026758	12582912
1608802724026794	3072
1608802724026795	3072
1608802724026796	4608
1608802724029631	-3072
1608802724029632	-3072
1608802724029633	-4608
1608802724029742	-12582912
gradients/bert/encoder/layer_10/attention/self/transpose_grad/transpose
1608802724029656	12582912
1608802724029884	-12582912
gradients/bert/encoder/layer_10/attention/self/transpose_1_grad/transpose
1608802724029719	12582912
1608802724030001	-12582912
gradients/bert/encoder/layer_10/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_10/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_10/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724029769	3072
1608802724341582	-3072
gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul
1608802724029810	12582912
1608802724030168	-12582912
gradients/bert/encoder/layer_10/attention/self/query/MatMul_grad/MatMul_1
1608802724029846	2359296
1608802724340768	-2359296
gradients/bert/encoder/layer_10/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724029896	3072
1608802724331045	-3072
gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul
1608802724029930	12582912
1608802724030167	-12582912
gradients/bert/encoder/layer_10/attention/self/key/MatMul_grad/MatMul_1
1608802724029964	2359296
1608802724342448	-2359296
global_norm/L2Loss_166
1608802724030012	256
1608802724258139	-256
global_norm/L2Loss_165
1608802724030041	256
1608802724030050	9216
1608802724030088	-9216
1608802724258138	-256
global_norm/L2Loss_168
1608802724030098	256
1608802724258140	-256
gradients/AddN_17
global_norm/L2Loss_167
1608802724030179	256
1608802724030186	9216
1608802724030325	-9216
1608802724258140	-256
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724030338	12582912
1608802724032039	-12582912
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724030376	12582912
1608802724030804	-12582912
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724030409	12582912
1608802724030626	-12582912
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724030443	3584
1608802724345020	-3584
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_163
1608802724030542	256
1608802724258137	-256
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724030571	29440
1608802724030730	-29440
gradients/AddN_18
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724030646	12582912
1608802724030770	-12582912
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_9/output/LayerNorm/moments/mean_grad/Tile
1608802724030709	12582912
1608802724031100	-12582912
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724030746	29440
1608802724030903	-29440
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724030782	3072
1608802724345568	-3072
gradients/bert/encoder/layer_9/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_164
1608802724030852	256
1608802724258138	-256
gradients/bert/encoder/layer_9/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_9/output/LayerNorm/moments/variance_grad/Tile
1608802724030911	12582912
1608802724031100	-12582912
gradients/bert/encoder/layer_9/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_9/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_19
gradients/bert/encoder/layer_9/output/dropout/mul_1_grad/Mul
1608802724031108	12582912
1608802724031164	-12582912
gradients/bert/encoder/layer_9/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_9/output/dense/BiasAdd_grad/BiasAddGrad
1608802724031173	3072
1608802724344258	-3072
gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul
1608802724031216	78136320
1608802724031374	-78136320
gradients/bert/encoder/layer_9/output/dense/MatMul_grad/MatMul_1
1608802724031257	12582912
1608802724343321	-12582912
global_norm/L2Loss_162
1608802724031294	256
1608802724258136	-256
gradients/bert/encoder/layer_9/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_9/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_161
1608802724031385	256
1608802724031393	9216
1608802724031436	-9216
1608802724258136	-256
gradients/bert/encoder/layer_9/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_9/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_9/intermediate/dense/Pow_grad/mul_1
gradients/AddN_20
gradients/bert/encoder/layer_9/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724031571	12288
1608802724344955	-12288
gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul
1608802724031623	12582912
1608802724031857	-12582912
gradients/bert/encoder/layer_9/intermediate/dense/MatMul_grad/MatMul_1
1608802724031661	12582912
1608802724344197	-12582912
global_norm/L2Loss_160
1608802724031703	256
1608802724258135	-256
gradients/AddN_21
global_norm/L2Loss_159
1608802724031867	256
1608802724031877	13056
1608802724031913	-13056
1608802724258135	-256
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724031925	12582912
1608802724048059	-12582912
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724031957	12582912
1608802724036638	-12582912
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724031985	12582912
1608802724034894	-12582912
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724032015	3072
1608802724342327	-3072
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_157
1608802724032112	256
1608802724258134	-256
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724034410	29440
1608802724035896	-29440
gradients/AddN_22
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724034914	12582912
1608802724036326	-12582912
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724035591	14548992
1608802724044462	-14548992
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724035910	29440
1608802724037251	-29440
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724036340	3072
1608802724343261	-3072
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_158
1608802724036810	256
1608802724258134	-256
gradients/bert/encoder/layer_9/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724037262	12582912
1608802724044461	-12582912
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_9/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_23
gradients/bert/encoder/layer_9/attention/output/dropout/mul_1_grad/Mul
1608802724044470	12582912
1608802724044526	-12582912
gradients/bert/encoder/layer_9/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_9/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724044535	3072
1608802724344137	-3072
gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul
1608802724044608	12582912
1608802724045180	-12582912
gradients/bert/encoder/layer_9/attention/output/dense/MatMul_grad/MatMul_1
1608802724044788	3145728
1608802724343200	-3145728
global_norm/L2Loss_156
1608802724044916	256
1608802724258134	-256
gradients/bert/encoder/layer_9/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_155
1608802724044998	256
1608802724045033	9216
1608802724045104	-9216
1608802724258133	-256
gradients/bert/encoder/layer_9/attention/self/transpose_3_grad/transpose
1608802724045128	12582912
1608802724045712	-12582912
gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul
1608802724045207	25165824
1608802724045311	3072
1608802724045316	3072
1608802724045318	3072
1608802724045457	-3072
1608802724045459	-3072
1608802724045460	-3072
1608802724045880	-25165824
gradients/bert/encoder/layer_9/attention/self/MatMul_1_grad/MatMul_1
1608802724045504	12582912
1608802724045573	3072
1608802724045576	3072
1608802724045578	3072
1608802724045694	-3072
1608802724045696	-3072
1608802724045697	-3072
1608802724045827	-12582912
gradients/bert/encoder/layer_9/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_9/attention/self/transpose_2_grad/transpose
1608802724045791	12582912
1608802724046202	-12582912
gradients/bert/encoder/layer_9/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_9/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_9/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724045948	3072
1608802724342266	-3072
gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul
1608802724046079	12582912
1608802724047734	-12582912
gradients/bert/encoder/layer_9/attention/self/value/MatMul_grad/MatMul_1
1608802724046142	2359296
1608802724341396	-2359296
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum
1608802724046227	196608
1608802724046463	-196608
global_norm/L2Loss_154
1608802724046291	256
1608802724258132	-256
global_norm/L2Loss_153
1608802724046334	256
1608802724046351	9216
1608802724046409	-9216
1608802724258131	-256
gradients/bert/encoder/layer_9/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_9/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul
1608802724046524	12582912
1608802724046615	3072
1608802724046618	3072
1608802724046620	3072
1608802724046735	-3072
1608802724046736	-3072
1608802724046738	-3072
1608802724047036	-12582912
gradients/bert/encoder/layer_9/attention/self/MatMul_grad/MatMul_1
1608802724046778	12582912
1608802724046842	3072
1608802724046844	3072
1608802724046846	3072
1608802724046954	-3072
1608802724046956	-3072
1608802724046957	-3072
1608802724047088	-12582912
gradients/bert/encoder/layer_9/attention/self/transpose_grad/transpose
1608802724046995	14548992
1608802724047306	-14548992
gradients/bert/encoder/layer_9/attention/self/transpose_1_grad/transpose
1608802724047054	12582912
1608802724047488	-12582912
gradients/bert/encoder/layer_9/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_9/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_9/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724047128	3072
1608802724339077	-3072
gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul
1608802724047188	12582912
1608802724047733	-12582912
gradients/bert/encoder/layer_9/attention/self/query/MatMul_grad/MatMul_1
1608802724047246	2359296
1608802724344068	-2359296
gradients/bert/encoder/layer_9/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724047324	3072
1608802724341332	-3072
gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul
1608802724047378	14548992
1608802724047732	-14548992
gradients/bert/encoder/layer_9/attention/self/key/MatMul_grad/MatMul_1
1608802724047430	2359296
1608802724339763	-2359296
global_norm/L2Loss_150
1608802724047511	256
1608802724258128	-256
global_norm/L2Loss_149
1608802724047557	256
1608802724047573	9216
1608802724047641	-9216
1608802724258128	-256
global_norm/L2Loss_152
1608802724047659	256
1608802724258130	-256
gradients/AddN_24
global_norm/L2Loss_151
1608802724047751	256
1608802724047765	9216
1608802724047830	-9216
1608802724258129	-256
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724047853	12582912
1608802724059157	-12582912
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724047910	12582912
1608802724048615	-12582912
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724047960	14548992
1608802724048298	-14548992
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724048019	3072
1608802724342141	-3072
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_147
1608802724048172	256
1608802724258126	-256
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724048215	18432
1608802724048500	-18432
gradients/AddN_25
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724048323	12582912
1608802724048561	-12582912
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_8/output/LayerNorm/moments/mean_grad/Tile
1608802724048456	12582912
1608802724051990	-12582912
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724048524	18432
1608802724048849	-18432
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724048574	3072
1608802724343074	-3072
gradients/bert/encoder/layer_8/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_148
1608802724048692	256
1608802724258127	-256
gradients/bert/encoder/layer_8/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_8/output/LayerNorm/moments/variance_grad/Tile
1608802724048862	12582912
1608802724051989	-12582912
gradients/bert/encoder/layer_8/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_8/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_26
gradients/bert/encoder/layer_8/output/dropout/mul_1_grad/Mul
1608802724052010	12582912
1608802724052407	-12582912
gradients/bert/encoder/layer_8/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_8/output/dense/BiasAdd_grad/BiasAddGrad
1608802724052422	3072
1608802724341269	-3072
gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul
1608802724052491	78136320
1608802724058197	-78136320
gradients/bert/encoder/layer_8/output/dense/MatMul_grad/MatMul_1
1608802724052732	12582912
1608802724344001	-12582912
global_norm/L2Loss_146
1608802724053191	256
1608802724258125	-256
gradients/bert/encoder/layer_8/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_8/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_145
1608802724058232	256
1608802724058248	14848
1608802724058315	-14848
1608802724258124	-256
gradients/bert/encoder/layer_8/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_8/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_8/intermediate/dense/Pow_grad/mul_1
gradients/AddN_27
gradients/bert/encoder/layer_8/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724058526	14848
1608802724343014	-14848
gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul
1608802724058595	12582912
1608802724058841	-12582912
gradients/bert/encoder/layer_8/intermediate/dense/MatMul_grad/MatMul_1
1608802724058665	12582912
1608802724342077	-12582912
global_norm/L2Loss_144
1608802724058755	256
1608802724258123	-256
gradients/AddN_28
global_norm/L2Loss_143
1608802724058856	256
1608802724058872	9216
1608802724058932	-9216
1608802724258123	-256
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724058960	12582912
1608802724068400	-12582912
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724059015	12582912
1608802724059672	-12582912
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724059062	14548992
1608802724059392	-14548992
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724059118	3072
1608802724342017	-3072
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_141
1608802724059267	256
1608802724258121	-256
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724059311	28928
1608802724059561	-28928
gradients/AddN_29
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724059428	12582912
1608802724059621	-12582912
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724059527	12582912
1608802724061203	-12582912
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724059585	28928
1608802724060268	-28928
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724059633	3072
1608802724341202	-3072
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_142
1608802724059751	256
1608802724258122	-256
gradients/bert/encoder/layer_8/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724060281	12582912
1608802724061201	-12582912
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_8/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_30
gradients/bert/encoder/layer_8/attention/output/dropout/mul_1_grad/Mul
1608802724061215	12582912
1608802724061296	-12582912
gradients/bert/encoder/layer_8/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_8/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724061308	3072
1608802724341139	-3072
gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul
1608802724061371	12582912
1608802724061713	-12582912
gradients/bert/encoder/layer_8/attention/output/dense/MatMul_grad/MatMul_1
1608802724061434	2359296
1608802724340324	-2359296
global_norm/L2Loss_140
1608802724061517	256
1608802724258120	-256
gradients/bert/encoder/layer_8/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_139
1608802724061582	256
1608802724061597	9216
1608802724061653	-9216
1608802724258119	-256
gradients/bert/encoder/layer_8/attention/self/transpose_3_grad/transpose
1608802724061671	12582912
1608802724063571	-12582912
gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul
1608802724061734	25165824
1608802724061823	3072
1608802724061826	3072
1608802724061828	3072
1608802724062828	-3072
1608802724062830	-3072
1608802724062832	-3072
1608802724063781	-25165824
gradients/bert/encoder/layer_8/attention/self/MatMul_1_grad/MatMul_1
1608802724062869	12582912
1608802724062934	3072
1608802724062936	3072
1608802724062938	3072
1608802724063555	-3072
1608802724063557	-3072
1608802724063558	-3072
1608802724063677	-12582912
gradients/bert/encoder/layer_8/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_8/attention/self/transpose_2_grad/transpose
1608802724063643	12582912
1608802724064007	-12582912
gradients/bert/encoder/layer_8/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_8/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_8/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724063843	3072
1608802724339579	-3072
gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul
1608802724063898	12582912
1608802724068102	-12582912
gradients/bert/encoder/layer_8/attention/self/value/MatMul_grad/MatMul_1
1608802724063952	3145728
1608802724342887	-3145728
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum
1608802724064029	196608
1608802724064285	-196608
global_norm/L2Loss_138
1608802724064085	256
1608802724258118	-256
global_norm/L2Loss_137
1608802724064124	256
1608802724064139	9216
1608802724064193	-9216
1608802724258118	-256
gradients/bert/encoder/layer_8/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_8/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul
1608802724064507	12582912
1608802724064605	3072
1608802724064608	3072
1608802724064610	3072
1608802724066052	-3072
1608802724066054	-3072
1608802724066055	-3072
1608802724067440	-12582912
gradients/bert/encoder/layer_8/attention/self/MatMul_grad/MatMul_1
1608802724066093	12582912
1608802724066154	3072
1608802724066157	3072
1608802724066158	3072
1608802724067361	-3072
1608802724067362	-3072
1608802724067364	-3072
1608802724067497	-12582912
gradients/bert/encoder/layer_8/attention/self/transpose_grad/transpose
1608802724067400	12582912
1608802724067718	-12582912
gradients/bert/encoder/layer_8/attention/self/transpose_1_grad/transpose
1608802724067458	12582912
1608802724067893	-12582912
gradients/bert/encoder/layer_8/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_8/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_8/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724067538	3072
1608802724340257	-3072
gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul
1608802724067601	12582912
1608802724068101	-12582912
gradients/bert/encoder/layer_8/attention/self/query/MatMul_grad/MatMul_1
1608802724067659	2359296
1608802724342826	-2359296
gradients/bert/encoder/layer_8/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724067735	3072
1608802724341956	-3072
gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul
1608802724067790	12582912
1608802724068099	-12582912
gradients/bert/encoder/layer_8/attention/self/key/MatMul_grad/MatMul_1
1608802724067840	2359296
1608802724341078	-2359296
global_norm/L2Loss_134
1608802724067911	256
1608802724258115	-256
global_norm/L2Loss_133
1608802724067951	256
1608802724067966	15104
1608802724068022	-15104
1608802724258114	-256
global_norm/L2Loss_136
1608802724068037	256
1608802724258117	-256
gradients/AddN_31
global_norm/L2Loss_135
1608802724068115	256
1608802724068126	14592
1608802724068180	-14592
1608802724258116	-256
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724068198	12582912
1608802724081225	-12582912
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724068243	12582912
1608802724068926	-12582912
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724068296	12582912
1608802724068646	-12582912
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724068351	3072
1608802724339459	-3072
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_131
1608802724068528	256
1608802724258112	-256
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724068566	16384
1608802724068812	-16384
gradients/AddN_32
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724068678	12582912
1608802724068873	-12582912
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_7/output/LayerNorm/moments/mean_grad/Tile
1608802724068779	12582912
1608802724074201	-12582912
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724068838	16384
1608802724071060	-16384
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724068888	3072
1608802724340195	-3072
gradients/bert/encoder/layer_7/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_132
1608802724068997	256
1608802724258113	-256
gradients/bert/encoder/layer_7/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_7/output/LayerNorm/moments/variance_grad/Tile
1608802724071074	12582912
1608802724074199	-12582912
gradients/bert/encoder/layer_7/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_7/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_33
gradients/bert/encoder/layer_7/output/dropout/mul_1_grad/Mul
1608802724074220	12582912
1608802724074607	-12582912
gradients/bert/encoder/layer_7/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_7/output/dense/BiasAdd_grad/BiasAddGrad
1608802724074620	3072
1608802724338761	-3072
gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul
1608802724074680	78136320
1608802724080400	-78136320
gradients/bert/encoder/layer_7/output/dense/MatMul_grad/MatMul_1
1608802724074919	9830400
1608802724340959	-9830400
global_norm/L2Loss_130
1608802724075388	256
1608802724258112	-256
gradients/bert/encoder/layer_7/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_7/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_129
1608802724080412	256
1608802724080427	9216
1608802724080481	-9216
1608802724258111	-256
gradients/bert/encoder/layer_7/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_7/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_7/intermediate/dense/Pow_grad/mul_1
gradients/AddN_34
gradients/bert/encoder/layer_7/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724080684	12288
1608802724340131	-12288
gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul
1608802724080745	12582912
1608802724080937	-12582912
gradients/bert/encoder/layer_7/intermediate/dense/MatMul_grad/MatMul_1
1608802724080806	12582912
1608802724339387	-12582912
global_norm/L2Loss_128
1608802724080865	256
1608802724258110	-256
gradients/AddN_35
global_norm/L2Loss_127
1608802724080955	256
1608802724080969	9216
1608802724081022	-9216
1608802724258109	-256
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724081048	12582912
1608802724090513	-12582912
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724081096	12582912
1608802724081696	-12582912
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724081143	12582912
1608802724081440	-12582912
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724081191	3840
1608802724340069	-3840
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_125
1608802724081329	256
1608802724258108	-256
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724081365	16384
1608802724081593	-16384
gradients/AddN_36
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724081469	12582912
1608802724081649	-12582912
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724081562	12582912
1608802724083347	-12582912
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724081617	16384
1608802724082467	-16384
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724081660	3072
1608802724338067	-3072
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_126
1608802724081768	256
1608802724258108	-256
gradients/bert/encoder/layer_7/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724082479	12582912
1608802724083346	-12582912
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_7/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_37
gradients/bert/encoder/layer_7/attention/output/dropout/mul_1_grad/Mul
1608802724083366	12582912
1608802724083445	-12582912
gradients/bert/encoder/layer_7/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_7/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724083458	3072
1608802724339319	-3072
gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul
1608802724083520	12582912
1608802724083844	-12582912
gradients/bert/encoder/layer_7/attention/output/dense/MatMul_grad/MatMul_1
1608802724083579	2359296
1608802724338007	-2359296
global_norm/L2Loss_124
1608802724083656	256
1608802724258107	-256
gradients/bert/encoder/layer_7/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_123
1608802724083714	256
1608802724083728	12800
1608802724083788	-12800
1608802724258106	-256
gradients/bert/encoder/layer_7/attention/self/transpose_3_grad/transpose
1608802724083806	12582912
1608802724085727	-12582912
gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul
1608802724083865	25165824
1608802724083939	3072
1608802724083942	3072
1608802724083943	3072
1608802724084988	-3072
1608802724084990	-3072
1608802724084991	-3072
1608802724085940	-25165824
gradients/bert/encoder/layer_7/attention/self/MatMul_1_grad/MatMul_1
1608802724085028	12582912
1608802724085082	3072
1608802724085085	3072
1608802724085086	3072
1608802724085712	-3072
1608802724085714	-3072
1608802724085715	-3072
1608802724085828	-12582912
gradients/bert/encoder/layer_7/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_7/attention/self/transpose_2_grad/transpose
1608802724085795	12582912
1608802724086152	-12582912
gradients/bert/encoder/layer_7/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_7/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_7/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724085998	3072
1608802724337450	-3072
gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul
1608802724086047	12582912
1608802724090318	-12582912
gradients/bert/encoder/layer_7/attention/self/value/MatMul_grad/MatMul_1
1608802724086099	2359296
1608802724340006	-2359296
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum
1608802724086169	196608
1608802724086426	-196608
global_norm/L2Loss_122
1608802724086221	256
1608802724258105	-256
global_norm/L2Loss_121
1608802724086258	256
1608802724086270	9216
1608802724086319	-9216
1608802724258104	-256
gradients/bert/encoder/layer_7/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_7/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul
1608802724086630	12582912
1608802724086703	3072
1608802724086705	3072
1608802724086707	3072
1608802724088186	-3072
1608802724088187	-3072
1608802724088189	-3072
1608802724089571	-12582912
gradients/bert/encoder/layer_7/attention/self/MatMul_grad/MatMul_1
1608802724088221	12582912
1608802724088273	3072
1608802724088275	3072
1608802724088277	3072
1608802724089496	-3072
1608802724089498	-3072
1608802724089499	-3072
1608802724089617	-12582912
gradients/bert/encoder/layer_7/attention/self/transpose_grad/transpose
1608802724089534	12582912
1608802724089912	-12582912
gradients/bert/encoder/layer_7/attention/self/transpose_1_grad/transpose
1608802724089587	12582912
1608802724090156	-12582912
gradients/bert/encoder/layer_7/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_7/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_7/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724089725	3072
1608802724337387	-3072
gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul
1608802724089806	12582912
1608802724090317	-12582912
gradients/bert/encoder/layer_7/attention/self/query/MatMul_grad/MatMul_1
1608802724089862	2359296
1608802724339941	-2359296
gradients/bert/encoder/layer_7/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724089928	3072
1608802724339258	-3072
gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul
1608802724089971	12582912
1608802724090316	-12582912
gradients/bert/encoder/layer_7/attention/self/key/MatMul_grad/MatMul_1
1608802724090110	2359296
1608802724338572	-2359296
global_norm/L2Loss_118
1608802724090170	256
1608802724258102	-256
global_norm/L2Loss_117
1608802724090204	256
1608802724090214	9216
1608802724090256	-9216
1608802724258102	-256
global_norm/L2Loss_120
1608802724090268	256
1608802724258103	-256
gradients/AddN_38
global_norm/L2Loss_119
1608802724090331	256
1608802724090339	9216
1608802724090375	-9216
1608802724258103	-256
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724090387	12582912
1608802724103531	-12582912
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724090421	12582912
1608802724090828	-12582912
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724090452	12582912
1608802724090658	-12582912
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724090486	3072
1608802724338510	-3072
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_115
1608802724090581	256
1608802724258101	-256
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724090608	23552
1608802724090762	-23552
gradients/AddN_39
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724090676	12582912
1608802724090798	-12582912
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_6/output/LayerNorm/moments/mean_grad/Tile
1608802724090739	12582912
1608802724096332	-12582912
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724090774	23552
1608802724093208	-23552
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724090807	3072
1608802724339197	-3072
gradients/bert/encoder/layer_6/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_116
1608802724090873	256
1608802724258101	-256
gradients/bert/encoder/layer_6/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_6/output/LayerNorm/moments/variance_grad/Tile
1608802724093230	12582912
1608802724096331	-12582912
gradients/bert/encoder/layer_6/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_6/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_40
gradients/bert/encoder/layer_6/output/dropout/mul_1_grad/Mul
1608802724096346	12582912
1608802724096753	-12582912
gradients/bert/encoder/layer_6/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_6/output/dense/BiasAdd_grad/BiasAddGrad
1608802724096767	3072
1608802724337885	-3072
gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul
1608802724096832	50331648
1608802724102556	-50331648
gradients/bert/encoder/layer_6/output/dense/MatMul_grad/MatMul_1
1608802724097086	12582912
1608802724340707	-12582912
global_norm/L2Loss_114
1608802724097547	256
1608802724258100	-256
gradients/bert/encoder/layer_6/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_6/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_113
1608802724102573	256
1608802724102591	9216
1608802724102656	-9216
1608802724258100	-256
gradients/bert/encoder/layer_6/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_6/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_6/intermediate/dense/Pow_grad/mul_1
gradients/AddN_41
gradients/bert/encoder/layer_6/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724102889	20480
1608802724339882	-20480
gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul
1608802724102969	12582912
1608802724103198	-12582912
gradients/bert/encoder/layer_6/intermediate/dense/MatMul_grad/MatMul_1
1608802724103043	12582912
1608802724339137	-12582912
global_norm/L2Loss_112
1608802724103109	256
1608802724258099	-256
gradients/AddN_42
global_norm/L2Loss_111
1608802724103218	256
1608802724103234	9216
1608802724103296	-9216
1608802724258099	-256
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724103322	12582912
1608802724112796	-12582912
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724103376	12582912
1608802724104103	-12582912
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724103432	12582912
1608802724103797	-12582912
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724103491	3072
1608802724341515	-3072
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_109
1608802724103663	256
1608802724258098	-256
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724103705	29696
1608802724103983	-29696
gradients/AddN_43
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724103834	12582912
1608802724104048	-12582912
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724103945	12582912
1608802724105523	-12582912
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724104010	29696
1608802724104626	-29696
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724104062	3072
1608802724338449	-3072
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_110
1608802724104185	256
1608802724258098	-256
gradients/bert/encoder/layer_6/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724104643	12582912
1608802724105522	-12582912
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_6/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_44
gradients/bert/encoder/layer_6/attention/output/dropout/mul_1_grad/Mul
1608802724105544	12582912
1608802724105630	-12582912
gradients/bert/encoder/layer_6/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_6/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724105644	3072
1608802724340645	-3072
gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul
1608802724105710	12582912
1608802724106081	-12582912
gradients/bert/encoder/layer_6/attention/output/dense/MatMul_grad/MatMul_1
1608802724105777	3145728
1608802724338384	-3145728
global_norm/L2Loss_108
1608802724105865	256
1608802724258097	-256
gradients/bert/encoder/layer_6/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_107
1608802724105931	256
1608802724105946	9216
1608802724106014	-9216
1608802724258097	-256
gradients/bert/encoder/layer_6/attention/self/transpose_3_grad/transpose
1608802724106035	12582912
1608802724107892	-12582912
gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul
1608802724106103	25165824
1608802724106208	3072
1608802724106211	3072
1608802724106213	3072
1608802724107144	-3072
1608802724107146	-3072
1608802724107148	-3072
1608802724108097	-25165824
gradients/bert/encoder/layer_6/attention/self/MatMul_1_grad/MatMul_1
1608802724107191	12582912
1608802724107254	3072
1608802724107257	3072
1608802724107259	3072
1608802724107875	-3072
1608802724107877	-3072
1608802724107878	-3072
1608802724108014	-12582912
gradients/bert/encoder/layer_6/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_6/attention/self/transpose_2_grad/transpose
1608802724107976	12582912
1608802724108375	-12582912
gradients/bert/encoder/layer_6/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_6/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_6/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724108169	3072
1608802724337750	-3072
gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul
1608802724108228	12582912
1608802724112469	-12582912
gradients/bert/encoder/layer_6/attention/self/value/MatMul_grad/MatMul_1
1608802724108288	2359296
1608802724343383	-2359296
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum
1608802724108398	196608
1608802724108634	-196608
global_norm/L2Loss_106
1608802724108461	256
1608802724258096	-256
global_norm/L2Loss_105
1608802724108505	256
1608802724108520	9216
1608802724108581	-9216
1608802724258096	-256
gradients/bert/encoder/layer_6/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_6/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul
1608802724108793	12582912
1608802724108890	3072
1608802724108893	3072
1608802724108895	3072
1608802724110354	-3072
1608802724110356	-3072
1608802724110358	-3072
1608802724111754	-12582912
gradients/bert/encoder/layer_6/attention/self/MatMul_grad/MatMul_1
1608802724110396	12582912
1608802724110458	3072
1608802724110460	3072
1608802724110462	3072
1608802724111668	-3072
1608802724111670	-3072
1608802724111671	-3072
1608802724111808	-12582912
gradients/bert/encoder/layer_6/attention/self/transpose_grad/transpose
1608802724111710	12582912
1608802724112033	-12582912
gradients/bert/encoder/layer_6/attention/self/transpose_1_grad/transpose
1608802724111773	12582912
1608802724112223	-12582912
gradients/bert/encoder/layer_6/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_6/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_6/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724111852	3072
1608802724337633	-3072
gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul
1608802724111915	12582912
1608802724112469	-12582912
gradients/bert/encoder/layer_6/attention/self/query/MatMul_grad/MatMul_1
1608802724111972	2359296
1608802724340515	-2359296
gradients/bert/encoder/layer_6/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724112051	3072
1608802724338323	-3072
gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul
1608802724112111	12582912
1608802724112466	-12582912
gradients/bert/encoder/layer_6/attention/self/key/MatMul_grad/MatMul_1
1608802724112164	2359296
1608802724336180	-2359296
global_norm/L2Loss_102
1608802724112242	256
1608802724258094	-256
global_norm/L2Loss_101
1608802724112289	256
1608802724112305	15872
1608802724112377	-15872
1608802724258094	-256
global_norm/L2Loss_104
1608802724112396	256
1608802724258095	-256
gradients/AddN_45
global_norm/L2Loss_103
1608802724112482	256
1608802724112494	15360
1608802724112551	-15360
1608802724258095	-256
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724112579	12582912
1608802724125576	-12582912
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724112634	12582912
1608802724113331	-12582912
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724112691	12582912
1608802724113036	-12582912
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724112747	3072
1608802724324502	-3072
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_99
1608802724112911	256
1608802724258093	-256
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724112952	16384
1608802724113215	-16384
gradients/AddN_46
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724113070	12582912
1608802724113279	-12582912
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_5/output/LayerNorm/moments/mean_grad/Tile
1608802724113178	12582912
1608802724118509	-12582912
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724113241	16384
1608802724115357	-16384
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724113291	3072
1608802724339703	-3072
gradients/bert/encoder/layer_5/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_100
1608802724113406	256
1608802724258093	-256
gradients/bert/encoder/layer_5/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_5/output/LayerNorm/moments/variance_grad/Tile
1608802724115371	12582912
1608802724118507	-12582912
gradients/bert/encoder/layer_5/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_5/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_47
gradients/bert/encoder/layer_5/output/dropout/mul_1_grad/Mul
1608802724118532	12582912
1608802724118922	-12582912
gradients/bert/encoder/layer_5/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_5/output/dense/BiasAdd_grad/BiasAddGrad
1608802724118938	3072
1608802724327546	-3072
gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul
1608802724119005	50331648
1608802724124703	-50331648
gradients/bert/encoder/layer_5/output/dense/MatMul_grad/MatMul_1
1608802724119235	12582912
1608802724326816	-12582912
global_norm/L2Loss_98
1608802724119708	256
1608802724258092	-256
gradients/bert/encoder/layer_5/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_5/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_97
1608802724124716	256
1608802724124731	9216
1608802724124790	-9216
1608802724258092	-256
gradients/bert/encoder/layer_5/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_5/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_5/intermediate/dense/Pow_grad/mul_1
gradients/AddN_48
gradients/bert/encoder/layer_5/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724124996	12288
1608802724338887	-12288
gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul
1608802724125063	12582912
1608802724125266	-12582912
gradients/bert/encoder/layer_5/intermediate/dense/MatMul_grad/MatMul_1
1608802724125130	12582912
1608802724335666	-12582912
global_norm/L2Loss_96
1608802724125188	256
1608802724258091	-256
gradients/AddN_49
global_norm/L2Loss_95
1608802724125285	256
1608802724125298	9216
1608802724125354	-9216
1608802724258091	-256
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724125382	12582912
1608802724134914	-12582912
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724125433	12582912
1608802724126081	-12582912
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724125485	12582912
1608802724125807	-12582912
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724125539	4608
1608802724338133	-4608
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_93
1608802724125685	256
1608802724258090	-256
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724125724	16384
1608802724125971	-16384
gradients/AddN_50
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724125837	12582912
1608802724126031	-12582912
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724125938	12582912
1608802724127661	-12582912
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724125995	16384
1608802724126772	-16384
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724126044	3072
1608802724338824	-3072
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_94
1608802724126154	256
1608802724258090	-256
gradients/bert/encoder/layer_5/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724126787	12582912
1608802724127659	-12582912
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_5/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_51
gradients/bert/encoder/layer_5/attention/output/dropout/mul_1_grad/Mul
1608802724127692	12582912
1608802724127780	-12582912
gradients/bert/encoder/layer_5/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_5/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724127793	3072
1608802724323541	-3072
gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul
1608802724127856	12582912
1608802724128209	-12582912
gradients/bert/encoder/layer_5/attention/output/dense/MatMul_grad/MatMul_1
1608802724127920	2359296
1608802724323200	-2359296
global_norm/L2Loss_92
1608802724128005	256
1608802724258089	-256
gradients/bert/encoder/layer_5/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_91
1608802724128068	256
1608802724128083	12800
1608802724128148	-12800
1608802724258089	-256
gradients/bert/encoder/layer_5/attention/self/transpose_3_grad/transpose
1608802724128168	12582912
1608802724130031	-12582912
gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul
1608802724128230	25165824
1608802724128321	3072
1608802724128324	3072
1608802724128326	3072
1608802724129283	-3072
1608802724129285	-3072
1608802724129287	-3072
1608802724130239	-25165824
gradients/bert/encoder/layer_5/attention/self/MatMul_1_grad/MatMul_1
1608802724129326	12582912
1608802724129384	3072
1608802724129387	3072
1608802724129388	3072
1608802724130014	-3072
1608802724130016	-3072
1608802724130018	-3072
1608802724130141	-12582912
gradients/bert/encoder/layer_5/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_5/attention/self/transpose_2_grad/transpose
1608802724130106	12582912
1608802724130470	-12582912
gradients/bert/encoder/layer_5/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_5/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_5/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724130302	3072
1608802724323484	-3072
gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul
1608802724130359	12582912
1608802724134618	-12582912
gradients/bert/encoder/layer_5/attention/self/value/MatMul_grad/MatMul_1
1608802724130413	3145728
1608802724322854	-3145728
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum
1608802724130490	196608
1608802724130727	-196608
global_norm/L2Loss_90
1608802724130550	256
1608802724258088	-256
global_norm/L2Loss_89
1608802724130593	256
1608802724130607	9216
1608802724130666	-9216
1608802724258088	-256
gradients/bert/encoder/layer_5/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_5/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul
1608802724130934	12582912
1608802724131022	3072
1608802724131024	3072
1608802724131026	3072
1608802724132493	-3072
1608802724132495	-3072
1608802724132496	-3072
1608802724133929	-12582912
gradients/bert/encoder/layer_5/attention/self/MatMul_grad/MatMul_1
1608802724132532	12582912
1608802724132596	3072
1608802724132598	3072
1608802724132599	3072
1608802724133851	-3072
1608802724133852	-3072
1608802724133853	-3072
1608802724133980	-12582912
gradients/bert/encoder/layer_5/attention/self/transpose_grad/transpose
1608802724133889	12582912
1608802724134187	-12582912
gradients/bert/encoder/layer_5/attention/self/transpose_1_grad/transpose
1608802724133947	12582912
1608802724134402	-12582912
gradients/bert/encoder/layer_5/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_5/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_5/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724134018	3072
1608802724323823	-3072
gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul
1608802724134077	12582912
1608802724134618	-12582912
gradients/bert/encoder/layer_5/attention/self/query/MatMul_grad/MatMul_1
1608802724134130	2359296
1608802724323422	-2359296
gradients/bert/encoder/layer_5/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724134208	3072
1608802724322630	-3072
gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul
1608802724134294	12582912
1608802724134616	-12582912
gradients/bert/encoder/layer_5/attention/self/key/MatMul_grad/MatMul_1
1608802724134346	2359296
1608802724322348	-2359296
global_norm/L2Loss_86
1608802724134423	256
1608802724258086	-256
global_norm/L2Loss_85
1608802724134466	256
1608802724134480	9216
1608802724134535	-9216
1608802724258086	-256
global_norm/L2Loss_88
1608802724134551	256
1608802724258087	-256
gradients/AddN_52
global_norm/L2Loss_87
1608802724134634	256
1608802724134645	9216
1608802724134698	-9216
1608802724258087	-256
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724134723	12582912
1608802724147740	-12582912
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724134773	12582912
1608802724135415	-12582912
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724134825	12582912
1608802724135142	-12582912
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724134876	3072
1608802724322405	-3072
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_83
1608802724135022	256
1608802724258085	-256
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724135060	29696
1608802724135308	-29696
gradients/AddN_53
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724135173	12582912
1608802724135366	-12582912
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_4/output/LayerNorm/moments/mean_grad/Tile
1608802724135276	12582912
1608802724140705	-12582912
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724135331	29696
1608802724137557	-29696
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724135378	3072
1608802724322798	-3072
gradients/bert/encoder/layer_4/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_84
1608802724135489	256
1608802724258085	-256
gradients/bert/encoder/layer_4/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_4/output/LayerNorm/moments/variance_grad/Tile
1608802724137572	12582912
1608802724140703	-12582912
gradients/bert/encoder/layer_4/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_4/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_54
gradients/bert/encoder/layer_4/output/dropout/mul_1_grad/Mul
1608802724140725	12582912
1608802724141101	-12582912
gradients/bert/encoder/layer_4/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_4/output/dense/BiasAdd_grad/BiasAddGrad
1608802724141117	3072
1608802724323760	-3072
gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul
1608802724141182	50331648
1608802724146877	-50331648
gradients/bert/encoder/layer_4/output/dense/MatMul_grad/MatMul_1
1608802724141419	12582912
1608802724323366	-12582912
global_norm/L2Loss_82
1608802724141887	256
1608802724258084	-256
gradients/bert/encoder/layer_4/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_4/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_81
1608802724146889	256
1608802724146903	9216
1608802724146959	-9216
1608802724258083	-256
gradients/bert/encoder/layer_4/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_4/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_4/intermediate/dense/Pow_grad/mul_1
gradients/AddN_55
gradients/bert/encoder/layer_4/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724147167	12288
1608802724323022	-12288
gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul
1608802724147233	12582912
1608802724147432	-12582912
gradients/bert/encoder/layer_4/intermediate/dense/MatMul_grad/MatMul_1
1608802724147297	12582912
1608802724322743	-12582912
global_norm/L2Loss_80
1608802724147353	256
1608802724258083	-256
gradients/AddN_56
global_norm/L2Loss_79
1608802724147452	256
1608802724147464	14336
1608802724147518	-14336
1608802724258082	-256
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724147550	12582912
1608802724156949	-12582912
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724147599	12582912
1608802724148225	-12582912
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724147649	12582912
1608802724147963	-12582912
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724147704	3072
1608802724322966	-3072
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_77
1608802724147846	256
1608802724258081	-256
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724147885	30208
1608802724148121	-30208
gradients/AddN_57
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724147993	12582912
1608802724148177	-12582912
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724148090	12582912
1608802724149852	-12582912
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724148143	30208
1608802724148969	-30208
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724148188	3072
1608802724323312	-3072
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_78
1608802724148295	256
1608802724258082	-256
gradients/bert/encoder/layer_4/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724148982	12582912
1608802724149851	-12582912
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_4/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_58
gradients/bert/encoder/layer_4/attention/output/dropout/mul_1_grad/Mul
1608802724149871	12582912
1608802724149952	-12582912
gradients/bert/encoder/layer_4/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_4/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724149967	3072
1608802724322688	-3072
gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul
1608802724150032	12582912
1608802724150365	-12582912
gradients/bert/encoder/layer_4/attention/output/dense/MatMul_grad/MatMul_1
1608802724150094	2359296
1608802724322186	-2359296
global_norm/L2Loss_76
1608802724150173	256
1608802724258081	-256
gradients/bert/encoder/layer_4/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_75
1608802724150233	256
1608802724150246	9216
1608802724150307	-9216
1608802724258080	-256
gradients/bert/encoder/layer_4/attention/self/transpose_3_grad/transpose
1608802724150326	12582912
1608802724152220	-12582912
gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul
1608802724150386	25165824
1608802724150481	4608
1608802724150484	3072
1608802724150485	3072
1608802724151476	-4608
1608802724151478	-3072
1608802724151478	-3072
1608802724152434	-25165824
gradients/bert/encoder/layer_4/attention/self/MatMul_1_grad/MatMul_1
1608802724151515	12582912
1608802724151569	4608
1608802724151572	3072
1608802724151573	3072
1608802724152204	-4608
1608802724152206	-3072
1608802724152206	-3072
1608802724152328	-12582912
gradients/bert/encoder/layer_4/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_4/attention/self/transpose_2_grad/transpose
1608802724152294	12582912
1608802724152685	-12582912
gradients/bert/encoder/layer_4/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_4/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_4/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724152521	4608
1608802724323648	-4608
gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul
1608802724152576	12582912
1608802724156673	-12582912
gradients/bert/encoder/layer_4/attention/self/value/MatMul_grad/MatMul_1
1608802724152629	2359296
1608802724322913	-2359296
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum
1608802724152704	196608
1608802724152907	-196608
global_norm/L2Loss_74
1608802724152755	256
1608802724258080	-256
global_norm/L2Loss_73
1608802724152793	256
1608802724152805	9216
1608802724152855	-9216
1608802724258079	-256
gradients/bert/encoder/layer_4/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_4/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul
1608802724153116	12582912
1608802724153199	3072
1608802724153201	3072
1608802724153202	3072
1608802724154667	-3072
1608802724154669	-3072
1608802724154670	-3072
1608802724156047	-12582912
gradients/bert/encoder/layer_4/attention/self/MatMul_grad/MatMul_1
1608802724154704	12582912
1608802724154753	3072
1608802724154755	3072
1608802724154756	3072
1608802724155974	-3072
1608802724155975	-3072
1608802724155976	-3072
1608802724156095	-12582912
gradients/bert/encoder/layer_4/attention/self/transpose_grad/transpose
1608802724156009	12582912
1608802724156289	-12582912
gradients/bert/encoder/layer_4/attention/self/transpose_1_grad/transpose
1608802724156063	12582912
1608802724156474	-12582912
gradients/bert/encoder/layer_4/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_4/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_4/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724156130	3072
1608802724323595	-3072
gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul
1608802724156184	12582912
1608802724156672	-12582912
gradients/bert/encoder/layer_4/attention/self/query/MatMul_grad/MatMul_1
1608802724156235	2359296
1608802724322572	-2359296
gradients/bert/encoder/layer_4/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724156306	3072
1608802724322236	-3072
gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul
1608802724156370	12582912
1608802724156670	-12582912
gradients/bert/encoder/layer_4/attention/self/key/MatMul_grad/MatMul_1
1608802724156422	2359296
1608802724322131	-2359296
global_norm/L2Loss_70
1608802724156491	256
1608802724258078	-256
global_norm/L2Loss_69
1608802724156531	256
1608802724156544	9216
1608802724156596	-9216
1608802724258077	-256
global_norm/L2Loss_72
1608802724156610	256
1608802724258079	-256
gradients/AddN_59
global_norm/L2Loss_71
1608802724156688	256
1608802724156698	9216
1608802724156747	-9216
1608802724258078	-256
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724156771	12582912
1608802724169827	-12582912
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724156817	12582912
1608802724157423	-12582912
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724156865	12582912
1608802724157163	-12582912
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724156914	3072
1608802724334197	-3072
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_67
1608802724157053	256
1608802724258076	-256
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724157090	19200
1608802724157321	-19200
gradients/AddN_60
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724157195	12582912
1608802724157376	-12582912
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_3/output/LayerNorm/moments/mean_grad/Tile
1608802724157290	12582912
1608802724162803	-12582912
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724157343	19200
1608802724159687	-19200
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724157387	3072
1608802724334250	-3072
gradients/bert/encoder/layer_3/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_68
1608802724157492	256
1608802724258077	-256
gradients/bert/encoder/layer_3/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_3/output/LayerNorm/moments/variance_grad/Tile
1608802724159699	12582912
1608802724162802	-12582912
gradients/bert/encoder/layer_3/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_3/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_61
gradients/bert/encoder/layer_3/output/dropout/mul_1_grad/Mul
1608802724162820	12582912
1608802724163233	-12582912
gradients/bert/encoder/layer_3/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_3/output/dense/BiasAdd_grad/BiasAddGrad
1608802724163245	3072
1608802724323128	-3072
gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul
1608802724163306	75497472
1608802724169012	-75497472
gradients/bert/encoder/layer_3/output/dense/MatMul_grad/MatMul_1
1608802724163544	12582912
1608802724322515	-12582912
global_norm/L2Loss_66
1608802724164021	256
1608802724258076	-256
gradients/bert/encoder/layer_3/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_3/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_65
1608802724169027	256
1608802724169038	15616
1608802724169088	-15616
1608802724258075	-256
gradients/bert/encoder/layer_3/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_3/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_3/intermediate/dense/Pow_grad/mul_1
gradients/AddN_62
gradients/bert/encoder/layer_3/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724169298	15616
1608802724322289	-15616
gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul
1608802724169358	12582912
1608802724169542	-12582912
gradients/bert/encoder/layer_3/intermediate/dense/MatMul_grad/MatMul_1
1608802724169413	12582912
1608802724334142	-12582912
global_norm/L2Loss_64
1608802724169467	256
1608802724258074	-256
gradients/AddN_63
global_norm/L2Loss_63
1608802724169561	256
1608802724169572	9216
1608802724169624	-9216
1608802724258074	-256
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724169646	12582912
1608802724179069	-12582912
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724169689	12582912
1608802724170290	-12582912
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724169743	12582912
1608802724170026	-12582912
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724169795	3072
1608802724322460	-3072
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_61
1608802724169920	256
1608802724258073	-256
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724169955	28928
1608802724170168	-28928
gradients/AddN_64
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724170053	12582912
1608802724170219	-12582912
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724170139	12582912
1608802724171976	-12582912
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724170189	28928
1608802724171103	-28928
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724170230	3072
1608802724323076	-3072
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_62
1608802724170360	256
1608802724258073	-256
gradients/bert/encoder/layer_3/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724171116	12582912
1608802724171975	-12582912
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_3/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_65
gradients/bert/encoder/layer_3/attention/output/dropout/mul_1_grad/Mul
1608802724171995	12582912
1608802724172066	-12582912
gradients/bert/encoder/layer_3/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_3/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724172078	3072
1608802724334089	-3072
gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul
1608802724172134	12582912
1608802724172466	-12582912
gradients/bert/encoder/layer_3/attention/output/dense/MatMul_grad/MatMul_1
1608802724172189	3145728
1608802724332102	-3145728
global_norm/L2Loss_60
1608802724172259	256
1608802724258072	-256
gradients/bert/encoder/layer_3/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_59
1608802724172313	256
1608802724172325	9216
1608802724172390	-9216
1608802724258072	-256
gradients/bert/encoder/layer_3/attention/self/transpose_3_grad/transpose
1608802724172408	12582912
1608802724174407	-12582912
gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul
1608802724172485	25165824
1608802724172557	3072
1608802724172560	3072
1608802724172561	3072
1608802724173626	-3072
1608802724173627	-3072
1608802724173629	-3072
1608802724174624	-25165824
gradients/bert/encoder/layer_3/attention/self/MatMul_1_grad/MatMul_1
1608802724173663	12582912
1608802724173709	3072
1608802724173711	3072
1608802724173712	3072
1608802724174393	-3072
1608802724174394	-3072
1608802724174396	-3072
1608802724174503	-12582912
gradients/bert/encoder/layer_3/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_3/attention/self/transpose_2_grad/transpose
1608802724174473	12582912
1608802724174824	-12582912
gradients/bert/encoder/layer_3/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_3/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_3/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724174679	3072
1608802724326977	-3072
gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul
1608802724174729	12582912
1608802724178819	-12582912
gradients/bert/encoder/layer_3/attention/self/value/MatMul_grad/MatMul_1
1608802724174775	2359296
1608802724326253	-2359296
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum
1608802724174842	196608
1608802724175120	-196608
global_norm/L2Loss_58
1608802724174896	256
1608802724258071	-256
global_norm/L2Loss_57
1608802724174932	256
1608802724174943	9216
1608802724174990	-9216
1608802724258071	-256
gradients/bert/encoder/layer_3/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_3/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul
1608802724175325	12582912
1608802724175412	3072
1608802724175414	3072
1608802724175416	3072
1608802724176887	-3072
1608802724176889	-3072
1608802724176890	-3072
1608802724178261	-12582912
gradients/bert/encoder/layer_3/attention/self/MatMul_grad/MatMul_1
1608802724176920	12582912
1608802724176964	3072
1608802724176966	3072
1608802724176967	3072
1608802724178192	-3072
1608802724178193	-3072
1608802724178194	-3072
1608802724178303	-12582912
gradients/bert/encoder/layer_3/attention/self/transpose_grad/transpose
1608802724178227	12582912
1608802724178481	-12582912
gradients/bert/encoder/layer_3/attention/self/transpose_1_grad/transpose
1608802724178276	12582912
1608802724178636	-12582912
gradients/bert/encoder/layer_3/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_3/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_3/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724178337	3072
1608802724326198	-3072
gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul
1608802724178386	12582912
1608802724178819	-12582912
gradients/bert/encoder/layer_3/attention/self/query/MatMul_grad/MatMul_1
1608802724178433	2359296
1608802724325517	-2359296
gradients/bert/encoder/layer_3/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724178498	3072
1608802724325578	-3072
gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul
1608802724178543	12582912
1608802724178817	-12582912
gradients/bert/encoder/layer_3/attention/self/key/MatMul_grad/MatMul_1
1608802724178589	2359296
1608802724333204	-2359296
global_norm/L2Loss_54
1608802724178654	256
1608802724258069	-256
global_norm/L2Loss_53
1608802724178691	256
1608802724178702	15104
1608802724178748	-15104
1608802724258069	-256
global_norm/L2Loss_56
1608802724178762	256
1608802724258070	-256
gradients/AddN_66
global_norm/L2Loss_55
1608802724178832	256
1608802724178841	14592
1608802724178884	-14592
1608802724258070	-256
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724178908	12582912
1608802724192034	-12582912
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724178949	12582912
1608802724179510	-12582912
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724178993	12582912
1608802724179261	-12582912
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724179038	3072
1608802724333148	-3072
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_51
1608802724179163	256
1608802724258068	-256
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724179195	16384
1608802724179406	-16384
gradients/AddN_67
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724179291	12582912
1608802724179457	-12582912
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_2/output/LayerNorm/moments/mean_grad/Tile
1608802724179379	12582912
1608802724185019	-12582912
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724179427	16384
1608802724181904	-16384
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724179467	3072
1608802724333981	-3072
gradients/bert/encoder/layer_2/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_52
1608802724179580	256
1608802724258068	-256
gradients/bert/encoder/layer_2/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_2/output/LayerNorm/moments/variance_grad/Tile
1608802724181916	12582912
1608802724185018	-12582912
gradients/bert/encoder/layer_2/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_2/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_68
gradients/bert/encoder/layer_2/output/dropout/mul_1_grad/Mul
1608802724185035	12582912
1608802724185458	-12582912
gradients/bert/encoder/layer_2/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_2/output/dense/BiasAdd_grad/BiasAddGrad
1608802724185472	3072
1608802724333437	-3072
gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul
1608802724185530	78136320
1608802724191231	-78136320
gradients/bert/encoder/layer_2/output/dense/MatMul_grad/MatMul_1
1608802724185768	12582912
1608802724333384	-12582912
global_norm/L2Loss_50
1608802724186239	256
1608802724258067	-256
gradients/bert/encoder/layer_2/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_2/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_49
1608802724191242	256
1608802724191253	9216
1608802724191306	-9216
1608802724258067	-256
gradients/bert/encoder/layer_2/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_2/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_2/intermediate/dense/Pow_grad/mul_1
gradients/AddN_69
gradients/bert/encoder/layer_2/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724191525	12288
1608802724333330	-12288
gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul
1608802724191588	12582912
1608802724191759	-12582912
gradients/bert/encoder/layer_2/intermediate/dense/MatMul_grad/MatMul_1
1608802724191638	12582912
1608802724327663	-12582912
global_norm/L2Loss_48
1608802724191691	256
1608802724258066	-256
gradients/AddN_70
global_norm/L2Loss_47
1608802724191774	256
1608802724191784	9216
1608802724191832	-9216
1608802724258065	-256
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724191853	12582912
1608802724201205	-12582912
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724191908	12582912
1608802724192512	-12582912
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724191965	12582912
1608802724192235	-12582912
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724192006	3840
1608802724333055	-3840
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_45
1608802724192140	256
1608802724258064	-256
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724192171	16384
1608802724192378	-16384
gradients/AddN_71
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724192263	12582912
1608802724192429	-12582912
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724192343	12582912
1608802724194199	-12582912
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724192399	16384
1608802724193326	-16384
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724192438	3072
1608802724333868	-3072
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_46
1608802724192582	256
1608802724258065	-256
gradients/bert/encoder/layer_2/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724193336	12582912
1608802724194198	-12582912
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_2/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_72
gradients/bert/encoder/layer_2/attention/output/dropout/mul_1_grad/Mul
1608802724194214	12582912
1608802724194281	-12582912
gradients/bert/encoder/layer_2/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_2/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724194293	3072
1608802724331210	-3072
gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul
1608802724194344	12582912
1608802724194683	-12582912
gradients/bert/encoder/layer_2/attention/output/dense/MatMul_grad/MatMul_1
1608802724194395	2359296
1608802724328444	-2359296
global_norm/L2Loss_44
1608802724194461	256
1608802724258064	-256
gradients/bert/encoder/layer_2/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_43
1608802724194511	256
1608802724194521	12800
1608802724194571	-12800
1608802724258063	-256
gradients/bert/encoder/layer_2/attention/self/transpose_3_grad/transpose
1608802724194585	12582912
1608802724196580	-12582912
gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul
1608802724194701	25165824
1608802724194781	3072
1608802724194783	3072
1608802724194784	3072
1608802724195842	-3072
1608802724195843	-3072
1608802724195845	-3072
1608802724196801	-25165824
gradients/bert/encoder/layer_2/attention/self/MatMul_1_grad/MatMul_1
1608802724195874	12582912
1608802724195916	3072
1608802724195918	3072
1608802724195919	3072
1608802724196567	-3072
1608802724196568	-3072
1608802724196570	-3072
1608802724196667	-12582912
gradients/bert/encoder/layer_2/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_2/attention/self/transpose_2_grad/transpose
1608802724196640	12582912
1608802724196984	-12582912
gradients/bert/encoder/layer_2/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_2/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_2/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724196851	3072
1608802724331819	-3072
gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul
1608802724196895	12582912
1608802724200958	-12582912
gradients/bert/encoder/layer_2/attention/self/value/MatMul_grad/MatMul_1
1608802724196940	3145728
1608802724330262	-3145728
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum
1608802724197000	196608
1608802724197298	-196608
global_norm/L2Loss_42
1608802724197073	256
1608802724258063	-256
global_norm/L2Loss_41
1608802724197108	256
1608802724197117	9216
1608802724197160	-9216
1608802724258063	-256
gradients/bert/encoder/layer_2/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_2/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul
1608802724197504	12582912
1608802724197578	3072
1608802724197580	3072
1608802724197581	3072
1608802724199067	-3072
1608802724199068	-3072
1608802724199069	-3072
1608802724200439	-12582912
gradients/bert/encoder/layer_2/attention/self/MatMul_grad/MatMul_1
1608802724199098	12582912
1608802724199139	3072
1608802724199141	3072
1608802724199142	3072
1608802724200377	-3072
1608802724200378	-3072
1608802724200379	-3072
1608802724200478	-12582912
gradients/bert/encoder/layer_2/attention/self/transpose_grad/transpose
1608802724200408	12582912
1608802724200653	-12582912
gradients/bert/encoder/layer_2/attention/self/transpose_1_grad/transpose
1608802724200453	12582912
1608802724200793	-12582912
gradients/bert/encoder/layer_2/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_2/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_2/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724200511	3072
1608802724327493	-3072
gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul
1608802724200558	12582912
1608802724200957	-12582912
gradients/bert/encoder/layer_2/attention/self/query/MatMul_grad/MatMul_1
1608802724200611	2359296
1608802724326703	-2359296
gradients/bert/encoder/layer_2/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724200666	3072
1608802724329297	-3072
gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul
1608802724200709	12582912
1608802724200956	-12582912
gradients/bert/encoder/layer_2/attention/self/key/MatMul_grad/MatMul_1
1608802724200751	2359296
1608802724324735	-2359296
global_norm/L2Loss_38
1608802724200810	256
1608802724258061	-256
global_norm/L2Loss_37
1608802724200843	256
1608802724200853	9216
1608802724200894	-9216
1608802724258061	-256
global_norm/L2Loss_40
1608802724200906	256
1608802724258062	-256
gradients/AddN_73
global_norm/L2Loss_39
1608802724200969	256
1608802724200977	9216
1608802724201022	-9216
1608802724258062	-256
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724201042	12582912
1608802724214196	-12582912
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724201080	12582912
1608802724201678	-12582912
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724201138	12582912
1608802724201409	-12582912
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724201177	3072
1608802724331765	-3072
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_35
1608802724201315	256
1608802724258060	-256
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724201345	23552
1608802724201536	-23552
gradients/AddN_74
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724201434	12582912
1608802724201589	-12582912
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_1/output/LayerNorm/moments/mean_grad/Tile
1608802724201511	12582912
1608802724207189	-12582912
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724201562	23552
1608802724204084	-23552
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724201600	3072
1608802724325973	-3072
gradients/bert/encoder/layer_1/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_36
1608802724201748	256
1608802724258060	-256
gradients/bert/encoder/layer_1/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_1/output/LayerNorm/moments/variance_grad/Tile
1608802724204094	12582912
1608802724207188	-12582912
gradients/bert/encoder/layer_1/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_75
gradients/bert/encoder/layer_1/output/dropout/mul_1_grad/Mul
1608802724207204	12582912
1608802724207627	-12582912
gradients/bert/encoder/layer_1/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_1/output/dense/BiasAdd_grad/BiasAddGrad
1608802724207638	3072
1608802724325911	-3072
gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul
1608802724207689	50331648
1608802724213396	-50331648
gradients/bert/encoder/layer_1/output/dense/MatMul_grad/MatMul_1
1608802724207933	12582912
1608802724334036	-12582912
global_norm/L2Loss_34
1608802724208406	256
1608802724258059	-256
gradients/bert/encoder/layer_1/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_1/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_33
1608802724213407	256
1608802724213418	9216
1608802724213471	-9216
1608802724258059	-256
gradients/bert/encoder/layer_1/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_1/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_1/intermediate/dense/Pow_grad/mul_1
gradients/AddN_76
gradients/bert/encoder/layer_1/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724213691	20480
1608802724332909	-20480
gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul
1608802724213741	12582912
1608802724213922	-12582912
gradients/bert/encoder/layer_1/intermediate/dense/MatMul_grad/MatMul_1
1608802724213788	12582912
1608802724325181	-12582912
global_norm/L2Loss_32
1608802724213855	256
1608802724258058	-256
gradients/AddN_77
global_norm/L2Loss_31
1608802724213936	256
1608802724213945	9216
1608802724213998	-9216
1608802724258057	-256
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724214016	12582912
1608802724223345	-12582912
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724214074	12582912
1608802724214686	-12582912
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724214129	20447232
1608802724214397	-20447232
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724214169	3072
1608802724333655	-3072
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_29
1608802724214307	256
1608802724258056	-256
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724214336	32000
1608802724214516	-32000
gradients/AddN_78
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724214419	12582912
1608802724214577	-12582912
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724214492	20447232
1608802724216373	-20447232
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724214533	32000
1608802724215491	-32000
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724214586	3072
1608802724333815	-3072
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_30
1608802724214745	256
1608802724258057	-256
gradients/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724215500	12582912
1608802724216372	-12582912
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_79
gradients/bert/encoder/layer_1/attention/output/dropout/mul_1_grad/Mul
1608802724216383	12582912
1608802724216437	-12582912
gradients/bert/encoder/layer_1/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_1/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724216448	3072
1608802724329138	-3072
gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul
1608802724216502	12582912
1608802724216844	-12582912
gradients/bert/encoder/layer_1/attention/output/dense/MatMul_grad/MatMul_1
1608802724216543	2359296
1608802724333927	-2359296
global_norm/L2Loss_28
1608802724216600	256
1608802724258056	-256
gradients/bert/encoder/layer_1/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_27
1608802724216639	256
1608802724216648	9216
1608802724216734	-9216
1608802724258055	-256
gradients/bert/encoder/layer_1/attention/self/transpose_3_grad/transpose
1608802724216747	12582912
1608802724218735	-12582912
gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul
1608802724216862	37748736
1608802724216924	3072
1608802724216926	3072
1608802724216927	3072
1608802724218006	-3072
1608802724218007	-3072
1608802724218008	-3072
1608802724218968	-37748736
gradients/bert/encoder/layer_1/attention/self/MatMul_1_grad/MatMul_1
1608802724218034	12582912
1608802724218072	3072
1608802724218074	3072
1608802724218075	3072
1608802724218723	-3072
1608802724218724	-3072
1608802724218725	-3072
1608802724218822	-12582912
gradients/bert/encoder/layer_1/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_1/attention/self/transpose_2_grad/transpose
1608802724218790	20447232
1608802724219141	-20447232
gradients/bert/encoder/layer_1/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_1/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_1/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724219015	3072
1608802724333763	-3072
gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul
1608802724219055	12582912
1608802724223085	-12582912
gradients/bert/encoder/layer_1/attention/self/value/MatMul_grad/MatMul_1
1608802724219099	2359296
1608802724333596	-2359296
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum
1608802724219155	196608
1608802724219441	-196608
global_norm/L2Loss_26
1608802724219223	256
1608802724258055	-256
global_norm/L2Loss_25
1608802724219252	256
1608802724219261	9216
1608802724219301	-9216
1608802724258055	-256
gradients/bert/encoder/layer_1/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_1/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul
1608802724219642	20447232
1608802724219715	3072
1608802724219717	3072
1608802724219718	3072
1608802724221211	-3072
1608802724221213	-3072
1608802724221214	-3072
1608802724222566	-20447232
gradients/bert/encoder/layer_1/attention/self/MatMul_grad/MatMul_1
1608802724221240	12582912
1608802724221278	3072
1608802724221280	3072
1608802724221281	3072
1608802724222510	-3072
1608802724222511	-3072
1608802724222512	-3072
1608802724222614	-12582912
gradients/bert/encoder/layer_1/attention/self/transpose_grad/transpose
1608802724222537	12582912
1608802724222774	-12582912
gradients/bert/encoder/layer_1/attention/self/transpose_1_grad/transpose
1608802724222579	20447232
1608802724222905	-20447232
gradients/bert/encoder/layer_1/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_1/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_1/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724222646	3072
1608802724333708	-3072
gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul
1608802724222692	12582912
1608802724223084	-12582912
gradients/bert/encoder/layer_1/attention/self/query/MatMul_grad/MatMul_1
1608802724222732	2359296
1608802724333544	-2359296
gradients/bert/encoder/layer_1/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724222787	3072
1608802724331709	-3072
gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul
1608802724222828	12582912
1608802724223083	-12582912
gradients/bert/encoder/layer_1/attention/self/key/MatMul_grad/MatMul_1
1608802724222864	2359296
1608802724325069	-2359296
global_norm/L2Loss_22
1608802724222920	256
1608802724258053	-256
global_norm/L2Loss_21
1608802724222972	256
1608802724222981	18176
1608802724223020	-18176
1608802724258052	-256
global_norm/L2Loss_24
1608802724223032	256
1608802724258054	-256
gradients/AddN_80
global_norm/L2Loss_23
1608802724223094	256
1608802724223101	17664
1608802724223150	-17664
1608802724258053	-256
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724223168	12582912
1608802724236344	-12582912
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724223225	20447232
1608802724223826	-20447232
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724223281	12582912
1608802724223543	-12582912
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724223318	3072
1608802724332209	-3072
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_19
1608802724223457	256
1608802724258051	-256
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724223486	16384
1608802724223661	-16384
gradients/AddN_81
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724223565	12582912
1608802724223736	-12582912
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_0/output/LayerNorm/moments/mean_grad/Tile
1608802724223637	12582912
1608802724229323	-12582912
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724223679	16384
1608802724226219	-16384
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724223746	3072
1608802724329973	-3072
gradients/bert/encoder/layer_0/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_20
1608802724223898	256
1608802724258052	-256
gradients/bert/encoder/layer_0/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_0/output/LayerNorm/moments/variance_grad/Tile
1608802724226230	12582912
1608802724229322	-12582912
gradients/bert/encoder/layer_0/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_82
gradients/bert/encoder/layer_0/output/dropout/mul_1_grad/Mul
1608802724229337	12582912
1608802724229774	-12582912
gradients/bert/encoder/layer_0/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_0/output/dense/BiasAdd_grad/BiasAddGrad
1608802724229784	3072
1608802724333490	-3072
gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul
1608802724229831	62914560
1608802724235554	-62914560
gradients/bert/encoder/layer_0/output/dense/MatMul_grad/MatMul_1
1608802724230077	12582912
1608802724325296	-12582912
global_norm/L2Loss_18
1608802724230559	256
1608802724258051	-256
gradients/bert/encoder/layer_0/intermediate/dense/mul_3_grad/Mul
gradients/bert/encoder/layer_0/intermediate/dense/mul_2_grad/Mul_1
global_norm/L2Loss_17
1608802724235564	256
1608802724235574	9216
1608802724235624	-9216
1608802724258050	-256
gradients/bert/encoder/layer_0/intermediate/dense/Tanh_grad/TanhGrad
gradients/bert/encoder/layer_0/intermediate/dense/mul_1_grad/Mul_1
gradients/bert/encoder/layer_0/intermediate/dense/Pow_grad/mul_1
gradients/AddN_83
gradients/bert/encoder/layer_0/intermediate/dense/BiasAdd_grad/BiasAddGrad
1608802724235841	12288
1608802724324959	-12288
gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul
1608802724235886	12582912
1608802724236075	-12582912
gradients/bert/encoder/layer_0/intermediate/dense/MatMul_grad/MatMul_1
1608802724235935	9437184
1608802724324677	-9437184
global_norm/L2Loss_16
1608802724236023	256
1608802724258050	-256
gradients/AddN_84
global_norm/L2Loss_15
1608802724236088	256
1608802724236096	9216
1608802724236150	-9216
1608802724258049	-256
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724236168	12582912
1608802724247395	-12582912
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724236225	12582912
1608802724236826	-12582912
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Neg
1608802724236280	12582912
1608802724236540	-12582912
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub_grad/Sum
1608802724236319	3072
1608802724324900	-3072
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_13
1608802724236456	256
1608802724258048	-256
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724236485	16384
1608802724236666	-16384
gradients/AddN_85
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Mul
1608802724236563	12582912
1608802724236727	-12582912
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean_grad/Tile
1608802724236644	12582912
1608802724238555	-12582912
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum
1608802724236684	16384
1608802724237639	-16384
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724236735	3584
1608802724324443	-3584
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean_grad/truediv
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_14
1608802724236896	256
1608802724258049	-256
gradients/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance_grad/Tile
1608802724237649	12582912
1608802724238554	-12582912
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance_grad/truediv
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_86
gradients/bert/encoder/layer_0/attention/output/dropout/mul_1_grad/Mul
1608802724238570	12582912
1608802724238625	-12582912
gradients/bert/encoder/layer_0/attention/output/dropout/mul_grad/Mul
gradients/bert/encoder/layer_0/attention/output/dense/BiasAdd_grad/BiasAddGrad
1608802724238635	3072
1608802724324618	-3072
gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul
1608802724238696	12582912
1608802724239031	-12582912
gradients/bert/encoder/layer_0/attention/output/dense/MatMul_grad/MatMul_1
1608802724238738	3145728
1608802724324388	-3145728
global_norm/L2Loss_12
1608802724238792	256
1608802724258048	-256
gradients/bert/encoder/layer_0/attention/self/Reshape_3_grad/Reshape
global_norm/L2Loss_11
1608802724238834	256
1608802724238843	12800
1608802724238921	-12800
1608802724258047	-256
gradients/bert/encoder/layer_0/attention/self/transpose_3_grad/transpose
1608802724238933	12582912
1608802724240922	-12582912
gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul
1608802724239045	25165824
1608802724239105	3072
1608802724239107	3072
1608802724239108	3072
1608802724240195	-3072
1608802724240196	-3072
1608802724240197	-3072
1608802724241149	-25165824
gradients/bert/encoder/layer_0/attention/self/MatMul_1_grad/MatMul_1
1608802724240221	12582912
1608802724240256	3072
1608802724240258	3072
1608802724240258	3072
1608802724240911	-3072
1608802724240912	-3072
1608802724240913	-3072
1608802724241004	-12582912
gradients/bert/encoder/layer_0/attention/self/dropout/mul_1_grad/Mul
gradients/bert/encoder/layer_0/attention/self/transpose_2_grad/transpose
1608802724240970	12582912
1608802724241308	-12582912
gradients/bert/encoder/layer_0/attention/self/dropout/mul_grad/Mul
gradients/bert/encoder/layer_0/attention/self/Reshape_2_grad/Reshape
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul
gradients/bert/encoder/layer_0/attention/self/value/BiasAdd_grad/BiasAddGrad
1608802724241193	3072
1608802724324213	-3072
gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul
1608802724241231	12582912
1608802724247281	-12582912
gradients/bert/encoder/layer_0/attention/self/value/MatMul_grad/MatMul_1
1608802724241267	2359296
1608802724324562	-2359296
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum
1608802724241321	196608
1608802724241644	-196608
global_norm/L2Loss_10
1608802724241413	256
1608802724258047	-256
global_norm/L2Loss_9
1608802724241442	256
1608802724241451	9216
1608802724241488	-9216
1608802724258047	-256
gradients/bert/encoder/layer_0/attention/self/Softmax_grad/sub
gradients/bert/encoder/layer_0/attention/self/Mul_grad/Mul
gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul
1608802724241844	12582912
1608802724241905	3072
1608802724241906	3072
1608802724241907	3072
1608802724243406	-3072
1608802724243407	-3072
1608802724243408	-3072
1608802724244759	-12582912
gradients/bert/encoder/layer_0/attention/self/MatMul_grad/MatMul_1
1608802724243431	23068672
1608802724243465	3072
1608802724243467	3072
1608802724243467	3072
1608802724244705	-3072
1608802724244706	-3072
1608802724244707	-3072
1608802724244810	-23068672
gradients/bert/encoder/layer_0/attention/self/transpose_grad/transpose
1608802724244732	12582912
1608802724244952	-12582912
gradients/bert/encoder/layer_0/attention/self/transpose_1_grad/transpose
1608802724244771	12582912
1608802724245070	-12582912
gradients/bert/encoder/layer_0/attention/self/Reshape_grad/Reshape
gradients/bert/encoder/layer_0/attention/self/Reshape_1_grad/Reshape
gradients/bert/encoder/layer_0/attention/self/query/BiasAdd_grad/BiasAddGrad
1608802724244836	3072
1608802724324041	-3072
gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul
1608802724244876	12582912
1608802724247280	-12582912
gradients/bert/encoder/layer_0/attention/self/query/MatMul_grad/MatMul_1
1608802724244913	2359296
1608802724324269	-2359296
gradients/bert/encoder/layer_0/attention/self/key/BiasAdd_grad/BiasAddGrad
1608802724244964	3072
1608802724324320	-3072
gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul
1608802724245000	12582912
1608802724247279	-12582912
gradients/bert/encoder/layer_0/attention/self/key/MatMul_grad/MatMul_1
1608802724245034	2359296
1608802724324159	-2359296
global_norm/L2Loss_6
1608802724245084	256
1608802724258045	-256
global_norm/L2Loss_5
1608802724245114	256
1608802724245122	9216
1608802724247196	-9216
1608802724258045	-256
global_norm/L2Loss_8
1608802724247219	256
1608802724258046	-256
gradients/AddN_87
global_norm/L2Loss_7
1608802724247297	256
1608802724247306	9216
1608802724247344	-9216
1608802724258046	-256
gradients/bert/encoder/Reshape_1_grad/Reshape
gradients/bert/embeddings/dropout/mul_grad/Mul
gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul
1608802724247408	12582912
1608802724255233	-12582912
gradients/bert/embeddings/LayerNorm/batchnorm/mul_1_grad/Mul_1
1608802724247450	23068672
1608802724247890	-23068672
gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Neg
1608802724247486	12582912
1608802724247705	-12582912
gradients/bert/embeddings/LayerNorm/batchnorm/sub_grad/Sum
1608802724247521	3072
1608802724323989	-3072
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Mul
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Mul_1
global_norm/L2Loss_3
1608802724247621	256
1608802724258044	-256
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Sum
1608802724247650	29696
1608802724247821	-29696
gradients/AddN_88
gradients/bert/embeddings/LayerNorm/batchnorm/mul_2_grad/Reshape
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Mul
1608802724247727	12582912
1608802724247855	-12582912
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Mul_1
gradients/bert/embeddings/LayerNorm/moments/mean_grad/Tile
1608802724247791	12582912
1608802724251770	-12582912
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum
1608802724247833	29696
1608802724250889	-29696
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Sum_1
1608802724247868	3072
1608802724324105	-3072
gradients/bert/embeddings/LayerNorm/moments/mean_grad/truediv
gradients/bert/embeddings/LayerNorm/batchnorm/mul_grad/Reshape
global_norm/L2Loss_4
1608802724247942	256
1608802724258044	-256
gradients/bert/embeddings/LayerNorm/batchnorm/Rsqrt_grad/RsqrtGrad
gradients/bert/embeddings/LayerNorm/moments/variance_grad/Tile
1608802724250903	12582912
1608802724251770	-12582912
gradients/bert/embeddings/LayerNorm/moments/variance_grad/truediv
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/scalar
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/sub
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/Mul
gradients/bert/embeddings/LayerNorm/moments/SquaredDifference_grad/mul_1
gradients/AddN_89
gradients/bert/embeddings/add_1_grad/Sum
1608802724251781	393216
1608802724252249	-393216
gradients/bert/embeddings/Reshape_1_grad/Reshape
gradients/bert/embeddings/Slice_grad/Pad
1608802724251994	1572864
1608802724323933	-1572864
gradients/bert/embeddings/MatMul_grad/MatMul_1
1608802724252261	6144
1608802724331535	-6144
gradients/AddN_90/inputs_1
1608802724252324	93769728
1608802724323875	-93769728
global_norm/L2Loss_2
1608802724255252	256
1608802724255262	16384
1608802724257719	-16384
1608802724258044	-256
global_norm/L2Loss_1
1608802724257732	256
1608802724258043	-256
gradients/AddN_90
global_norm/L2Loss
1608802724257842	256
1608802724257850	16384
1608802724257891	-16384
1608802724258043	-256
global_norm/stack
1608802724257922	1024
1608802724257952	1792
1608802724258030	-1792
1608802724258249	-1024
global_norm/Sum
1608802724258220	256
1608802724290554	-256
global_norm/mul
global_norm/global_norm
clip_by_global_norm/IsFinite
1608802724258319	256
1608802724258431	-256
clip_by_global_norm/truediv
clip_by_global_norm/Minimum
clip_by_global_norm/mul
clip_by_global_norm/Select
clip_by_global_norm/mul_72
clip_by_global_norm/mul_76
clip_by_global_norm/mul_73
clip_by_global_norm/mul_65
clip_by_global_norm/mul_88
clip_by_global_norm/mul_84
clip_by_global_norm/mul_62
clip_by_global_norm/mul_66
clip_by_global_norm/mul_70
clip_by_global_norm/mul_89
clip_by_global_norm/mul_77
clip_by_global_norm/mul_80
clip_by_global_norm/mul_85
clip_by_global_norm/mul_90
clip_by_global_norm/mul_74
clip_by_global_norm/mul_78
clip_by_global_norm/mul_81
clip_by_global_norm/mul_63
clip_by_global_norm/mul_67
clip_by_global_norm/mul_92
clip_by_global_norm/mul_96
clip_by_global_norm/mul_79
clip_by_global_norm/mul_82
clip_by_global_norm/mul_86
clip_by_global_norm/mul_91
clip_by_global_norm/mul_93
clip_by_global_norm/mul_71
clip_by_global_norm/mul_75
clip_by_global_norm/mul_104
clip_by_global_norm/mul_83
clip_by_global_norm/mul_87
clip_by_global_norm/mul_1
clip_by_global_norm/mul_3
clip_by_global_norm/mul_4
clip_by_global_norm/mul_7
clip_by_global_norm/mul_5
clip_by_global_norm/mul_8
clip_by_global_norm/mul_11
clip_by_global_norm/mul_6
clip_by_global_norm/mul_9
clip_by_global_norm/mul_12
clip_by_global_norm/mul_15
clip_by_global_norm/mul_100
clip_by_global_norm/mul_10
clip_by_global_norm/mul_13
clip_by_global_norm/mul_16
clip_by_global_norm/mul_40
clip_by_global_norm/mul_119
clip_by_global_norm/mul_123
clip_by_global_norm/mul_14
clip_by_global_norm/mul_17
clip_by_global_norm/mul_103
clip_by_global_norm/mul_24
clip_by_global_norm/mul_107
clip_by_global_norm/mul_32
clip_by_global_norm/mul_115
clip_by_global_norm/mul_18
clip_by_global_norm/mul_124
clip_by_global_norm/mul_127
clip_by_global_norm/mul_94
clip_by_global_norm/mul_54
clip_by_global_norm/mul_57
clip_by_global_norm/mul_105
clip_by_global_norm/mul_108
clip_by_global_norm/mul_111
clip_by_global_norm/mul_116
clip_by_global_norm/mul_120
clip_by_global_norm/mul_35
clip_by_global_norm/mul_37
clip_by_global_norm/mul_131
clip_by_global_norm/mul_95
clip_by_global_norm/mul_97
clip_by_global_norm/mul_55
clip_by_global_norm/mul_58
clip_by_global_norm/mul_151
clip_by_global_norm/mul_112
clip_by_global_norm/mul_117
clip_by_global_norm/mul_121
clip_by_global_norm/mul_125
clip_by_global_norm/mul_128
clip_by_global_norm/mul_132
clip_by_global_norm/mul_38
clip_by_global_norm/mul_139
clip_by_global_norm/mul_98
clip_by_global_norm/mul_101
clip_by_global_norm/mul_152
clip_by_global_norm/mul_59
clip_by_global_norm/mul_113
clip_by_global_norm/mul_118
clip_by_global_norm/mul_122
clip_by_global_norm/mul_126
clip_by_global_norm/mul_129
clip_by_global_norm/mul_133
clip_by_global_norm/mul_135
clip_by_global_norm/mul_140
clip_by_global_norm/mul_39
clip_by_global_norm/mul_99
clip_by_global_norm/mul_102
clip_by_global_norm/mul_48
clip_by_global_norm/mul_109
clip_by_global_norm/mul_114
clip_by_global_norm/mul_166
clip_by_global_norm/mul_170
clip_by_global_norm/mul_174
clip_by_global_norm/mul_130
clip_by_global_norm/mul_182
clip_by_global_norm/mul_136
clip_by_global_norm/mul_141
clip_by_global_norm/mul_143
clip_by_global_norm/mul_147
clip_by_global_norm/mul_153
clip_by_global_norm/mul_154
clip_by_global_norm/mul_44
clip_by_global_norm/mul_110
clip_by_global_norm/mul_167
clip_by_global_norm/mul_171
clip_by_global_norm/mul_175
clip_by_global_norm/mul_178
clip_by_global_norm/mul_183
clip_by_global_norm/mul_187
clip_by_global_norm/mul_137
clip_by_global_norm/mul_142
clip_by_global_norm/mul_144
clip_by_global_norm/mul_148
clip_by_global_norm/mul_29
clip_by_global_norm/mul_155
clip_by_global_norm/mul_158
clip_by_global_norm/mul_41
clip_by_global_norm/mul_168
clip_by_global_norm/mul_172
clip_by_global_norm/mul_176
clip_by_global_norm/mul_179
clip_by_global_norm/mul_185
clip_by_global_norm/mul_188
clip_by_global_norm/mul_134
clip_by_global_norm/mul_138
clip_by_global_norm/mul_199
clip_by_global_norm/mul_145
clip_by_global_norm/mul_149
clip_by_global_norm/mul_21
clip_by_global_norm/mul_156
clip_by_global_norm/mul_159
clip_by_global_norm/mul_162
clip_by_global_norm/mul_106
clip_by_global_norm/mul_42
clip_by_global_norm/mul_177
clip_by_global_norm/mul_180
clip_by_global_norm/mul_186
clip_by_global_norm/mul_189
clip_by_global_norm/mul_191
clip_by_global_norm/mul_195
clip_by_global_norm/mul_200
clip_by_global_norm/mul_203
clip_by_global_norm/mul_146
clip_by_global_norm/mul_150
clip_by_global_norm/mul_157
clip_by_global_norm/mul_160
clip_by_global_norm/mul_163
clip_by_global_norm/mul_169
clip_by_global_norm/mul_173
clip_by_global_norm/mul_181
clip_by_global_norm/mul_45
clip_by_global_norm/mul_190
clip_by_global_norm/mul_192
clip_by_global_norm/mul_196
clip_by_global_norm/mul_201
clip_by_global_norm/mul_204
clip_by_global_norm/mul_2
clip_by_global_norm/mul_161
clip_by_global_norm/mul_164
clip_by_global_norm/mul_25
clip_by_global_norm/mul_36
clip_by_global_norm/mul_43
clip_by_global_norm/mul_193
clip_by_global_norm/mul_197
clip_by_global_norm/mul_202
clip_by_global_norm/mul_205
clip_by_global_norm/mul_60
clip_by_global_norm/mul_165
clip_by_global_norm/mul_20
clip_by_global_norm/mul_33
clip_by_global_norm/mul_194
clip_by_global_norm/mul_198
clip_by_global_norm/mul_46
clip_by_global_norm/mul_206
clip_by_global_norm/mul_52
clip_by_global_norm/mul_56
clip_by_global_norm/mul_184
clip_by_global_norm/mul_49
clip_by_global_norm/mul_50
clip_by_global_norm/mul_51
clip_by_global_norm/mul_19
clip_by_global_norm/mul_22
clip_by_global_norm/mul_26
clip_by_global_norm/mul_30
clip_by_global_norm/mul_23
clip_by_global_norm/mul_27
clip_by_global_norm/mul_31
clip_by_global_norm/mul_47
clip_by_global_norm/mul_28
clip_by_global_norm/mul_53
clip_by_global_norm/mul_34
clip_by_global_norm/mul_61
clip_by_global_norm/mul_64
clip_by_global_norm/mul_68
clip_by_global_norm/mul_69
Square_71
1608802724290572	2359296
1608802724334276	-2359296
Mul_388
Square_75
1608802724292851	2359296
1608802724334362	-2359296
Mul_410
Square_72
1608802724295353	3072
1608802724334433	-3072
Mul_394
Square_64
1608802724295887	16640
1608802724334498	-16640
Mul_351
Square_87
1608802724295946	2359296
1608802724334568	-2359296
Mul_474
Square_83
1608802724296409	3072
1608802724334650	-3072
Mul_453
Square_61
1608802724297045	3072
1608802724334716	-3072
Mul_335
Square_65
1608802724299664	12582912
1608802724334792	-12582912
Mul_356
Square_69
1608802724302177	2359296
1608802724334855	-2359296
Mul_377
Square_88
1608802724302253	3072
1608802724334917	-3072
Mul_480
Square_76
1608802724302324	3328
1608802724334988	-3328
Mul_416
Square_79
1608802724302450	9437184
1608802724335057	-9437184
Mul_431
Square_84
1608802724302602	3072
1608802724335128	-3072
Mul_458
Square_89
1608802724302676	3145728
1608802724335190	-3145728
Mul_485
Square_73
1608802724302771	2359296
1608802724335251	-2359296
Mul_399
Square_77
1608802724302875	3072
1608802724335310	-3072
Mul_421
Square_80
1608802724302938	17408
1608802724335379	-17408
Mul_437
Square_62
1608802724303059	3072
1608802724335444	-3072
Mul_340
Square_66
1608802724303117	3072
1608802724335507	-3072
Mul_362
Square_91
1608802724303173	2359296
1608802724335568	-2359296
Mul_496
Mul_517
1608802724303256	9437184
1608802724323228	-9437184
Square_95
Square_78
1608802724303430	3072
1608802724335695	-3072
Mul_426
Square_81
1608802724303606	9437184
1608802724335758	-9437184
Mul_442
Square_85
1608802724304874	2359296
1608802724335823	-2359296
Mul_463
Square_90
1608802724304936	3072
1608802724335886	-3072
Mul_491
Square_92
1608802724305089	3072
1608802724335950	-3072
Mul_502
Square_70
1608802724305148	3072
1608802724336015	-3072
Mul_383
Square_74
1608802724305246	3072
1608802724336092	-3072
Mul_405
Mul_560
1608802724305304	2359296
1608802724323677	-2359296
Square_103
Square_82
1608802724305488	3072
1608802724336208	-3072
Mul_448
Square_86
1608802724305785	3072
1608802724336278	-3072
Mul_469
Square
1608802724306075	93763584
1608802724336340	-93763584
Mul_5
Square_2
1608802724307429	1966080
1608802724336417	-1966080
Mul_17
Square_3
1608802724307516	5376
1608802724336483	-5376
Mul_23
Square_6
1608802724307580	3072
1608802724336549	-3072
Mul_39
Square_4
1608802724307758	3072
1608802724336614	-3072
Mul_28
Square_7
1608802724307816	4194304
1608802724336675	-4194304
Mul_44
Square_10
1608802724307998	3072
1608802724336734	-3072
Mul_61
Square_5
1608802724308060	2359296
1608802724336797	-2359296
Mul_33
Square_8
1608802724308236	3072
1608802724336858	-3072
Mul_50
Square_11
1608802724308614	2359296
1608802724336919	-2359296
Mul_66
Square_14
1608802724309325	3072
1608802724336981	-3072
Mul_82
Square_99
1608802724310641	3072
1608802724337042	-3072
Mul_539
Square_9
1608802724311285	2359296
1608802724337105	-2359296
Mul_55
Square_12
1608802724311347	3072
1608802724337163	-3072
Mul_72
Square_15
1608802724311410	9437184
1608802724337228	-9437184
Mul_87
Square_39
1608802724311467	2359296
1608802724337289	-2359296
Mul_216
Mul_641
1608802724311580	3072
1608802724324761	-3072
Square_118
Mul_663
1608802724311708	3072
1608802724324820	-3072
Square_122
Square_13
1608802724311849	5888
1608802724337480	-5888
Mul_77
Square_16
1608802724311936	12288
1608802724337540	-12288
Mul_93
Mul_555
1608802724312018	3072
1608802724324985	-3072
Square_102
Square_23
1608802724312127	2359296
1608802724337659	-2359296
Mul_130
Mul_577
1608802724312195	3072
1608802724325096	-3072
Square_106
Square_31
1608802724312309	9437184
1608802724337781	-9437184
Mul_173
Mul_620
1608802724312379	3072
1608802724325211	-3072
Square_114
Square_17
1608802724312440	13369344
1608802724350786	-13369344
Mul_98
Mul_668
1608802724312526	2359296
1608802724325323	-2359296
Square_123
Mul_684
1608802724312691	3072
1608802724325379	-3072
Square_126
Mul_507
1608802724312759	3072
1608802724325436	-3072
Square_93
Square_53
1608802724313041	2359296
1608802724338162	-2359296
Mul_291
Square_56
1608802724313222	3072
1608802724338226	-3072
Mul_308
Mul_566
1608802724313278	4608
1608802724325607	-4608
Square_104
Mul_582
1608802724313904	2359296
1608802724325660	-2359296
Square_107
Mul_598
1608802724313975	3072
1608802724325719	-3072
Square_110
Mul_625
1608802724314048	3072
1608802724325772	-3072
Square_115
Mul_646
1608802724314103	2359296
1608802724325829	-2359296
Square_119
Square_34
1608802724314165	3072
1608802724338603	-3072
Mul_190
Square_36
1608802724314223	3072
1608802724338667	-3072
Mul_200
Mul_706
1608802724314281	3072
1608802724326002	-3072
Square_130
Mul_512
1608802724314334	3072
1608802724326058	-3072
Square_94
Mul_523
1608802724314386	12288
1608802724326112	-12288
Square_96
Square_54
1608802724314440	3072
1608802724338916	-3072
Mul_297
Square_57
1608802724314497	2359296
1608802724338979	-2359296
Mul_313
Mul_813
1608802724314555	3072
1608802724326284	-3072
Square_150
Mul_603
1608802724314613	9437184
1608802724326341	-9437184
Square_111
Mul_630
1608802724314674	3072
1608802724326397	-3072
Square_116
Mul_652
1608802724314736	3072
1608802724326450	-3072
Square_120
Mul_674
1608802724314797	3072
1608802724326507	-3072
Square_124
Mul_689
1608802724314853	9437184
1608802724326566	-9437184
Square_127
Mul_711
1608802724314910	3072
1608802724326620	-3072
Square_131
Square_37
1608802724314967	2359296
1608802724339486	-2359296
Mul_205
Mul_749
1608802724315028	3072
1608802724326731	-3072
Square_138
Square_97
1608802724315084	9437184
1608802724339609	-9437184
Mul_528
Mul_544
1608802724315143	3072
1608802724326844	-3072
Square_100
Mul_818
1608802724315196	2359296
1608802724326897	-2359296
Square_151
Square_58
1608802724315260	3072
1608802724339788	-3072
Mul_319
Mul_609
1608802724315319	12288
1608802724327004	-12288
Square_112
Mul_635
1608802724315372	2359296
1608802724327056	-2359296
Square_117
Mul_657
1608802724315432	2359296
1608802724327113	-2359296
Square_121
Mul_679
1608802724315495	3072
1608802724327173	-3072
Square_125
Mul_695
1608802724315548	12800
1608802724327228	-12800
Square_128
Mul_716
1608802724315605	3072
1608802724327283	-3072
Square_132
Mul_727
1608802724315659	3072
1608802724327341	-3072
Square_134
Mul_754
1608802724315711	2359296
1608802724327405	-2359296
Square_139
Square_38
1608802724315770	3072
1608802724340352	-3072
Mul_211
Square_98
1608802724315826	3072
1608802724340423	-3072
Mul_534
Mul_549
1608802724315878	2359296
1608802724327579	-2359296
Square_101
Square_47
1608802724315935	9437184
1608802724340547	-9437184
Mul_259
Mul_588
1608802724315999	3072
1608802724327689	-3072
Square_108
Mul_614
1608802724316054	9437184
1608802724327748	-9437184
Square_113
Mul_893
1608802724316110	2359296
1608802724327803	-2359296
Square_165
Square_169
1608802724316166	2359296
1608802724340799	-2359296
Mul_915
Square_173
1608802724316242	3072
1608802724340861	-3072
Mul_937
Mul_700
1608802724316301	9437184
1608802724327964	-9437184
Square_129
Square_181
1608802724316369	2359296
1608802724340984	-2359296
Mul_979
Mul_732
1608802724316440	2359296
1608802724328074	-2359296
Square_135
Mul_760
1608802724316500	3072
1608802724328128	-3072
Square_140
Mul_770
1608802724316557	3072
1608802724328181	-3072
Square_142
Mul_792
1608802724316613	3072
1608802724328235	-3072
Square_146
Mul_824
1608802724316667	3072
1608802724328294	-3072
Square_152
Mul_829
1608802724316726	2359296
1608802724328350	-2359296
Square_153
Square_43
1608802724316780	2359296
1608802724341426	-2359296
Mul_238
Mul_593
1608802724316838	3072
1608802724328472	-3072
Square_109
Mul_899
1608802724316893	3072
1608802724328529	-3072
Square_166
Square_170
1608802724316947	3072
1608802724341610	-3072
Mul_921
Square_174
1608802724317004	3072
1608802724341671	-3072
Mul_942
Square_177
1608802724317059	9437184
1608802724341739	-9437184
Mul_958
Square_182
1608802724317115	3072
1608802724341799	-3072
Mul_985
Mul_1007
1608802724317168	3072
1608802724328827	-3072
Square_186
Mul_738
1608802724317226	3072
1608802724328885	-3072
Square_136
Mul_765
1608802724317277	3072
1608802724328940	-3072
Square_141
Mul_775
1608802724317334	9437184
1608802724328997	-9437184
Square_143
Mul_797
1608802724317391	3072
1608802724329051	-3072
Square_147
Square_28
1608802724317446	3072
1608802724342172	-3072
Mul_158
Mul_835
1608802724317499	3072
1608802724329164	-3072
Square_154
Mul_851
1608802724317550	3072
1608802724329220	-3072
Square_157
Square_40
1608802724317612	3072
1608802724342357	-3072
Mul_222
Mul_904
1608802724317672	4033536
1608802724329331	-4033536
Square_167
Square_171
1608802724317729	2359296
1608802724342480	-2359296
Mul_926
Square_175
1608802724317789	9437184
1608802724342549	-9437184
Mul_947
Square_178
1608802724317844	3072
1608802724342610	-3072
Mul_964
Square_184
1608802724317903	3072
1608802724342673	-3072
Mul_996
Mul_1012
1608802724317957	2359296
1608802724329604	-2359296
Square_187
Mul_721
1608802724318016	2359296
1608802724329659	-2359296
Square_133
Mul_743
1608802724318073	2359296
1608802724329717	-2359296
Square_137
Mul_1071
1608802724318133	3072
1608802724329776	-3072
Square_198
Mul_781
1608802724318189	18432
1608802724329829	-18432
Square_144
Mul_802
1608802724318243	3072
1608802724329885	-3072
Square_148
Square_20
1608802724318293	3072
1608802724343107	-3072
Mul_114
Mul_840
1608802724318347	2359296
1608802724330000	-2359296
Square_155
Mul_856
1608802724318410	3072
1608802724330061	-3072
Square_158
Mul_872
1608802724318464	9437184
1608802724330117	-9437184
Square_161
Mul_571
1608802724318523	2359296
1608802724330172	-2359296
Square_105
Square_41
1608802724318581	2359296
1608802724343416	-2359296
Mul_227
Square_176
1608802724318639	12288
1608802724343475	-12288
Mul_953
Square_179
1608802724318693	3072
1608802724343535	-3072
Mul_969
Mul_1001
1608802724318750	2359296
1608802724330402	-2359296
Square_185
Mul_1018
1608802724318809	3072
1608802724330456	-3072
Square_188
Mul_1028
1608802724318867	3072
1608802724330509	-3072
Square_190
Mul_1050
1608802724318922	3072
1608802724330566	-3072
Square_194
Square_199
1608802724318988	2359296
1608802724343847	-2359296
Mul_1076
Square_202
1608802724319047	3072
1608802724343911	-3072
Mul_1092
Mul_786
1608802724319106	9437184
1608802724330731	-9437184
Square_145
Mul_807
1608802724319164	2359296
1608802724330784	-2359296
Square_149
Mul_846
1608802724319219	3072
1608802724330838	-3072
Square_156
Mul_861
1608802724319273	9437184
1608802724330897	-9437184
Square_159
Mul_878
1608802724319331	3072
1608802724330955	-3072
Square_162
Square_168
1608802724319388	3072
1608802724344289	-3072
Mul_910
Square_172
1608802724319440	3072
1608802724344351	-3072
Mul_932
Square_180
1608802724319495	3072
1608802724344426	-3072
Mul_974
Square_44
1608802724319549	3072
1608802724344487	-3072
Mul_244
Mul_1023
1608802724319604	3072
1608802724331239	-3072
Square_189
Mul_1033
1608802724319659	9437184
1608802724331292	-9437184
Square_191
Mul_1055
1608802724319719	3072
1608802724331348	-3072
Square_195
Square_200
1608802724319772	3072
1608802724344736	-3072
Mul_1082
Mul_1097
1608802724319826	122112
1608802724331452	-122112
Square_203
Square_1
1608802724319882	6144
1608802724344860	-6144
Mul_11
Mul_867
1608802724319933	12288
1608802724331564	-12288
Square_160
Mul_883
1608802724319985	3072
1608802724331625	-3072
Square_163
Square_24
1608802724320038	3072
1608802724345046	-3072
Mul_136
Square_35
1608802724320092	3072
1608802724345106	-3072
Mul_195
Square_42
1608802724320145	3072
1608802724345172	-3072
Mul_233
Mul_1039
1608802724320200	12288
1608802724331850	-12288
Square_192
Mul_1060
1608802724320256	3584
1608802724331903	-3584
Square_196
Square_201
1608802724320317	3072
1608802724345353	-3072
Mul_1087
Mul_1102
1608802724320386	6144
1608802724332013	-6144
Square_204
Square_59
1608802724320441	2359296
1608802724345478	-2359296
Mul_324
Mul_888
1608802724320501	3072
1608802724332131	-3072
Square_164
Square_19
1608802724320555	3072
1608802724345598	-3072
Mul_109
Square_32
1608802724320609	12288
1608802724345669	-12288
Mul_179
Mul_1044
1608802724320664	9437184
1608802724332936	-9437184
Square_193
Mul_1065
1608802724320717	2359296
1608802724332983	-2359296
Square_197
Square_45
1608802724320775	3072
1608802724345854	-3072
Mul_249
Square_205
1608802724320825	256
1608802724345915	-256
Mul_1108
Square_51
1608802724320879	3072
1608802724345977	-3072
Mul_281
Square_55
1608802724320936	2359296
1608802724346039	-2359296
Mul_302
Square_183
1608802724320998	2359296
1608802724346103	-2359296
Mul_990
Square_48
1608802724321057	12288
1608802724346165	-12288
Mul_265
Square_49
1608802724321108	9437184
1608802724346226	-9437184
Mul_270
Square_50
1608802724321164	3072
1608802724346288	-3072
Mul_276
Square_18
1608802724321221	3072
1608802724346348	-3072
Mul_104
Square_21
1608802724321278	2359296
1608802724346415	-2359296
Mul_119
Square_25
1608802724321335	2359296
1608802724346475	-2359296
Mul_141
Square_29
1608802724321395	3072
1608802724346533	-3072
Mul_163
Square_22
1608802724321449	3072
1608802724346594	-3072
Mul_125
Square_26
1608802724321507	3072
1608802724346654	-3072
Mul_147
Square_30
1608802724321560	3072
1608802724346718	-3072
Mul_168
Square_46
1608802724321613	3072
1608802724346780	-3072
Mul_254
Square_27
1608802724321669	2359296
1608802724346842	-2359296
Mul_152
Square_52
1608802724321738	3072
1608802724346904	-3072
Mul_286
Square_33
1608802724321791	9437184
1608802724346968	-9437184
Mul_184
Square_60
1608802724321852	3072
1608802724347027	-3072
Mul_330
Square_63
1608802724321908	9437184
1608802724347089	-9437184
Mul_345
Square_67
1608802724321988	3072
1608802724347151	-3072
Mul_367
Square_68
1608802724322039	3072
1608802724347211	-3072
Mul_372
Mul_390
add_244
Mul_412
add_258
Mul_396
add_248
Mul_353
add_221
Mul_476
add_298
Mul_455
add_285
Mul_337
add_211
Mul_358
add_224
Mul_379
add_237
Mul_482
add_302
Mul_418
add_262
Mul_433
add_271
Mul_460
add_288
Mul_487
add_305
Mul_401
add_251
Mul_423
add_265
Mul_439
add_275
Mul_342
add_214
Mul_364
add_228
Mul_498
add_312
add_325
Mul_519
Mul_428
add_268
Mul_444
add_278
Mul_465
add_291
Mul_493
add_309
Mul_504
add_316
Mul_385
add_241
Mul_407
add_255
add_352
Mul_562
Mul_450
add_282
Mul_471
add_295
Mul_7
add_3
Mul_19
add_11
Mul_25
add_15
Mul_41
add_25
Mul_30
add_18
Mul_46
add_28
Mul_63
add_39
Mul_35
add_21
Mul_52
add_32
Mul_68
add_42
Mul_84
add_52
Mul_541
add_339
Mul_57
add_35
Mul_74
add_46
Mul_89
add_55
Mul_218
add_136
add_403
Mul_643
add_417
Mul_665
Mul_79
add_49
Mul_95
add_59
add_349
Mul_557
Mul_132
add_82
add_363
Mul_579
Mul_175
add_109
add_390
Mul_622
Mul_100
add_62
add_420
Mul_670
add_430
Mul_686
add_319
Mul_509
Mul_293
add_183
Mul_310
add_194
add_356
Mul_568
add_366
Mul_584
add_376
Mul_600
add_393
Mul_627
add_406
Mul_648
Mul_192
add_120
Mul_202
add_126
add_444
Mul_708
add_322
Mul_514
add_329
Mul_525
Mul_299
add_187
Mul_315
add_197
add_511
Mul_815
add_379
Mul_605
add_396
Mul_632
add_410
Mul_654
add_424
Mul_676
add_433
Mul_691
add_447
Mul_713
Mul_207
add_129
add_471
Mul_751
Mul_530
add_332
add_342
Mul_546
add_514
Mul_820
Mul_321
add_201
add_383
Mul_611
add_399
Mul_637
add_413
Mul_659
add_427
Mul_681
add_437
Mul_697
add_450
Mul_718
add_457
Mul_729
add_474
Mul_756
Mul_213
add_133
Mul_536
add_336
add_345
Mul_551
Mul_261
add_163
add_370
Mul_590
add_386
Mul_616
add_561
Mul_895
Mul_917
add_575
Mul_939
add_589
add_440
Mul_702
Mul_981
add_615
add_460
Mul_734
add_478
Mul_762
add_484
Mul_772
add_498
Mul_794
add_518
Mul_826
add_521
Mul_831
Mul_240
add_150
add_373
Mul_595
add_565
Mul_901
Mul_923
add_579
Mul_944
add_592
Mul_960
add_602
Mul_987
add_619
add_633
Mul_1009
add_464
Mul_740
add_481
Mul_767
add_487
Mul_777
add_501
Mul_799
Mul_160
add_100
add_525
Mul_837
add_535
Mul_853
Mul_224
add_140
add_568
Mul_906
Mul_928
add_582
Mul_949
add_595
Mul_966
add_606
Mul_998
add_626
add_636
Mul_1014
add_453
Mul_723
add_467
Mul_745
add_673
Mul_1073
add_491
Mul_783
add_504
Mul_804
Mul_116
add_72
add_528
Mul_842
add_538
Mul_858
add_548
Mul_874
add_359
Mul_573
Mul_229
add_143
Mul_955
add_599
Mul_971
add_609
add_629
Mul_1003
add_640
Mul_1020
add_646
Mul_1030
add_660
Mul_1052
Mul_1078
add_676
Mul_1094
add_686
add_494
Mul_788
add_507
Mul_809
add_532
Mul_848
add_541
Mul_863
add_552
Mul_880
Mul_912
add_572
Mul_934
add_586
Mul_976
add_612
Mul_246
add_154
add_643
Mul_1025
add_649
Mul_1035
add_663
Mul_1057
Mul_1084
add_680
add_689
Mul_1099
Mul_13
add_7
add_545
Mul_869
add_555
Mul_885
Mul_138
add_86
Mul_197
add_123
Mul_235
add_147
add_653
Mul_1041
add_666
Mul_1062
Mul_1089
add_683
add_692
Mul_1104
Mul_326
add_204
add_558
Mul_890
Mul_111
add_69
Mul_181
add_113
add_656
Mul_1046
add_669
Mul_1067
Mul_251
add_157
Mul_1110
add_696
Mul_283
add_177
Mul_304
add_190
Mul_992
add_622
Mul_267
add_167
Mul_272
add_170
Mul_278
add_174
Mul_106
add_66
Mul_121
add_75
Mul_143
add_89
Mul_165
add_103
Mul_127
add_79
Mul_149
add_93
Mul_170
add_106
Mul_256
add_160
Mul_154
add_96
Mul_288
add_180
Mul_186
add_116
Mul_332
add_208
Mul_347
add_217
Mul_369
add_231
Mul_374
add_234
add_245
Assign_214
add_259
Assign_226
add_249
Assign_217
add_222
Assign_193
add_299
Assign_262
add_286
Assign_250
add_212
Assign_184
add_225
Assign_196
add_238
Assign_208
add_303
Assign_265
add_263
Assign_229
add_272
Assign_238
add_289
Assign_253
add_306
Assign_268
add_252
Assign_220
add_266
Assign_232
add_276
Assign_241
add_215
Assign_187
add_229
Assign_199
add_313
Assign_274
Assign_286
add_326
add_269
Assign_235
add_279
Assign_244
add_292
Assign_256
add_310
Assign_271
add_317
Assign_277
add_242
Assign_211
add_256
Assign_223
Assign_310
add_353
add_283
Assign_247
add_296
Assign_259
add_4
Assign_1
add_12
Assign_7
add_16
Assign_10
add_26
Assign_19
add_19
Assign_13
add_29
Assign_22
add_40
Assign_31
add_22
Assign_16
add_33
Assign_25
add_43
Assign_34
add_53
Assign_43
add_340
Assign_298
add_36
Assign_28
add_47
Assign_37
add_56
Assign_46
add_137
Assign_118
Assign_355
add_404
Assign_367
add_418
add_50
Assign_40
add_60
Assign_49
Assign_307
add_350
add_83
Assign_70
Assign_319
add_364
add_110
Assign_94
Assign_343
add_391
add_63
Assign_52
Assign_370
add_421
Assign_379
add_431
Assign_280
add_320
add_184
Assign_160
add_195
Assign_169
Assign_313
add_357
Assign_322
add_367
Assign_331
add_377
Assign_346
add_394
Assign_358
add_407
add_121
Assign_103
add_127
Assign_109
Assign_391
add_445
Assign_283
add_323
Assign_289
add_330
add_188
Assign_163
add_198
Assign_172
Assign_451
add_512
Assign_334
add_380
Assign_349
add_397
Assign_361
add_411
Assign_373
add_425
Assign_382
add_434
Assign_394
add_448
add_130
Assign_112
Assign_415
add_472
add_333
Assign_292
Assign_301
add_343
Assign_454
add_515
add_202
Assign_175
Assign_337
add_384
Assign_352
add_400
Assign_364
add_414
Assign_376
add_428
Assign_385
add_438
Assign_397
add_451
Assign_403
add_458
Assign_418
add_475
add_134
Assign_115
add_337
Assign_295
Assign_304
add_346
add_164
Assign_142
Assign_325
add_371
Assign_340
add_387
Assign_496
add_562
add_576
Assign_508
add_590
Assign_520
Assign_388
add_441
add_616
Assign_544
Assign_406
add_461
Assign_421
add_479
Assign_427
add_485
Assign_439
add_499
Assign_457
add_519
Assign_460
add_522
add_151
Assign_130
Assign_328
add_374
Assign_499
add_566
add_580
Assign_511
add_593
Assign_523
add_603
Assign_532
add_620
Assign_547
Assign_559
add_634
Assign_409
add_465
Assign_424
add_482
Assign_430
add_488
Assign_442
add_502
add_101
Assign_85
Assign_463
add_526
Assign_472
add_536
add_141
Assign_121
Assign_502
add_569
add_583
Assign_514
add_596
Assign_526
add_607
Assign_535
add_627
Assign_553
Assign_562
add_637
Assign_400
add_454
Assign_412
add_468
Assign_595
add_674
Assign_433
add_492
Assign_445
add_505
add_73
Assign_61
Assign_466
add_529
Assign_475
add_539
Assign_484
add_549
Assign_316
add_360
add_144
Assign_124
add_600
Assign_529
add_610
Assign_538
Assign_556
add_630
Assign_565
add_641
Assign_571
add_647
Assign_583
add_661
add_677
Assign_598
add_687
Assign_607
Assign_436
add_495
Assign_448
add_508
Assign_469
add_533
Assign_478
add_542
Assign_487
add_553
add_573
Assign_505
add_587
Assign_517
add_613
Assign_541
add_155
Assign_133
Assign_568
add_644
Assign_574
add_650
Assign_586
add_664
add_681
Assign_601
Assign_610
add_690
add_8
Assign_4
Assign_481
add_546
Assign_490
add_556
add_87
Assign_73
add_124
Assign_106
add_148
Assign_127
Assign_577
add_654
Assign_589
add_667
add_684
Assign_604
Assign_613
add_693
add_205
Assign_178
Assign_493
add_559
add_70
Assign_58
add_114
Assign_97
Assign_580
add_657
Assign_592
add_670
add_158
Assign_136
add_697
Assign_616
add_178
Assign_154
add_191
Assign_166
add_623
Assign_550
add_168
Assign_145
add_171
Assign_148
add_175
Assign_151
add_67
Assign_55
add_76
Assign_64
add_90
Assign_76
add_104
Assign_88
add_80
Assign_67
add_94
Assign_79
add_107
Assign_91
add_161
Assign_139
add_97
Assign_82
add_181
Assign_157
add_117
Assign_100
add_209
Assign_181
add_218
Assign_190
add_232
Assign_202
add_235
Assign_205
Sqrt_71
1608802724347260	2359296
1608802724367145	-2359296
Assign_215
Sqrt_75
1608802724347328	2359296
1608802724367178	-2359296
Assign_227
Sqrt_72
1608802724347398	3072
1608802724367204	-3072
Assign_218
Sqrt_64
1608802724347459	20992
1608802724367234	-20992
Assign_194
Sqrt_87
1608802724347519	2359296
1608802724367264	-2359296
Assign_263
Sqrt_83
1608802724347776	3072
1608802724367296	-3072
Assign_251
Sqrt_61
1608802724347830	3072
1608802724367326	-3072
Assign_185
Sqrt_65
1608802724347886	12582912
1608802724367352	-12582912
Assign_197
Sqrt_69
1608802724347944	2359296
1608802724367382	-2359296
Assign_209
Sqrt_88
1608802724348001	3072
1608802724367408	-3072
Assign_266
Sqrt_76
1608802724348056	3072
1608802724367437	-3072
Assign_230
Sqrt_79
1608802724348109	9437184
1608802724367466	-9437184
Assign_239
Sqrt_84
1608802724348165	3072
1608802724367493	-3072
Assign_254
Sqrt_89
1608802724348217	2359296
1608802724367520	-2359296
Assign_269
Sqrt_73
1608802724348273	2359296
1608802724367546	-2359296
Assign_221
Sqrt_77
1608802724348330	3072
1608802724367576	-3072
Assign_233
Sqrt_80
1608802724348394	12288
1608802724367605	-12288
Assign_242
Sqrt_62
1608802724348451	3072
1608802724367629	-3072
Assign_188
Sqrt_66
1608802724348504	3072
1608802724367658	-3072
Assign_200
Sqrt_91
1608802724348567	2359296
1608802724367682	-2359296
Assign_275
Sqrt_95
1608802724348629	11796480
1608802724367714	-11796480
Assign_287
Sqrt_78
1608802724348687	3072
1608802724367743	-3072
Assign_236
Sqrt_81
1608802724348743	9437184
1608802724367769	-9437184
Assign_245
Sqrt_85
1608802724348802	2359296
1608802724367798	-2359296
Assign_257
Sqrt_90
1608802724348857	3072
1608802724367821	-3072
Assign_272
Sqrt_92
1608802724348914	3072
1608802724367850	-3072
Assign_278
Sqrt_70
1608802724348970	3072
1608802724367877	-3072
Assign_212
Sqrt_74
1608802724349024	3072
1608802724367903	-3072
Assign_224
Sqrt_103
1608802724349077	2359296
1608802724367931	-2359296
Assign_311
Sqrt_82
1608802724349133	3072
1608802724367956	-3072
Assign_248
Sqrt_86
1608802724349193	3072
1608802724367986	-3072
Assign_260
Sqrt
1608802724349246	93769728
1608802724368017	-93769728
Assign_2
Sqrt_2
1608802724349307	2359296
1608802724368042	-2359296
Assign_8
Sqrt_3
1608802724349363	3072
1608802724368074	-3072
Assign_11
Sqrt_6
1608802724349417	3072
1608802724368099	-3072
Assign_20
Sqrt_4
1608802724349471	3072
1608802724368129	-3072
Assign_14
Sqrt_7
1608802724349524	2359296
1608802724368157	-2359296
Assign_23
Sqrt_10
1608802724349579	3072
1608802724368182	-3072
Assign_32
Sqrt_5
1608802724349634	2359296
1608802724368210	-2359296
Assign_17
Sqrt_8
1608802724349689	3072
1608802724368234	-3072
Assign_26
Sqrt_11
1608802724349741	2359296
1608802724368262	-2359296
Assign_35
Sqrt_14
1608802724349800	3072
1608802724368291	-3072
Assign_44
Sqrt_99
1608802724349854	3072
1608802724368315	-3072
Assign_299
Sqrt_9
1608802724349909	2359296
1608802724368345	-2359296
Assign_29
Sqrt_12
1608802724349971	3072
1608802724368392	-3072
Assign_38
Sqrt_15
1608802724350032	9437184
1608802724368425	-9437184
Assign_47
Sqrt_39
1608802724350099	2359296
1608802724368458	-2359296
Assign_119
Sqrt_118
1608802724350163	3072
1608802724368486	-3072
Assign_356
Sqrt_122
1608802724350236	3072
1608802724368523	-3072
Assign_368
Sqrt_13
1608802724350295	3072
1608802724368549	-3072
Assign_41
Sqrt_16
1608802724350354	12288
1608802724368577	-12288
Assign_50
Sqrt_102
1608802724350415	3072
1608802724368606	-3072
Assign_308
Sqrt_23
1608802724350474	2359296
1608802724368633	-2359296
Assign_71
Sqrt_106
1608802724350544	3072
1608802724368663	-3072
Assign_320
Sqrt_31
1608802724350606	9437184
1608802724368689	-9437184
Assign_95
Sqrt_114
1608802724350670	3072
1608802724368718	-3072
Assign_344
Sqrt_17
1608802724350730	9437184
1608802724368749	-9437184
Assign_53
Sqrt_123
1608802724350794	2359296
1608802724368776	-2359296
Assign_371
Sqrt_126
1608802724350864	3072
1608802724368805	-3072
Assign_380
Sqrt_93
1608802724350923	3072
1608802724368830	-3072
Assign_281
Sqrt_53
1608802724350983	2359296
1608802724368861	-2359296
Assign_161
Sqrt_56
1608802724351049	3072
1608802724368889	-3072
Assign_170
Sqrt_104
1608802724351114	3072
1608802724368915	-3072
Assign_314
Sqrt_107
1608802724351173	2359296
1608802724368945	-2359296
Assign_323
Sqrt_110
1608802724351238	3072
1608802724368969	-3072
Assign_332
Sqrt_115
1608802724351298	3072
1608802724368999	-3072
Assign_347
Sqrt_119
1608802724351362	2359296
1608802724369029	-2359296
Assign_359
Sqrt_34
1608802724351427	3072
1608802724369053	-3072
Assign_104
Sqrt_36
1608802724351489	3072
1608802724369083	-3072
Assign_110
Sqrt_130
1608802724351549	3072
1608802724369110	-3072
Assign_392
Sqrt_94
1608802724351610	3072
1608802724369142	-3072
Assign_284
Sqrt_96
1608802724351675	15360
1608802724369169	-15360
Assign_290
Sqrt_54
1608802724351734	3072
1608802724369194	-3072
Assign_164
Sqrt_57
1608802724351796	2359296
1608802724369223	-2359296
Assign_173
Sqrt_150
1608802724351866	3072
1608802724369249	-3072
Assign_452
Sqrt_111
1608802724351927	12582912
1608802724369280	-12582912
Assign_335
Sqrt_116
1608802724351998	3072
1608802724369309	-3072
Assign_350
Sqrt_120
1608802724352076	3072
1608802724369334	-3072
Assign_362
Sqrt_124
1608802724352130	3072
1608802724369360	-3072
Assign_374
Sqrt_127
1608802724352183	9437184
1608802724369385	-9437184
Assign_383
Sqrt_131
1608802724352242	3072
1608802724369415	-3072
Assign_395
Sqrt_37
1608802724352296	2359296
1608802724369443	-2359296
Assign_113
Sqrt_138
1608802724352355	3072
1608802724369469	-3072
Assign_416
Sqrt_97
1608802724352426	9437184
1608802724369498	-9437184
Assign_293
Sqrt_100
1608802724352492	3072
1608802724369521	-3072
Assign_302
Sqrt_151
1608802724352561	2359296
1608802724369550	-2359296
Assign_455
Sqrt_58
1608802724352626	3072
1608802724369578	-3072
Assign_176
Sqrt_112
1608802724352685	12288
1608802724369607	-12288
Assign_338
Sqrt_117
1608802724352743	2359296
1608802724369635	-2359296
Assign_353
Sqrt_121
1608802724352813	2359296
1608802724369660	-2359296
Assign_365
Sqrt_125
1608802724352875	3072
1608802724369687	-3072
Assign_377
Sqrt_128
1608802724352933	12288
1608802724369716	-12288
Assign_386
Sqrt_132
1608802724352992	3072
1608802724369744	-3072
Assign_398
Sqrt_134
1608802724353053	3072
1608802724369771	-3072
Assign_404
Sqrt_139
1608802724353118	2359296
1608802724369796	-2359296
Assign_419
Sqrt_38
1608802724353181	3072
1608802724369824	-3072
Assign_116
Sqrt_98
1608802724353241	3072
1608802724369852	-3072
Assign_296
Sqrt_101
1608802724353300	2359296
1608802724369878	-2359296
Assign_305
Sqrt_47
1608802724353378	9437184
1608802724369906	-9437184
Assign_143
Sqrt_108
1608802724353443	3072
1608802724369932	-3072
Assign_326
Sqrt_113
1608802724353505	9437184
1608802724369970	-9437184
Assign_341
Sqrt_165
1608802724353568	2359296
1608802724369999	-2359296
Assign_497
Sqrt_169
1608802724353632	2359296
1608802724370026	-2359296
Assign_509
Sqrt_173
1608802724353705	3072
1608802724370057	-3072
Assign_521
Sqrt_129
1608802724353767	9437184
1608802724370084	-9437184
Assign_389
Sqrt_181
1608802724353831	2359296
1608802724370113	-2359296
Assign_545
Sqrt_135
1608802724353896	2359296
1608802724370141	-2359296
Assign_407
Sqrt_140
1608802724353962	3072
1608802724370168	-3072
Assign_422
Sqrt_142
1608802724354022	3072
1608802724370197	-3072
Assign_428
Sqrt_146
1608802724354081	3072
1608802724370221	-3072
Assign_440
Sqrt_152
1608802724354140	3072
1608802724370249	-3072
Assign_458
Sqrt_153
1608802724354201	2359296
1608802724370278	-2359296
Assign_461
Sqrt_43
1608802724354266	2359296
1608802724370304	-2359296
Assign_131
Sqrt_109
1608802724354331	3072
1608802724370331	-3072
Assign_329
Sqrt_166
1608802724354390	3072
1608802724370356	-3072
Assign_500
Sqrt_170
1608802724354452	3072
1608802724370386	-3072
Assign_512
Sqrt_174
1608802724354518	3072
1608802724370410	-3072
Assign_524
Sqrt_177
1608802724354578	9437184
1608802724370441	-9437184
Assign_533
Sqrt_182
1608802724354641	3072
1608802724370468	-3072
Assign_548
Sqrt_186
1608802724354703	3072
1608802724370495	-3072
Assign_560
Sqrt_136
1608802724354769	3072
1608802724370524	-3072
Assign_410
Sqrt_141
1608802724354828	3072
1608802724370548	-3072
Assign_425
Sqrt_143
1608802724354896	9437184
1608802724370577	-9437184
Assign_431
Sqrt_147
1608802724354961	3072
1608802724370606	-3072
Assign_443
Sqrt_28
1608802724355019	3072
1608802724370632	-3072
Assign_86
Sqrt_154
1608802724355085	3072
1608802724370661	-3072
Assign_464
Sqrt_157
1608802724355144	3072
1608802724370684	-3072
Assign_473
Sqrt_40
1608802724355202	3072
1608802724370716	-3072
Assign_122
Sqrt_167
1608802724355261	2359296
1608802724370745	-2359296
Assign_503
Sqrt_171
1608802724355331	2359296
1608802724370774	-2359296
Assign_515
Sqrt_175
1608802724355396	9437184
1608802724370804	-9437184
Assign_527
Sqrt_178
1608802724355462	3072
1608802724370829	-3072
Assign_536
Sqrt_184
1608802724355521	3072
1608802724370860	-3072
Assign_554
Sqrt_187
1608802724355579	2359296
1608802724370888	-2359296
Assign_563
Sqrt_133
1608802724355649	2359296
1608802724370917	-2359296
Assign_401
Sqrt_137
1608802724355712	2359296
1608802724370945	-2359296
Assign_413
Sqrt_198
1608802724355774	3072
1608802724370969	-3072
Assign_596
Sqrt_144
1608802724355836	12288
1608802724371001	-12288
Assign_434
Sqrt_148
1608802724355900	3072
1608802724371030	-3072
Assign_446
Sqrt_20
1608802724355960	3072
1608802724371055	-3072
Assign_62
Sqrt_155
1608802724356020	2359296
1608802724371086	-2359296
Assign_467
Sqrt_158
1608802724356082	3072
1608802724371112	-3072
Assign_476
Sqrt_161
1608802724356146	9437184
1608802724371140	-9437184
Assign_485
Sqrt_105
1608802724356214	2359296
1608802724371168	-2359296
Assign_317
Sqrt_41
1608802724356277	2359296
1608802724371194	-2359296
Assign_125
Sqrt_176
1608802724356341	12288
1608802724371226	-12288
Assign_530
Sqrt_179
1608802724356409	3072
1608802724371250	-3072
Assign_539
Sqrt_185
1608802724356483	2359296
1608802724371282	-2359296
Assign_557
Sqrt_188
1608802724356546	3072
1608802724371310	-3072
Assign_566
Sqrt_190
1608802724356606	3072
1608802724371334	-3072
Assign_572
Sqrt_194
1608802724356667	3072
1608802724371371	-3072
Assign_584
Sqrt_199
1608802724356728	2359296
1608802724371395	-2359296
Assign_599
Sqrt_202
1608802724356792	3072
1608802724371424	-3072
Assign_608
Sqrt_145
1608802724356854	9437184
1608802724371454	-9437184
Assign_437
Sqrt_149
1608802724356917	2359296
1608802724371480	-2359296
Assign_449
Sqrt_156
1608802724356982	3072
1608802724371509	-3072
Assign_470
Sqrt_159
1608802724357046	9437184
1608802724371535	-9437184
Assign_479
Sqrt_162
1608802724357110	3072
1608802724371563	-3072
Assign_488
Sqrt_168
1608802724357170	3072
1608802724371593	-3072
Assign_506
Sqrt_172
1608802724357229	3072
1608802724371619	-3072
Assign_518
Sqrt_180
1608802724357292	3072
1608802724371646	-3072
Assign_542
Sqrt_44
1608802724357353	3072
1608802724371671	-3072
Assign_134
Sqrt_189
1608802724357414	3072
1608802724371701	-3072
Assign_569
Sqrt_191
1608802724357474	9437184
1608802724371730	-9437184
Assign_575
Sqrt_195
1608802724357539	3072
1608802724371756	-3072
Assign_587
Sqrt_200
1608802724357603	3072
1608802724371785	-3072
Assign_602
Sqrt_203
1608802724357664	169472
1608802724371811	-169472
Assign_611
Sqrt_1
1608802724357729	6144
1608802724371840	-6144
Assign_5
Sqrt_160
1608802724357790	23040
1608802724371865	-23040
Assign_482
Sqrt_163
1608802724357854	3072
1608802724371895	-3072
Assign_491
Sqrt_24
1608802724357914	3072
1608802724371927	-3072
Assign_74
Sqrt_35
1608802724357982	3072
1608802724371953	-3072
Assign_107
Sqrt_42
1608802724358043	3072
1608802724371981	-3072
Assign_128
Sqrt_192
1608802724358103	12288
1608802724372007	-12288
Assign_578
Sqrt_196
1608802724358168	3072
1608802724372039	-3072
Assign_590
Sqrt_201
1608802724358228	3072
1608802724372069	-3072
Assign_605
Sqrt_204
1608802724358286	6144
1608802724372095	-6144
Assign_614
Sqrt_59
1608802724358343	2359296
1608802724372126	-2359296
Assign_179
Sqrt_164
1608802724358410	3072
1608802724372151	-3072
Assign_494
Sqrt_19
1608802724358469	3072
1608802724372182	-3072
Assign_59
Sqrt_32
1608802724358529	12288
1608802724372209	-12288
Assign_98
Sqrt_193
1608802724358588	9437184
1608802724372234	-9437184
Assign_581
Sqrt_197
1608802724358650	2359296
1608802724372264	-2359296
Assign_593
Sqrt_45
1608802724358718	3072
1608802724372287	-3072
Assign_137
Sqrt_205
1608802724358782	256
1608802724372317	-256
Assign_617
Sqrt_51
1608802724358841	3072
1608802724372348	-3072
Assign_155
Sqrt_55
1608802724358904	2359296
1608802724372385	-2359296
Assign_167
Sqrt_183
1608802724358967	2359296
1608802724372415	-2359296
Assign_551
Sqrt_48
1608802724359033	12288
1608802724372441	-12288
Assign_146
Sqrt_49
1608802724359093	9437184
1608802724372473	-9437184
Assign_149
Sqrt_50
1608802724359158	3072
1608802724372501	-3072
Assign_152
Sqrt_18
1608802724359219	3072
1608802724372529	-3072
Assign_56
Sqrt_21
1608802724359285	2359296
1608802724372557	-2359296
Assign_65
Sqrt_25
1608802724359348	2359296
1608802724372582	-2359296
Assign_77
Sqrt_29
1608802724359412	3072
1608802724372613	-3072
Assign_89
Sqrt_22
1608802724359525	3072
1608802724372638	-3072
Assign_68
Sqrt_26
1608802724359581	3072
1608802724372668	-3072
Assign_80
Sqrt_30
1608802724359633	3072
1608802724372697	-3072
Assign_92
Sqrt_46
1608802724359686	3072
1608802724372722	-3072
Assign_140
Sqrt_27
1608802724359740	2359296
1608802724372760	-2359296
Assign_83
Sqrt_52
1608802724359796	3072
1608802724372784	-3072
Assign_158
Sqrt_33
1608802724359855	9437184
1608802724372816	-9437184
Assign_101
Sqrt_60
1608802724359915	3072
1608802724372845	-3072
Assign_182
Sqrt_63
1608802724359968	9437184
1608802724372871	-9437184
Assign_191
Sqrt_67
1608802724360025	3072
1608802724372901	-3072
Assign_203
Sqrt_68
1608802724360077	3072
1608802724372925	-3072
Assign_206
add_246
add_260
add_250
add_223
add_300
add_287
add_213
add_226
add_239
add_304
add_264
add_273
add_290
add_307
add_253
add_267
add_277
add_216
add_230
add_314
add_327
add_270
add_280
add_293
add_311
add_318
add_243
add_257
add_354
add_284
add_297
add_5
add_13
add_17
add_27
add_20
add_30
add_41
add_23
add_34
add_44
add_54
add_341
add_37
add_48
add_57
add_138
add_405
add_419
add_51
add_61
add_351
add_84
add_365
add_111
add_392
add_64
add_422
add_432
add_321
add_185
add_196
add_358
add_368
add_378
add_395
add_408
add_122
add_128
add_446
add_324
add_331
add_189
add_199
add_513
add_381
add_398
add_412
add_426
add_435
add_449
add_131
add_473
add_334
add_344
add_516
add_203
add_385
add_401
add_415
add_429
add_439
add_452
add_459
add_476
add_135
add_338
add_347
add_165
add_372
add_388
add_563
add_577
add_591
add_442
add_617
add_462
add_480
add_486
add_500
add_520
add_523
add_152
add_375
add_567
add_581
add_594
add_604
add_621
add_635
add_466
add_483
add_489
add_503
add_102
add_527
add_537
add_142
add_570
add_584
add_597
add_608
add_628
add_638
add_455
add_469
add_675
add_493
add_506
add_74
add_530
add_540
add_550
add_361
add_145
add_601
add_611
add_631
add_642
add_648
add_662
add_678
add_688
add_496
add_509
add_534
add_543
add_554
add_574
add_588
add_614
add_156
add_645
add_651
add_665
add_682
add_691
add_9
add_547
add_557
add_88
add_125
add_149
add_655
add_668
add_685
add_694
add_206
add_560
add_71
add_115
add_658
add_671
add_159
add_698
add_179
add_192
add_624
add_169
add_172
add_176
add_68
add_77
add_91
add_105
add_81
add_95
add_108
add_162
add_98
add_182
add_118
add_210
add_219
add_233
add_236
truediv_72
truediv_76
truediv_73
truediv_65
truediv_88
truediv_84
truediv_62
truediv_66
truediv_70
truediv_89
truediv_77
truediv_80
truediv_85
truediv_90
truediv_74
truediv_78
truediv_81
truediv_63
truediv_67
truediv_92
truediv_96
truediv_79
truediv_82
truediv_86
truediv_91
truediv_93
truediv_71
truediv_75
truediv_104
truediv_83
truediv_87
truediv_1
truediv_3
truediv_4
truediv_7
truediv_5
truediv_8
truediv_11
truediv_6
truediv_9
truediv_12
truediv_15
truediv_100
truediv_10
truediv_13
truediv_16
truediv_40
truediv_119
truediv_123
truediv_14
truediv_17
truediv_103
truediv_24
truediv_107
truediv_32
truediv_115
truediv_18
truediv_124
truediv_127
truediv_94
truediv_54
truediv_57
truediv_105
truediv_108
truediv_111
truediv_116
truediv_120
truediv_35
truediv_37
truediv_131
truediv_95
truediv_97
truediv_55
truediv_58
truediv_151
truediv_112
truediv_117
truediv_121
truediv_125
truediv_128
truediv_132
truediv_38
truediv_139
truediv_98
truediv_101
truediv_152
truediv_59
truediv_113
truediv_118
truediv_122
truediv_126
truediv_129
truediv_133
truediv_135
truediv_140
truediv_39
truediv_99
truediv_102
truediv_48
truediv_109
truediv_114
truediv_166
truediv_170
truediv_174
truediv_130
truediv_182
truediv_136
truediv_141
truediv_143
truediv_147
truediv_153
truediv_154
truediv_44
truediv_110
truediv_167
truediv_171
truediv_175
truediv_178
truediv_183
truediv_187
truediv_137
truediv_142
truediv_144
truediv_148
truediv_29
truediv_155
truediv_158
truediv_41
truediv_168
truediv_172
truediv_176
truediv_179
truediv_185
truediv_188
truediv_134
truediv_138
truediv_199
truediv_145
truediv_149
truediv_21
truediv_156
truediv_159
truediv_162
truediv_106
truediv_42
truediv_177
truediv_180
truediv_186
truediv_189
truediv_191
truediv_195
truediv_200
truediv_203
truediv_146
truediv_150
truediv_157
truediv_160
truediv_163
truediv_169
truediv_173
truediv_181
truediv_45
truediv_190
truediv_192
truediv_196
truediv_201
truediv_204
truediv_2
truediv_161
truediv_164
truediv_25
truediv_36
truediv_43
truediv_193
truediv_197
truediv_202
truediv_205
truediv_60
truediv_165
truediv_20
truediv_33
truediv_194
truediv_198
truediv_46
truediv_206
truediv_52
truediv_56
truediv_184
truediv_49
truediv_50
truediv_51
truediv_19
truediv_22
truediv_26
truediv_30
truediv_23
truediv_27
truediv_31
truediv_47
truediv_28
truediv_53
truediv_34
truediv_61
truediv_64
truediv_68
truediv_69
add_247
add_261
mul_397
mul_354
add_301
mul_456
mul_338
add_227
add_240
mul_483
mul_419
add_274
mul_461
add_308
add_254
mul_424
mul_440
mul_343
mul_365
add_315
add_328
mul_429
add_281
add_294
mul_494
mul_505
mul_386
mul_408
add_355
mul_451
mul_472
add_6
add_14
mul_26
mul_42
mul_31
add_31
mul_64
add_24
mul_53
add_45
mul_85
mul_542
add_38
mul_75
add_58
add_139
mul_644
mul_666
mul_80
mul_96
mul_558
add_85
mul_580
add_112
mul_623
add_65
add_423
mul_687
mul_510
add_186
mul_311
mul_569
add_369
mul_601
mul_628
add_409
mul_193
mul_203
mul_709
mul_515
mul_526
mul_300
add_200
mul_816
add_382
mul_633
mul_655
mul_677
add_436
mul_714
add_132
mul_752
add_335
mul_547
add_517
mul_322
mul_612
add_402
add_416
mul_682
mul_698
mul_719
mul_730
add_477
mul_214
mul_537
add_348
add_166
mul_591
add_389
add_564
add_578
mul_940
add_443
add_618
add_463
mul_763
mul_773
mul_795
mul_827
add_524
add_153
mul_596
mul_902
mul_924
mul_945
add_605
mul_988
mul_1010
mul_741
mul_768
add_490
mul_800
mul_161
mul_838
mul_854
mul_225
add_571
add_585
add_598
mul_967
mul_999
add_639
add_456
add_470
mul_1074
mul_784
mul_805
mul_117
add_531
mul_859
add_551
add_362
add_146
mul_956
mul_972
add_632
mul_1021
mul_1031
mul_1053
add_679
mul_1095
add_497
add_510
mul_849
add_544
mul_881
mul_913
mul_935
mul_977
mul_247
mul_1026
add_652
mul_1058
mul_1085
mul_1100
add_10
mul_870
mul_886
mul_139
mul_198
mul_236
mul_1042
mul_1063
mul_1090
add_695
add_207
mul_891
mul_112
mul_182
add_659
add_672
mul_252
mul_1111
mul_284
add_193
add_625
mul_268
add_173
mul_279
mul_107
add_78
add_92
mul_166
mul_128
mul_150
mul_171
mul_257
add_99
mul_289
add_119
mul_333
add_220
mul_370
mul_375
mul_392
mul_414
sub_73
sub_65
mul_478
sub_84
sub_62
mul_360
mul_381
sub_89
sub_77
mul_435
sub_85
mul_489
mul_403
sub_78
sub_81
sub_63
sub_67
mul_500
mul_521
sub_79
mul_446
mul_467
sub_91
sub_93
sub_71
sub_75
mul_564
sub_83
sub_87
mul_9
mul_21
sub_4
sub_7
sub_5
mul_48
sub_11
mul_37
sub_9
mul_70
sub_15
sub_100
mul_59
sub_13
mul_91
mul_220
sub_119
sub_123
sub_14
sub_17
sub_103
mul_134
sub_107
mul_177
sub_115
mul_102
mul_672
sub_127
sub_94
mul_295
sub_57
sub_105
mul_586
sub_111
sub_116
mul_650
sub_35
sub_37
sub_131
sub_95
sub_97
sub_55
mul_317
sub_151
mul_607
sub_117
sub_121
sub_125
mul_693
sub_132
mul_209
sub_139
mul_532
sub_101
mul_822
sub_59
sub_113
mul_639
mul_661
sub_126
sub_129
sub_133
sub_135
mul_758
sub_39
sub_99
mul_553
mul_263
sub_109
mul_618
mul_897
mul_919
sub_174
mul_704
mul_983
mul_736
sub_141
sub_143
sub_147
sub_153
mul_833
mul_242
sub_110
sub_167
sub_171
sub_175
mul_962
sub_183
sub_187
sub_137
sub_142
mul_779
sub_148
sub_29
sub_155
sub_158
sub_41
mul_908
mul_930
mul_951
sub_179
sub_185
mul_1016
mul_725
mul_747
sub_199
sub_145
sub_149
sub_21
mul_844
sub_159
mul_876
mul_575
mul_231
sub_177
sub_180
mul_1005
sub_189
sub_191
sub_195
mul_1080
sub_203
mul_790
mul_811
sub_157
mul_865
sub_163
sub_169
sub_173
sub_181
sub_45
sub_190
mul_1037
sub_196
sub_201
sub_204
mul_15
sub_161
sub_164
sub_25
sub_36
sub_43
sub_193
sub_197
sub_202
mul_1106
mul_328
sub_165
sub_20
sub_33
mul_1048
mul_1069
sub_46
sub_206
sub_52
mul_306
mul_994
sub_49
mul_274
sub_51
sub_19
mul_123
mul_145
sub_30
sub_23
sub_27
sub_31
sub_47
mul_156
sub_53
mul_188
sub_61
mul_349
sub_68
sub_69
sub_72
sub_76
Assign_216
Assign_192
sub_88
Assign_249
Assign_183
sub_66
sub_70
Assign_264
Assign_228
sub_80
Assign_252
sub_90
sub_74
Assign_231
Assign_240
Assign_186
Assign_198
sub_92
sub_96
Assign_234
sub_82
sub_86
Assign_270
Assign_276
Assign_210
Assign_222
sub_104
Assign_246
Assign_258
sub_1
sub_3
Assign_9
Assign_18
Assign_12
sub_8
Assign_30
sub_6
Assign_24
sub_12
Assign_42
Assign_297
sub_10
Assign_36
sub_16
sub_40
Assign_354
Assign_366
Assign_39
Assign_48
Assign_306
sub_24
Assign_318
sub_32
Assign_342
sub_18
sub_124
Assign_378
Assign_279
sub_54
Assign_168
Assign_312
sub_108
Assign_330
Assign_345
sub_120
Assign_102
Assign_108
Assign_390
Assign_282
Assign_288
Assign_162
sub_58
Assign_450
sub_112
Assign_348
Assign_360
Assign_372
sub_128
Assign_393
sub_38
Assign_414
sub_98
Assign_300
sub_152
Assign_174
Assign_336
sub_118
sub_122
Assign_375
Assign_384
Assign_396
Assign_402
sub_140
Assign_114
Assign_294
sub_102
sub_48
Assign_324
sub_114
sub_166
sub_170
Assign_519
sub_130
sub_182
sub_136
Assign_420
Assign_426
Assign_438
Assign_456
sub_154
sub_44
Assign_327
Assign_498
Assign_510
Assign_522
sub_178
Assign_546
Assign_558
Assign_408
Assign_423
sub_144
Assign_441
Assign_84
Assign_462
Assign_471
Assign_120
sub_168
sub_172
sub_176
Assign_534
Assign_552
sub_188
sub_134
sub_138
Assign_594
Assign_432
Assign_444
Assign_60
sub_156
Assign_474
sub_162
sub_106
sub_42
Assign_528
Assign_537
sub_186
Assign_564
Assign_570
Assign_582
sub_200
Assign_606
sub_146
sub_150
Assign_468
sub_160
Assign_486
Assign_504
Assign_516
Assign_540
Assign_132
Assign_567
sub_192
Assign_585
Assign_600
Assign_609
sub_2
Assign_480
Assign_489
Assign_72
Assign_105
Assign_126
Assign_576
Assign_588
Assign_603
sub_205
sub_60
Assign_492
Assign_57
Assign_96
sub_194
sub_198
Assign_135
Assign_615
Assign_153
sub_56
sub_184
Assign_144
sub_50
Assign_150
Assign_54
sub_22
sub_26
Assign_87
Assign_66
Assign_78
Assign_90
Assign_138
sub_28
Assign_156
sub_34
Assign_180
sub_64
Assign_201
Assign_204
Assign_213
Assign_225
Assign_261
Assign_195
Assign_207
Assign_237
Assign_267
Assign_219
Assign_273
Assign_285
Assign_243
Assign_255
Assign_309
Assign
Assign_6
Assign_21
Assign_15
Assign_33
Assign_27
Assign_45
Assign_117
Assign_69
Assign_93
Assign_51
Assign_369
Assign_159
Assign_321
Assign_357
Assign_171
Assign_333
Assign_381
Assign_111
Assign_291
Assign_453
Assign_351
Assign_363
Assign_417
Assign_303
Assign_141
Assign_339
Assign_495
Assign_507
Assign_387
Assign_543
Assign_405
Assign_459
Assign_129
Assign_531
Assign_429
Assign_501
Assign_513
Assign_525
Assign_561
Assign_399
Assign_411
Assign_465
Assign_483
Assign_315
Assign_123
Assign_555
Assign_597
Assign_435
Assign_447
Assign_477
Assign_573
Assign_3
Assign_612
Assign_177
Assign_579
Assign_591
Assign_165
Assign_549
Assign_147
Assign_63
Assign_75
Assign_81
Assign_99
Assign_189
group_deps_1
